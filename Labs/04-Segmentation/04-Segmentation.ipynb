{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision, Lab 4: Segmentation\n",
    "\n",
    "Today we'll experiment with methods to segment a scene. Since we've been working with a sample ground robot in an indoor environment, we'll take as an example the task of segmenting unoccupied ground plane space from obstacles.\n",
    "\n",
    "If we can successfully determine which pixels in the image seen by our robot are on the flat floor and which are likely obstacles, we can combine that information with the ground plane homography-based rectification method we developed in the last lab to obtain a map of the space around the robot.\n",
    "\n",
    "The same approach could be used by an autonomous vehicle to determine where, in the image from its front-facing camera, the road is and where possible obstacles are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: Color-based segmentation\n",
    "\n",
    "In our indoor example, we have fairly consistent lighting and a consistently-colored floor, so a color based approach may work.\n",
    "\n",
    "The task here is, given an input pixel at location $(x,y)$ with RGB color $(r,g,b)$, classify the pixel as \"$floor$\" or \"$not floor$\".\n",
    "\n",
    "This is a classic machine learning problem. We have an input feature vector $(r, g, b)$ (some methods would also utilize $x$ and $y$ as well), and a desired output of 1 for $floor$ and 0 for $non-floor$.\n",
    "\n",
    "We probably want to transform the color space from RGB to HSV, since the RGB vector mixes color information with intensity information in the same measurements:\n",
    "\n",
    "<img src=\"img/lab03-1.jpg\" width=\"300\"/>\n",
    "\n",
    "whereas the HSV color space separates color from saturation (\"color purity\") and intensity:\n",
    "\n",
    "<img src=\"img/lab03-2.jpg\" width=\"300\"/>\n",
    "\n",
    "(Images are from Wikimedia Commons via the OpenCV documentation.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we could use a fancy classifier like logistic regression or a SVM, this problem is easily solved with a generative model. We apply the principle of maximum a posteriori classification and Bayes' rule:\n",
    "\n",
    "\\begin{equation}\n",
    "f(h, s, v) = \n",
    "    \\begin{cases}\n",
    "        1 & \\textit{when p(floor | h, s, v) > 0.5} \\\\\n",
    "        0 &  \\textit{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "p(floor | h, s, v) = \\frac{p(h, s, v | floor) p(floor)}{p(h, s, v)}\n",
    "\\end{equation}\n",
    "\n",
    "and since $p(h, s, v)$ is not known, we eliminate it:\n",
    "\n",
    "\\begin{equation}\n",
    "f(h, s, v) =\n",
    "    \\begin{cases}\n",
    "        1 & \\textit{when } p(h, s, v | floor) p(floor) > p(h, s, v | not floor) \\\\\n",
    "        0 &  \\textit{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "The entity $p(floor)$ is easy to estimate as the proportion of pixels in a sample of images that are floor pixels.\n",
    "\n",
    "What about $p(h, s, v | floor)$?\n",
    "\n",
    "The simplest approach here is just a lookup table. Since we're conditioning on floor, we just need to sample a bunch of floor pixels and record the frequency of each value of $h$, $s$, and $v$.\n",
    "\n",
    "The problem with that approach is that we'd need something like $10 * 255 * 255 * 255$ pixels (168M!) to get a decent estimate of the value in every bin of this frequency table.\n",
    "\n",
    "What we do instead is collapse some values of $h$, $s$, and $v$ into bins. For example, we might drop the 4 least significant bits of each of these values, then we'd end up needing only about $10 * 16 * 16 * 16$ (just 41K) samples to get a decent estimate.\n",
    "\n",
    "Note that some methods would further drop the V element, which represents intensity, on the assumption that the global amount of lighting doesn't matter.\n",
    "\n",
    "In fact, as the sun moves across the sky, the color composition of the light changes, and artificial light is quite different in color than sunlight. Still, it's a start.\n",
    "\n",
    "So we want to collapse the $(h, s, v)$ vector into two features, $(h>>4, s>>4)$. We'll end up with a lookup table containing just $16*16 = 256$ elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and save an image-plane-to-ground-plane homography to a YML file\n",
    "\n",
    "Let's use a simple version of the program from lab 2 that calls `getPerspectiveTransform()`\n",
    "to get a ground plane to image plane homography and save it to a YML file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <opencv2/opencv.hpp>\n",
    "#include <iostream>\n",
    "\n",
    "using namespace cv;\n",
    "using namespace std;\n",
    "\n",
    "#define VIDEO_FILE \"robot.mp4\"\n",
    "#define HOMOGRAPHY_FILE \"robot-homography.yml\"\n",
    "\n",
    "Mat matPauseScreen, matResult, matFinal;\n",
    "Point point;\n",
    "vector<Point> pts;\n",
    "int var = 0;\n",
    "int drag = 0;\n",
    "\n",
    "// Create mouse handler function\n",
    "void mouseHandler(int event, int x, int y, int, void*)\n",
    "{\n",
    "    if (var >= 4) return;\n",
    "    if (event == EVENT_LBUTTONDOWN) // Left button down\n",
    "    {\n",
    "        drag = 1;\n",
    "        matResult = matFinal.clone();\n",
    "        point = Point(x, y);\n",
    "        if (var >= 1) \n",
    "        {\n",
    "            line(matResult, pts[var - 1], point, Scalar(0, 255, 0, 255), 2);\n",
    "        }\n",
    "        circle(matResult, point, 2, Scalar(0, 255, 0), -1, 8, 0);\n",
    "        imshow(\"Source\", matResult);\n",
    "    }\n",
    "    if (event == EVENT_LBUTTONUP && drag) // When Press mouse left up\n",
    "    {\n",
    "        drag = 0; var++;\n",
    "        pts.push_back(point);\n",
    "        matFinal = matResult.clone();\n",
    "        if (var >= 4)\n",
    "        {\n",
    "            line(matFinal, pts[0], pts[3], Scalar(0, 255, 0, 255), 2);\n",
    "            fillPoly(matFinal, pts, Scalar(0, 120, 0, 20), 8, 0);\n",
    "\n",
    "            setMouseCallback(\"Source\", NULL, NULL);\n",
    "        }\n",
    "        imshow(\"Source\", matFinal);\n",
    "    }\n",
    "    if (drag)\n",
    "    {\n",
    "        matResult = matFinal.clone();\n",
    "        point = Point(x, y);\n",
    "        if (var >= 1) \n",
    "            line(matResult, pts[var - 1], point, Scalar(0, 255, 0, 255), 2);\n",
    "        circle(matResult, point, 2, Scalar(0, 255, 0), -1, 8, 0);\n",
    "        imshow(\"Source\", matResult);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    Mat matFrameCapture;\n",
    "    Mat matFrameDisplay;\n",
    "    int key = -1;\n",
    "\n",
    "    VideoCapture videoCapture(VIDEO_FILE);\n",
    "    if (!videoCapture.isOpened()) {\n",
    "        cerr << \"ERROR! Unable to open input video file \" << VIDEO_FILE << endl;\n",
    "        return -1;\n",
    "    }\n",
    "\n",
    "    while (key < 0)        // play video until press any key\n",
    "    {\n",
    "        // Get the next frame\n",
    "        videoCapture.read(matFrameCapture);\n",
    "        if (matFrameCapture.empty()) {\n",
    "            // End of video file\n",
    "            break;\n",
    "        }\n",
    "        float ratio = 640.0 / matFrameCapture.cols;\n",
    "        resize(matFrameCapture, matFrameDisplay, cv::Size(), ratio, ratio, INTER_LINEAR);\n",
    "\n",
    "        imshow(VIDEO_FILE, matFrameDisplay);\n",
    "        key = waitKey(30);\n",
    "\n",
    "        if (key >= 0)\n",
    "        {\n",
    "            destroyWindow(VIDEO_FILE);\n",
    "            matPauseScreen = matFrameCapture;\n",
    "            matFinal = matPauseScreen.clone();\n",
    "\n",
    "            namedWindow(\"Source\", WINDOW_AUTOSIZE);\n",
    "            setMouseCallback(\"Source\", mouseHandler, NULL);\n",
    "            imshow(\"Source\", matPauseScreen);\n",
    "            waitKey(0);\n",
    "            destroyWindow(\"Source\");\n",
    "\n",
    "            Point2f src[4];\n",
    "            for (int i = 0; i < 4; i++)\n",
    "            {\n",
    "                src[i].x = pts[i].x * 1.0;\n",
    "                src[i].y = pts[i].y * 1.0;\n",
    "            }\n",
    "            Point2f reals[4];\n",
    "            reals[0] = Point2f(800.0, 800.0);\n",
    "            reals[1] = Point2f(1000.0, 800.0);\n",
    "            reals[2] = Point2f(1000.0, 1000.0);\n",
    "            reals[3] = Point2f(800.0, 1000.0);\n",
    "\n",
    "            Mat homography_matrix = getPerspectiveTransform(src, reals);\n",
    "            std::cout << \"Estimated Homography Matrix is:\" << std::endl;\n",
    "            std::cout << homography_matrix << std::endl;\n",
    "\n",
    "            // perspective transform operation using transform matrix\n",
    "            cv::warpPerspective(matPauseScreen, matResult, homography_matrix, matPauseScreen.size(), cv::INTER_LINEAR);\n",
    "            imshow(\"Source\", matPauseScreen);\n",
    "            imshow(\"Result\", matResult);\n",
    "\n",
    "            waitKey(0);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "VIDEO_FILE = \"robot.mp4\"\n",
    "HOMOGRAPHY_FILE = \"robot-homography.yml\"\n",
    "\n",
    "matResult = None\n",
    "matFinal = None\n",
    "matPauseScreen = None\n",
    "\n",
    "point = (-1, -1)\n",
    "pts = []\n",
    "var = 0 \n",
    "drag = 0\n",
    "\n",
    "# Mouse handler function has 5 parameters input (no matter what)\n",
    "def mouseHandler(event, x, y, flags, param):\n",
    "    global point, pts, var, drag, matFinal, matResult\n",
    "\n",
    "    if (var >= 4):                           # if homography points are more than 4 points, do nothing\n",
    "        return\n",
    "    if (event == cv2.EVENT_LBUTTONDOWN):     # When Press mouse left down\n",
    "        drag = 1                             # Set it that the mouse is in pressing down mode\n",
    "        matResult = matFinal.copy()          # copy final image to draw image\n",
    "        point = (x, y)                       # memorize current mouse position to point var\n",
    "        if (var >= 1):                       # if the point has been added more than 1 points, draw a line\n",
    "            cv2.line(matResult, pts[var - 1], point, (0, 255, 0, 255), 2)    # draw a green line with thickness 2\n",
    "        cv2.circle(matResult, point, 2, (0, 255, 0), -1, 8, 0)             # draw a current green point\n",
    "        cv2.imshow(\"Source\", matResult)      # show the current drawing\n",
    "    if (event == cv2.EVENT_LBUTTONUP and drag):  # When Press mouse left up\n",
    "        drag = 0                             # no more mouse drag\n",
    "        pts.append(point)                    # add the current point to pts\n",
    "        var += 1                             # increase point number\n",
    "        matFinal = matResult.copy()          # copy the current drawing image to final image\n",
    "        if (var >= 4):                                                      # if the homograpy points are done\n",
    "            cv2.line(matFinal, pts[0], pts[3], (0, 255, 0, 255), 2)   # draw the last line\n",
    "            cv2.fillConvexPoly(matFinal, np.array(pts, 'int32'), (0, 120, 0, 20))        # draw polygon from points\n",
    "        cv2.imshow(\"Source\", matFinal);\n",
    "    if (drag):                                    # if the mouse is dragging\n",
    "        matResult = matFinal.copy()               # copy final images to draw image\n",
    "        point = (x, y)                   # memorize current mouse position to point var\n",
    "        if (var >= 1):                            # if the point has been added more than 1 points, draw a line\n",
    "            cv2.line(matResult, pts[var - 1], point, (0, 255, 0, 255), 2)    # draw a green line with thickness 2\n",
    "        cv2.circle(matResult, point, 2, (0, 255, 0), -1, 8, 0)         # draw a current green point\n",
    "        cv2.imshow(\"Source\", matResult)           # show the current drawing\n",
    "\n",
    "def main():\n",
    "    global matFinal, matResult, matPauseScreen\n",
    "    key = -1;\n",
    "\n",
    "    videoCapture = cv2.VideoCapture(VIDEO_FILE)\n",
    "    if not videoCapture.isOpened():\n",
    "        print(\"ERROR! Unable to open input video file \", VIDEO_FILE)\n",
    "        return -1\n",
    "\n",
    "    width  = videoCapture.get(cv2.CAP_PROP_FRAME_WIDTH)   # float `width`\n",
    "    height = videoCapture.get(cv2.CAP_PROP_FRAME_HEIGHT)  # float `height`\n",
    "\n",
    "    # Capture loop \n",
    "    while (key < 0):\n",
    "        # Get the next frame\n",
    "        _, matFrameCapture = videoCapture.read()\n",
    "        if matFrameCapture is None:\n",
    "            # End of video file\n",
    "            break\n",
    "\n",
    "        ratio = 640.0 / width\n",
    "        dim = (int(width * ratio), int(height * ratio))\n",
    "        matFrameDisplay = cv2.resize(matFrameDisplay, dim)\n",
    "\n",
    "        cv2.imshow(VIDEO_FILE, matFrameDisplay)\n",
    "        key = cv2.waitKey(30)\n",
    "\n",
    "        if (key >= 0):\n",
    "            cv2.destroyWindow(VIDEO_FILE)\n",
    "            matPauseScreen = matFrameCapture\n",
    "            matFinal = matPauseScreen.copy()\n",
    "            cv2.namedWindow(\"Source\", cv2.WINDOW_AUTOSIZE)\n",
    "            cv2.setMouseCallback(\"Source\", mouseHandler)\n",
    "            cv2.imshow(\"Source\", matPauseScreen)\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyWindow(\"Source\")\n",
    "\n",
    "            if (len(pts) < 4):\n",
    "                return\n",
    "\n",
    "            src = np.array(pts).astype(np.float32)\n",
    "            reals = np.array([(800, 800),\n",
    "                                (1000, 800),\n",
    "                                (1000, 1000),\n",
    "                                (800, 1000)], np.float32)\n",
    "\n",
    "            homography_matrix = cv2.getPerspectiveTransform(src, reals);\n",
    "            print(\"Estimated Homography Matrix is:\")\n",
    "            print(homography_matrix)\n",
    "\n",
    "            h, w, ch = matPauseScreen.shape\n",
    "            matResult = cv2.warpPerspective(matPauseScreen, homography_matrix, (w, h), cv2.INTER_LINEAR)\n",
    "            matPauseScreen = cv2.resize(matPauseScreen, dim)\n",
    "            cv2.imshow(\"Source\", matPauseScreen)\n",
    "            matResult = cv2.resize(matResult, dim)\n",
    "            cv2.imshow(\"Result\", matResult)\n",
    "\n",
    "            cv2.waitKey(0)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a class for reading/writing homography data\n",
    "\n",
    "Let's make a structure for our homography data. Objects\n",
    "of class <code>HomographyData</code> will keep the information about a homography. It contains:\n",
    "\n",
    "- cPoints: number of points for setting homography\n",
    "- aPoints: points information\n",
    "- matH: Homography matrix\n",
    "- widthOut: image width of the output image\n",
    "- heightOut: image height of the output image\n",
    "\n",
    "We need two methods: reading and writing homographies.\n",
    "\n",
    "At this step, you should create a new cpp file for create the special functions and structure. Please create 2 files: <code>HomographyData.h</code> and <code>HomographyData.cpp</code>\n",
    "\n",
    "For Visual Studio users: <code>HomographyData.h</code> should be created in your\n",
    "\"Header Files\" section, and <code>HomographyData.cpp</code> should be created in\n",
    "your \"Source Files\" section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C++ header\n",
    "\n",
    "Place the following in `HomographyData.h`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <opencv2/opencv.hpp>\n",
    "\n",
    "using namespace cv;\n",
    "\n",
    "typedef struct {\n",
    "    cv::Mat matH;\n",
    "    int widthOut;\n",
    "    int heightOut;\n",
    "    int cPoints;\n",
    "    cv::Point2f aPoints[4];\n",
    "} tsHomographyData;\n",
    "\n",
    "bool readHomography(std::string homography_file, tsHomographyData* pHomographyData);\n",
    "bool writeHomography(std::string homography_file, tsHomographyData* pHomographyData);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C++ source file\n",
    "\n",
    "Then provide the implementation in `HomographyData.cpp`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include \"homography.h\"\n",
    "\n",
    "bool readHomography(std::string homography_file, tsHomographyData* pHomographyData) {\n",
    "    cv::FileStorage fileStorage(homography_file, cv::FileStorage::Mode::READ);\n",
    "    if (!fileStorage.isOpened()) {\n",
    "        return false;\n",
    "    }\n",
    "    cv::FileNode points = fileStorage[\"aPoints\"];\n",
    "    cv::FileNodeIterator it = points.begin(), it_end = points.end();\n",
    "    pHomographyData->cPoints = 0;\n",
    "    for (int i = 0; it != it_end; it++, i++) {\n",
    "        (*it) >> pHomographyData->aPoints[i];\n",
    "        pHomographyData->cPoints++;\n",
    "    }\n",
    "    fileStorage[\"matH\"] >> pHomographyData->matH;\n",
    "    fileStorage[\"widthOut\"] >> pHomographyData->widthOut;\n",
    "    fileStorage[\"heightOut\"] >> pHomographyData->heightOut;\n",
    "    fileStorage.release();\n",
    "    return true;\n",
    "}\n",
    "\n",
    "bool writeHomography(std::string homography_file, tsHomographyData* pHomographyData)\n",
    "{\n",
    "    cv::FileStorage fileStorage(homography_file, cv::FileStorage::Mode::WRITE);\n",
    "    if (!fileStorage.isOpened()) {\n",
    "        return false;\n",
    "    }\n",
    "\n",
    "    fileStorage << \"aPoints\" << \"[\";\n",
    "    for (int i = 0; i < 4; i++)\n",
    "    {\n",
    "        fileStorage << pHomographyData->aPoints[i];\n",
    "    }\n",
    "    fileStorage << \"]\";\n",
    "    fileStorage << \"matH\" << pHomographyData->matH;\n",
    "    fileStorage << \"widthOut\" << pHomographyData->widthOut;\n",
    "    fileStorage << \"heightOut\" << pHomographyData->heightOut;\n",
    "    fileStorage.release();\n",
    "    return true;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from typing import List #use it for :List[...]\n",
    "\n",
    "class tsHomographyData:\n",
    "    matH = np.zeros((3, 3))\n",
    "    widthOut : int\n",
    "    heightOut : int\n",
    "    cPoints : int\n",
    "    aPoints:list = []\n",
    "\n",
    "\n",
    "def readHomography(homography_file: str) -> \"tsHomographyData\":\n",
    "    fileStorage = cv2.FileStorage(homography_file, cv2.FILE_STORAGE_READ)\n",
    "    if not fileStorage.isOpened():\n",
    "        return None\n",
    "\n",
    "    \n",
    "    pHomographyData.cPoints = 0\n",
    "    for i in range(points.size()):\n",
    "        points = fileStorage.getNode(\"aPoints\" + str(i))\n",
    "        pHomographyData.aPoints.append(points.mat())\n",
    "        pHomographyData.cPoints += 1\n",
    "    pHomographyData.matH = fileStorage.getNode(\"matH\").mat()\n",
    "    pHomographyData.widthOut = int(fileStorage.getNode(\"widthOut\").real())\n",
    "    pHomographyData.heightOut = int(fileStorage.getNode(\"heightOut\").real())\n",
    "    fileStorage.release()\n",
    "    return pHomographyData\n",
    "\n",
    "def writeHomography(homography_file: str, pHomographyData:\"tsHomographyData\") -> bool:\n",
    "    fileStorage = cv2.FileStorage(homography_file, cv2.FILE_STORAGE_WRITE)\n",
    "    if not fileStorage.isOpened():\n",
    "        return False\n",
    "\n",
    "    for i in range(4):\n",
    "        fileStorage.write(\"aPoints\" + str(i), pHomographyData.aPoints[i])\n",
    "\n",
    "    fileStorage.write(\"matH\", pHomographyData.matH)\n",
    "    fileStorage.write(\"widthOut\", pHomographyData.widthOut)\n",
    "    fileStorage.write(\"heightOut\", pHomographyData.heightOut)\n",
    "    fileStorage.release()\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using it in C++\n",
    "\n",
    "Go back to your main file, add include library\n",
    "\n",
    "<code>#include \"homography.h\"</code>\n",
    "\n",
    "If everything is correct, you can add the homograpy data in yml file by adding the code after homography calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include \"homography.h\"\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    "tsHomographyData homographyData;\n",
    "// add homography writing file\n",
    "for (int i = 0; i < 4; i++)\n",
    "{\n",
    "    homographyData.aPoints[i] = src[i];\n",
    "}\n",
    "homographyData.matH = homography_matrix;\n",
    "homographyData.widthOut = matPauseScreen.cols;\n",
    "homographyData.heightOut = matPauseScreen.rows;\n",
    "\n",
    "if (!writeHomography(HOMOGRAPHY_FILE, &homographyData)) {\n",
    "    cerr << \"ERROR! Unable to write homography data file \" << HOMOGRAPHY_FILE << endl;\n",
    "    return -1;\n",
    "}\n",
    "\n",
    "...\n",
    "\n",
    "// Read H from file\n",
    "\n",
    "HomographyData homographyData(HOMOGRAPHY_FILE);\n",
    "if (!homographyData.cPoints == 0) {\n",
    "    cerr << \"ERROR! Unable to read homography data file \" << HOMOGRAPHY_FILE << endl;\n",
    "    return -1;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using it in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from homography import tsHomographyData, readHomography, writeHomography\n",
    "\n",
    "\n",
    "# Write H to file\n",
    "\n",
    "homographyData = Homography()\n",
    "homographyData.cPoints = 0\n",
    "for i in range(4):\n",
    "    homographyData.aPoints.append(src[i])\n",
    "    homographyData.cPoints += 1\n",
    "homographyData.matH = homography_matrix\n",
    "homographyData.widthOut = width\n",
    "homographyData.heightOut = height\n",
    "homographyData.write(HOMOGRAPHY_FILE)\n",
    "\n",
    "...\n",
    "\n",
    "# Read H from file\n",
    "\n",
    "homographyData = Homography(HOMOGRAPHY_FILE)\n",
    "if homographyData.cPoints == 0:\n",
    "    print(\"ERROR! Unable to read homography data file \", HOMOGRAPHY_FILE)\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The output file `robot-homography.yml` might contain the following. This is from the Python version; the C++ version is slightly\n",
    "different.\n",
    "\n",
    "    %YAML:1.0\n",
    "    ---\n",
    "    aPoints0: !!opencv-matrix\n",
    "       rows: 2\n",
    "       cols: 1\n",
    "       dt: d\n",
    "       data: [ 860., 49. ]\n",
    "    aPoints1: !!opencv-matrix\n",
    "       rows: 2\n",
    "       cols: 1\n",
    "       dt: d\n",
    "       data: [ 1386., 52. ]\n",
    "    aPoints2: !!opencv-matrix\n",
    "       rows: 2\n",
    "       cols: 1\n",
    "       dt: d\n",
    "       data: [ 1620., 375. ]\n",
    "    aPoints3: !!opencv-matrix\n",
    "       rows: 2\n",
    "       cols: 1\n",
    "       dt: d\n",
    "       data: [ 784., 367. ]\n",
    "    matH: !!opencv-matrix\n",
    "       rows: 3\n",
    "       cols: 3\n",
    "       dt: d\n",
    "       data: [ -1.1391186748382573e-03, -2.2794919900117339e-03,\n",
    "           1.0627648167793186e-16, 1.6604445559011895e-05,\n",
    "           -3.2917310423991194e-03, -3.1863931027743840e-02,\n",
    "           -1.6241141452077307e-09, -1.8250980274059071e-06,\n",
    "           -9.0126408686546570e-04 ]\n",
    "    widthOut: 2419\n",
    "    heightOut: 1250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a pre-computed homography in a new program in OOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C++ Header file\n",
    "\n",
    "Create <code>homographyclass.h</code> header file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <opencv2/opencv.hpp>\n",
    "\n",
    "using namespace cv;\n",
    "\n",
    "class HomographyIO\n",
    "{\n",
    "    \n",
    "public:\n",
    "    cv::Mat matH;\n",
    "    int widthOut;\n",
    "    int heightOut;\n",
    "    int cPoints;\n",
    "    cv::Point2f aPoints[4];\n",
    "\n",
    "    HomographyIO();\n",
    "    HomographyIO(std::string homography_file);\n",
    "\n",
    "    bool read(std::string homography_file);\n",
    "    bool write(std::string homography_file);\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C++ Source file\n",
    "\n",
    "Create <code>homographyclass.cpp</code> file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include \"homographyclass.h\"\n",
    "\n",
    "HomographyIO::HomographyIO(std::string homography_file)\n",
    "{\n",
    "    read(homography_file);\n",
    "}\n",
    "\n",
    "HomographyIO::HomographyIO()\n",
    "{\n",
    "    cPoints = 0;\n",
    "}\n",
    "\n",
    "bool HomographyIO::read(std::string homography_file)\n",
    "{\n",
    "    cv::FileStorage fileStorage(homography_file, cv::FileStorage::Mode::READ);\n",
    "    if (!fileStorage.isOpened()) {\n",
    "        return false;\n",
    "    }\n",
    "    cv::FileNode points = fileStorage[\"aPoints\"];\n",
    "    cv::FileNodeIterator it = points.begin(), it_end = points.end();\n",
    "    cPoints = 0;\n",
    "    for (int i = 0; it != it_end; it++, i++) {\n",
    "        (*it) >> aPoints[i];\n",
    "        cPoints++;\n",
    "    }\n",
    "    fileStorage[\"matH\"] >> matH;\n",
    "    fileStorage[\"widthOut\"] >> widthOut;\n",
    "    fileStorage[\"heightOut\"] >> heightOut;\n",
    "    fileStorage.release();\n",
    "    return true;\n",
    "}\n",
    "\n",
    "bool HomographyIO::write(std::string homography_file)\n",
    "{\n",
    "    cv::FileStorage fileStorage(homography_file, cv::FileStorage::Mode::WRITE);\n",
    "    if (!fileStorage.isOpened()) {\n",
    "        return false;\n",
    "    }\n",
    "\n",
    "    fileStorage << \"aPoints\" << \"[\";\n",
    "    for (int i = 0; i < 4; i++)\n",
    "    {\n",
    "        fileStorage << aPoints[i];\n",
    "    }\n",
    "    fileStorage << \"]\";\n",
    "    fileStorage << \"matH\" << matH;\n",
    "    fileStorage << \"widthOut\" << widthOut;\n",
    "    fileStorage << \"heightOut\" << heightOut;\n",
    "    fileStorage.release();\n",
    "    return true;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the file in C++\n",
    "\n",
    "In <code>main.cpp</code> (or an any file which contains <code>main</code> function), modify the code as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include \"homographyclass.h\"\n",
    ".\n",
    ".\n",
    ".\n",
    "// For writing\n",
    "HomographyIO homographyData;\n",
    "for (int i = 0; i < 4; i++)\n",
    "{\n",
    "    homographyData.aPoints[i] = src[i];\n",
    "    homographyData.cPoints++;\n",
    "}\n",
    "homographyData.matH = homography_matrix;\n",
    "homographyData.widthOut = matPauseScreen.cols;\n",
    "homographyData.heightOut = matPauseScreen.rows;\n",
    "if (!homographyData.write(HOMOGRAPHY_FILE)) {\n",
    "    cerr << \"ERROR! Unable to write homography data file \" << HOMOGRAPHY_FILE << endl;\n",
    "    return -1;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// For Reading\n",
    "HomographyIO homographyData(HOMOGRAPHY_FILE);\n",
    "if (!homographyData.cPoints == 0) {\n",
    "    cerr << \"ERROR! Unable to read homography data file \" << HOMOGRAPHY_FILE << endl;\n",
    "    return -1;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a class file name <code>homography.py</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from typing import List #use it for :List[...]\n",
    "\n",
    "class Homograpy:\n",
    "    matH = np.zeros((3, 3))\n",
    "    widthOut : int\n",
    "    heightOut : int\n",
    "    cPoints : int\n",
    "    aPoints:list = []\n",
    "\n",
    "    def __init__(self, homography_file = None):\n",
    "        self.cPoints = 0\n",
    "        if homography_file is not None:\n",
    "            self.read(homography_file)\n",
    "\n",
    "    def read(self, homography_file):\n",
    "        fileStorage = cv2.FileStorage(homography_file, cv2.FILE_STORAGE_READ)\n",
    "        if not fileStorage.isOpened():\n",
    "            return False\n",
    "\n",
    "        self.cPoints = 0\n",
    "        for i in range(points.size()):\n",
    "            points = fileStorage.getNode(\"aPoints\" + str(i))\n",
    "            self.aPoints.append(points.mat())\n",
    "            self.cPoints += 1\n",
    "        self.matH = fileStorage.getNode(\"matH\").mat()\n",
    "        self.widthOut = int(fileStorage.getNode(\"widthOut\").real())\n",
    "        self.heightOut = int(fileStorage.getNode(\"heightOut\").real())\n",
    "        fileStorage.release()\n",
    "        return True\n",
    "\n",
    "    def write(self, homography_file):\n",
    "        fileStorage = cv2.FileStorage(homography_file, cv2.FILE_STORAGE_WRITE)\n",
    "        if not fileStorage.isOpened():\n",
    "            return False\n",
    "\n",
    "        for i in range(4):\n",
    "            fileStorage.write(\"aPoints\" + str(i), self.aPoints[i])\n",
    "\n",
    "        fileStorage.write(\"matH\", self.matH)\n",
    "        fileStorage.write(\"widthOut\", self.widthOut)\n",
    "        fileStorage.write(\"heightOut\", self.heightOut)\n",
    "        fileStorage.release()\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the main python file, insert the library class on the top of your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import homography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "put the code in the main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for writing\n",
    "\n",
    "homographyData = Homography()\n",
    "homographyData.cPoints = 0\n",
    "for i in range(4):\n",
    "    homographyData.aPoints.append(src[i])\n",
    "    homographyData.cPoints += 1\n",
    "homographyData.matH = homography_matrix\n",
    "homographyData.widthOut = width\n",
    "homographyData.heightOut = height\n",
    "homographyData.write(HOMOGRAPHY_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reading\n",
    "\n",
    "homographyData = Homography(HOMOGRAPHY_FILE)\n",
    "if homographyData.cPoints == 0:\n",
    "    print(\"ERROR! Unable to read homography data file \", HOMOGRAPHY_FILE)\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C++ Main full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <opencv2/opencv.hpp>\n",
    "#include <iostream>\n",
    "#include \"homography.h\"\n",
    "\n",
    "using namespace cv;\n",
    "using namespace std;\n",
    "\n",
    "#define VIDEO_FILE \"robot.mp4\"\n",
    "#define HOMOGRAPHY_FILE \"robot-homography.yml\"\n",
    "\n",
    "void displayFrame(cv::Mat& matFrameDisplay, int iFrame, int cFrames, tsHomographyData* pHomographyData) {\n",
    "    for (int i = 0; i < pHomographyData->cPoints; i++) {\n",
    "        cv::circle(matFrameDisplay, pHomographyData->aPoints[i], 10, cv::Scalar(255, 0, 0), 2, cv::LINE_8, 0);\n",
    "    }\n",
    "    imshow(VIDEO_FILE, matFrameDisplay);\n",
    "    stringstream ss;\n",
    "    ss << \"Frame \" << iFrame << \"/\" << cFrames;\n",
    "    ss << \": hit <space> for next frame or 'q' to quit\";\n",
    "    //cv::displayOverlay(VIDEO_FILE, ss.str(), 0);  // for linux + qt\n",
    "    putText(matFrameDisplay, ss.str(), Point(10, 30), FONT_HERSHEY_SIMPLEX, 1.0, Scalar(0, 0, 255), 3);\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    cv::Mat matFrameCapture;\n",
    "    cv::Mat matFrameDisplay;\n",
    "    int cFrames;\n",
    "    tsHomographyData homographyData;\n",
    "\n",
    "    cv::VideoCapture videoCapture(VIDEO_FILE);\n",
    "    if (!videoCapture.isOpened()) {\n",
    "        cerr << \"ERROR! Unable to open input video file \" << VIDEO_FILE << endl;\n",
    "        return -1;\n",
    "    }\n",
    "    cFrames = (int)videoCapture.get(cv::CAP_PROP_FRAME_COUNT);\n",
    "\n",
    "    // Create a named window that will be used later to display each frame\n",
    "    cv::namedWindow(VIDEO_FILE, (unsigned int)cv::WINDOW_NORMAL | cv::WINDOW_KEEPRATIO | cv::WINDOW_GUI_EXPANDED);\n",
    "\n",
    "    // Read homography from file\n",
    "    if (!readHomography(HOMOGRAPHY_FILE, &homographyData)) {\n",
    "        cerr << \"ERROR! Unable to read homography data file \" << HOMOGRAPHY_FILE << endl;\n",
    "        return -1;\n",
    "    }\n",
    "\n",
    "    int iFrame = 0;\n",
    "    while (true) {\n",
    "\n",
    "        // Block for next frame\n",
    "\n",
    "        videoCapture.read(matFrameCapture);\n",
    "        if (matFrameCapture.empty()) {\n",
    "            // End of video file\n",
    "            break;\n",
    "        }\n",
    "\n",
    "        displayFrame(matFrameCapture, iFrame, cFrames, &homographyData);\n",
    "\n",
    "        int iKey;\n",
    "        do {\n",
    "            iKey = cv::waitKey(10);\n",
    "            if (getWindowProperty(VIDEO_FILE, cv::WND_PROP_VISIBLE) == 0) {\n",
    "                return 0;\n",
    "            }\n",
    "            if (iKey == (int)'q' || iKey == (int)'Q') {\n",
    "                return 0;\n",
    "            }\n",
    "        } while (iKey != (int)' ');\n",
    "        iFrame++;\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Main Full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from homography import Homography\n",
    "\n",
    "VIDEO_FILE = \"robot.mp4\"\n",
    "HOMOGRAPHY_FILE = \"robot-homography.yml\"\n",
    "\n",
    "def displayFrame(matFrameDisplay, iFrame, cFrames, pHomographyData):\n",
    "    for i in range(pHomographyData.cPoints):\n",
    "        cv2.circle(matFrameDisplay, pHomographyData.aPoints[i], 10, (255, 0, 0), 2, cv2.LINE_8, 0)\n",
    "\n",
    "    cv2.imshow(VIDEO_FILE, matFrameDisplay)\n",
    "    ss = \"Frame \" + str(iFrame) + \"/\" + str(cFrames)\n",
    "    ss += \": hit <space> for next frame or 'q' to quit\"\n",
    "    # cv2.displayOverlay(VIDEO_FILE, ss, 0);  # for linux + qt\n",
    "    cv2.putText(matFrameDisplay, ss, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 3)\n",
    "\n",
    "def main():\n",
    "    global matFinal, matResult, matPauseScreen\n",
    "    key = -1\n",
    "\n",
    "    videoCapture = cv2.VideoCapture(VIDEO_FILE)\n",
    "    if not videoCapture.isOpened():\n",
    "        print(\"ERROR! Unable to open input video file \", VIDEO_FILE)\n",
    "        return -1\n",
    "\n",
    "    width  = videoCapture.get(cv2.CAP_PROP_FRAME_WIDTH)   # float `width`\n",
    "    height = videoCapture.get(cv2.CAP_PROP_FRAME_HEIGHT)  # float `height`\n",
    "\n",
    "    cFrames = videoCapture.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "    cv2.namedWindow(VIDEO_FILE, cv2.WINDOW_NORMAL | cv2.WINDOW_KEEPRATIO | cv2.WINDOW_GUI_EXPANDED)\n",
    "\n",
    "    homographyData = Homography(HOMOGRAPHY_FILE)\n",
    "    if homographyData is None:\n",
    "        print(\"ERROR! Unable to read homography data file \", HOMOGRAPHY_FILE)\n",
    "        return -1\n",
    "\n",
    "    iFrame = 0\n",
    "    # Capture loop \n",
    "    while (key < 0):\n",
    "        # Get the next frame\n",
    "        _, matFrameCapture = videoCapture.read()\n",
    "        if matFrameCapture is None:\n",
    "            # End of video file\n",
    "            break\n",
    "\n",
    "        displayFrame(matFrameCapture, iFrame, cFrames, homographyData)\n",
    "\n",
    "        iKey = -1\n",
    "        while iKey != ord(' '):\n",
    "            iKey = cv2.waitKey(10)\n",
    "            if (cv2.getWindowProperty(VIDEO_FILE, cv2.WND_PROP_VISIBLE) == 0):\n",
    "                return 0\n",
    "            if (iKey == ord('q') or iKey == ord('Q')):\n",
    "                return 0\n",
    "\n",
    "        iFrame += 1\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick version of segmentation\n",
    "\n",
    "As a simple experiment, find a pixel in the image that is clearly on the floor, then\n",
    "convert the input image to the HSV colorspace using code\n",
    "\n",
    "    cvtColor(matFrameBGR, matFrameHSV, COLOR_BGR2HSV);\n",
    "\n",
    "If you display HSV as an image you might get something like this:\n",
    "\n",
    "<img src=\"img/lab03-4.png\" width=\"600\"/>\n",
    "\n",
    "Output the HSV values at your preferred location. Create a 2D array <code>double aProbFloorHS[16][16]</code>\n",
    "and set the selected bin to a probability of 1.0 and all others to 0.\n",
    "Show, in a second window, a mask for the pixels selected by $p(h, s, v \\mid \\text{floor}) > 0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain HS probabilities using a machine learning model\n",
    "\n",
    "Next, take the first frame from the video, load it in <link>[gimp](https://www.gimp.org/downloads/)</link> or other image editing software, and make a mask for the floor pixels.\n",
    "\n",
    "To extract frames from the video at a frame rate of one frame per second of the video, try at the command line\n",
    "\n",
    "    $ sudo apt-get install ffmpeg\n",
    "    $ ffmpeg -i robot.mp4 -r 1 -f image2 frame-%03d.png\n",
    "\n",
    "Or you can save an image sequence from your OpenCV program:\n",
    "\n",
    "    stringstream ss;\n",
    "    ss << fixed << setprecision(3) << setfill('0');     // set fix digit prefix length to 3 and the blank digit is filled by 0\n",
    "    ss << \"frame-\" << setw(3) << iFrame << \".png\";      // set iFrame length to 3 from command above\n",
    "    imwrite(ss.str(), matFrameCapture);\n",
    "\n",
    "To make a mask in gimp, add a 50% transparent layer, color pixels you want to select, and delete the original image layer with: \n",
    "\n",
    "(right click $\\rightarrow$ Layer $\\rightarrow$ Stack $\\rightarrow$ Select Next Layer), and then\n",
    "\n",
    "(right click $\\rightarrow$ Layer $\\rightarrow$ Delete Layer).\n",
    "\n",
    "<img src=\"img/lab03-5.png\" width=\"300\"/>\n",
    "\n",
    "<img src=\"img/lab03-6.png\" width=\"600\"/>\n",
    "\n",
    "<img src=\"img/lab03-7.png\" width=\"600\"/>\n",
    "\n",
    "You can copy the resulting layer then create a new image from the clipboard and export as a black and white PNG.\n",
    "\n",
    "<img src=\"img/lab03-8.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add code to your program to read a mask along with the corresponding frame, apply the mask to the first frame of the video,\n",
    "and accumulalte the entries in the <code>aProbFloorHS</code> array. You'll also need a <code>aProbNonFloorHS</code> array and\n",
    "total pixel counts for each array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent exercise: Try to get a good probabilistic floor color model yourself\n",
    "\n",
    "Finally, display the mask in a partially transparent color on top of the image as the video is rendered. How well does it work? You might want to add additional images to your training set. Consider a tool such as hasty.ai to mark up multiple images.\n",
    "\n",
    "With a training set of 19 images, 64 bins for H, 16 bins for S, and 16 bins for V, I got a leave-one-out cross validated test accuracy of around 95%, with an F1 for the floor of 0.98 and an F1 for the obstacles of 0.51. See how well your model works and include this information in your report.\n",
    "\n",
    "We have full code example below. For the persons who use python, let's modify it! :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C++ Full code with segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include \"HeaderFunc.h\"\n",
    "\n",
    "\n",
    "#include <opencv2/core.hpp>\n",
    "#include <opencv2/highgui.hpp>\n",
    "#include <opencv2/imgproc.hpp>\n",
    "#include <iostream>\n",
    "#include <opencv2/videoio.hpp>\n",
    "#include <opencv2/opencv.hpp>\n",
    "\n",
    "using namespace std;\n",
    "using namespace cv;\n",
    "\n",
    "#define VIDEO_FILE \"Lab1-video.mp4\"\n",
    "#define ROTATE true\n",
    "#define HOMOGRAPHY_FILE \"robot-homography.yml\"\n",
    "#define HIST_BINS 32\n",
    "\n",
    "struct Base { };\n",
    "typedef struct  NamedType : Base\n",
    "{\n",
    "    cv::Mat matH;\n",
    "    int widthOut = 0;\n",
    "    int heightOut = 0;\n",
    "    int cPoints = 0;\n",
    "    cv::Point2f aPoints[4];\n",
    "} HomographyData;\n",
    "\n",
    "VideoWriter video;\n",
    "\n",
    "bool readHomography(HomographyData* pHomographyData) {\n",
    "    cv::FileStorage fileStorage(HOMOGRAPHY_FILE, cv::FileStorage::Mode::READ);\n",
    "    if (!fileStorage.isOpened()) {\n",
    "        return false;\n",
    "    }\n",
    "    cv::FileNode points = fileStorage[\"aPoints\"];\n",
    "    cv::FileNodeIterator it = points.begin(), it_end = points.end();\n",
    "    pHomographyData->cPoints = 0;\n",
    "    for (int i = 0; it != it_end; it++, i++) {\n",
    "        (*it) >> pHomographyData->aPoints[i];\n",
    "        pHomographyData->cPoints++;\n",
    "    }\n",
    "    fileStorage[\"matH\"] >> pHomographyData->matH;\n",
    "    fileStorage[\"widthOut\"] >> pHomographyData->widthOut;\n",
    "    fileStorage[\"heightOut\"] >> pHomographyData->heightOut;\n",
    "    fileStorage.release();\n",
    "    return true;\n",
    "}\n",
    "\n",
    "\n",
    "void displayFrame(Mat& matFrameDisplay, Mat img, int iFrame, int cFrames) {\n",
    "    //for (int i = 0; i < pHomographyData->cPoints; i++) {\n",
    "       // cv::circle(matFrameDisplay, pHomographyData->aPoints[i], 10, cv::Scalar(255, 0, 0), 2, cv::LINE_8, 0);\n",
    "   // }\n",
    "    double imgcol = ((double)img.cols / (double)img.rows) * (double)matFrameDisplay.rows;\n",
    "\n",
    "    bool a = true;\n",
    "\n",
    "    //cout << widthOut << \",\" << higthOut << endl;\n",
    "\n",
    "    resize(img, img, Size(imgcol, matFrameDisplay.rows), 0, 0, INTER_CUBIC);\n",
    "\n",
    "    int re = (matFrameDisplay.cols + imgcol);\n",
    "\n",
    "    Mat res(matFrameDisplay.rows, re, CV_64F);\n",
    "\n",
    "\n",
    "    a = false;\n",
    "\n",
    "    hconcat(matFrameDisplay, img, matFrameDisplay);\n",
    "    imshow(VIDEO_FILE, matFrameDisplay);\n",
    "\n",
    "}\n",
    "\n",
    "void reg(cv::Mat matFrameDisplay, Mat& img, HomographyData* pHomographyData) {\n",
    "\n",
    "    warpPerspective(matFrameDisplay, img, pHomographyData->matH, Size(pHomographyData->widthOut, pHomographyData->heightOut), cv::INTER_LINEAR, cv::BORDER_CONSTANT, 0);\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "void getMask(cv::Mat matFrameHSV, cv::Mat& matMask, double aProbFloor[HIST_BINS][HIST_BINS][HIST_BINS], double aProbNotFloor[HIST_BINS][HIST_BINS][HIST_BINS], double pFloor, double pNotFloor) {\n",
    "    matMask = cv::Mat::zeros(matFrameHSV.rows, matFrameHSV.cols, CV_8U);\n",
    "    for (int i = 0; i < matFrameHSV.rows; i++) {\n",
    "        for (int j = 0; j < matFrameHSV.cols; j++) {\n",
    "            cv::Vec3b hsv = matFrameHSV.at<cv::Vec3b>(i, j);\n",
    "            int Hbin = (int)hsv[0] * HIST_BINS / 256;\n",
    "            int Sbin = (int)hsv[1] * HIST_BINS / 256;\n",
    "            int Vbin = (int)hsv[2] * HIST_BINS / 256;\n",
    "            if (aProbFloor[Hbin][Sbin][Vbin] > 0.001 && aProbFloor[Hbin][Sbin][Vbin] * pFloor > aProbNotFloor[Hbin][Sbin][Vbin] * pNotFloor) {\n",
    "                matMask.at<uchar>(i, j) = 255;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "bool getFloorHist(cv::Mat matFrame, double (&aProbFloorHS)[HIST_BINS][HIST_BINS][HIST_BINS], double (&aProbNotFloorHS)[HIST_BINS][HIST_BINS][HIST_BINS], double& pFloor, double& pNotFloor) {\n",
    "\n",
    "    // stack mask picture\n",
    "    cv::String pathmask(\"image/CV Class Collaborate/mask/*.png\"); //select only jpg\n",
    "    vector<cv::String> fnmask;\n",
    "    vector<cv::Mat> dataMask;\n",
    "    cv::glob(pathmask, fnmask, true); // recurse\n",
    "    for (size_t k = 0; k < fnmask.size(); ++k)\n",
    "    {\n",
    "        cv::Mat im = cv::imread(fnmask[k]);\n",
    "        if (im.empty()) continue; //only proceed if sucsessful\n",
    "        // you probably want to do some preprocessing\n",
    "        dataMask.push_back(im);\n",
    "    }\n",
    "    // stack original picture\n",
    "    cv::String pathImg(\"image/CV Class Collaborate/original/*.png\"); //select only jpg\n",
    "    vector<cv::String> fnImg;\n",
    "    vector<cv::Mat> dataImg;\n",
    "    cv::glob(pathImg, fnImg, true); // recurse\n",
    "    for (size_t k = 0; k < fnImg.size(); ++k)\n",
    "    {\n",
    "        cv::Mat im = cv::imread(fnImg[k]);\n",
    "        if (im.empty()) continue; //only proceed if sucsessful\n",
    "        // you probably want to do some preprocessing\n",
    "        Mat temppp;\n",
    "        cv::cvtColor(im, temppp, cv::COLOR_BGR2HSV);\n",
    "        dataImg.push_back(temppp);\n",
    "    }\n",
    "\n",
    "    // Read mask.png and accumulate HS values of the floor pixels\n",
    "       //cv::Mat matMask = cv::imread(\"mask.png\");\n",
    "    for (int i = 0; i < HIST_BINS; i++) {\n",
    "        for (int j = 0; j < HIST_BINS; j++) {\n",
    "            for (int k = 0; k < HIST_BINS; k++) {\n",
    "\n",
    "                aProbFloorHS[i][j][k] = 0;\n",
    "                aProbNotFloorHS[i][j][k] = 0;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    // start cal prob\n",
    "    if (dataImg.size() == dataMask.size()) {\n",
    "        int cFloor = 0;\n",
    "        int cNotFloor = 0;\n",
    "        for (size_t index = 0; index < dataImg.size(); index++)\n",
    "        {\n",
    "            Mat matMaskGray;\n",
    "            cvtColor(dataMask[index], matMaskGray, COLOR_BGR2GRAY);\n",
    "\n",
    "            for (int i = 0; i < matMaskGray.rows; i++) {\n",
    "                for (int j = 0; j < matMaskGray.cols; j++) {\n",
    "                    //cout << i << \",\" << j << endl;\n",
    "                    bool bMasked = matMaskGray.at<uchar>(i, j) > 0;\n",
    "                    cv::Vec3b hsv = dataImg[index].at<cv::Vec3b>(i, j);\n",
    "                    int Hbin = (int)hsv[0] * HIST_BINS / 256;\n",
    "                    int Sbin = (int)hsv[1] * HIST_BINS / 256;\n",
    "                    int Vbin = (int)hsv[2] * HIST_BINS / 256;\n",
    "                    //cout << Hbin << \",\" << Sbin << endl;\n",
    "                    if (bMasked) {\n",
    "                        aProbFloorHS[Hbin][Sbin][Vbin] += 1.0;\n",
    "                        cFloor++;\n",
    "                    }\n",
    "                    else {\n",
    "                        aProbNotFloorHS[Hbin][Sbin][Vbin] += 1.0;\n",
    "                        cNotFloor++;\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "        }\n",
    "        for (int i = 0; i < HIST_BINS; i++) {\n",
    "            for (int j = 0; j < HIST_BINS; j++) {\n",
    "                for (int k = 0; k < HIST_BINS; k++) {\n",
    "                    aProbFloorHS[i][j][k] /= (double)cFloor;\n",
    "                    aProbNotFloorHS[i][j][k] /= (double)cNotFloor;\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        pFloor = (double)cFloor / (cFloor + cNotFloor);\n",
    "        pNotFloor = (double)cNotFloor / (cFloor + cNotFloor);\n",
    "        return true;\n",
    "        \n",
    "    }\n",
    "    else {\n",
    "        return false;\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "void maskFrame(Mat matFrame, Mat matMask) {\n",
    "    for (int i = 0; i < matFrame.rows; i++) {\n",
    "        for (int j = 0; j < matFrame.cols; j++) {\n",
    "            if (matMask.at<uchar>(i, j) > 0) {\n",
    "                cv::Vec3b bgr = matFrame.at<cv::Vec3b>(i, j);\n",
    "                bgr[1] = (uchar)(0.5 * bgr[1] + 0.5 * 255);\n",
    "                matFrame.at<cv::Vec3b>(i, j) = bgr;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "\n",
    "    cv::Mat matFrameCapture;\n",
    "    cv::Mat matFrameDisplay;\n",
    "    cv::Mat matFrameDisplayHomo;\n",
    "    int cFrames;\n",
    "    HomographyData homographyData;\n",
    "    cv::Mat matFrameHSV;\n",
    "    cv::Mat matFrameHSVHomo;\n",
    "\n",
    "\n",
    "    double aProbFloorHS[HIST_BINS][HIST_BINS][HIST_BINS];\n",
    "    double aProbNotFloorHS[HIST_BINS][HIST_BINS][HIST_BINS];\n",
    "\n",
    "\n",
    "    double pFloor, pNotFloor;\n",
    "\n",
    "    // Open input video file\n",
    "\n",
    "\n",
    "    cv::VideoCapture videoCapture(VIDEO_FILE);\n",
    "    if (!videoCapture.isOpened()) {\n",
    "        cerr << \"ERROR! Unable to open input video file \" << VIDEO_FILE << endl;\n",
    "        return -1;\n",
    "    }\n",
    "    cFrames = (int)videoCapture.get(cv::CAP_PROP_FRAME_COUNT);\n",
    "\n",
    "    // Create a named window that will be used later to display each frame\n",
    "\n",
    "    cv::namedWindow(VIDEO_FILE, (unsigned int)cv::WINDOW_NORMAL | cv::WINDOW_KEEPRATIO | cv::WINDOW_GUI_EXPANDED);\n",
    "    //cv::namedWindow(\"Floor Mask\", (unsigned int)cv::WINDOW_NORMAL | cv::WINDOW_KEEPRATIO | cv::WINDOW_GUI_EXPANDED);\n",
    "    // Read homography from file\n",
    "\n",
    "    VideoWriter video(\"Segmentation.avi\", cv::VideoWriter::fourcc('M', 'J', 'P', 'G'), 30, Size(3840, 1080), true);\n",
    "\n",
    "    if (!readHomography(&homographyData)) {\n",
    "        cerr << \"ERROR! Unable to read homography data file \" << HOMOGRAPHY_FILE << endl;\n",
    "        return -1;\n",
    "    }\n",
    "\n",
    "    // Capture loop\n",
    "    int iFrame = 0;\n",
    "    while (true) {\n",
    "        // Block for next frame\n",
    "        videoCapture.read(matFrameCapture);\n",
    "        if (matFrameCapture.empty()) {\n",
    "            // End of video file\n",
    "            break;\n",
    "        }\n",
    "\n",
    "\n",
    "        matFrameDisplay = matFrameCapture;\n",
    "        matFrameDisplayHomo = matFrameCapture.clone();\n",
    "\n",
    "        cv::cvtColor(matFrameDisplay, matFrameHSV, cv::COLOR_BGR2HSV);\n",
    "        cv::cvtColor(matFrameDisplayHomo, matFrameHSVHomo, cv::COLOR_BGR2HSV);\n",
    "        if (iFrame == 0) {\n",
    "            if (!getFloorHist(matFrameHSV, aProbFloorHS, aProbNotFloorHS, pFloor, pNotFloor)) {\n",
    "                return -1;\n",
    "            }\n",
    "        }\n",
    "        cv::Mat matMask;\n",
    "        cv::Mat matMaskHomo;\n",
    "        Mat imgHSV;\n",
    "        Mat img;\n",
    "\n",
    "        Mat matFrameoptic = matFrameDisplay.clone();\n",
    "\n",
    "        int countf = 0;\n",
    "        //================================ mark original =====================================\n",
    "\n",
    "        getMask(matFrameHSV, matMask, aProbFloorHS, aProbNotFloorHS, pFloor, pNotFloor);\n",
    "\n",
    "        maskFrame(matFrameDisplay, matMask);\n",
    "        //================================ mark Homo =====================================\n",
    "        reg(matFrameHSVHomo, imgHSV, &homographyData);\n",
    "        reg(matFrameDisplayHomo, img, &homographyData);\n",
    "        getMask(imgHSV, matMaskHomo, aProbFloorHS, aProbNotFloorHS, pFloor, pNotFloor);\n",
    "        maskFrame(img, matMaskHomo);\n",
    "\n",
    "\n",
    "        //displayFrame(matFrameDisplay, img, iFrame, cFrames);\n",
    "        //============================================= Display horizontal image ====================================================\n",
    "        double imgcol = ((double)img.cols / (double)img.rows) * (double)matFrameDisplay.rows;\n",
    "        resize(img, img, Size(imgcol, matFrameDisplay.rows), 0, 0, INTER_CUBIC);\n",
    "        int re = (matFrameDisplay.cols + imgcol);\n",
    "        Mat res(matFrameDisplay.rows, re, CV_64F);\n",
    "        hconcat(matFrameDisplay, img, matFrameDisplay);\n",
    "        imshow(VIDEO_FILE, matFrameDisplay);\n",
    "        \n",
    "   /*     if (iFrame ==0) {\n",
    "            imshow(VIDEO_FILE, matFrameDisplay);\n",
    "            waitKey(0);\n",
    "        }*/\n",
    "        \n",
    "\n",
    "        video.write(matFrameDisplay);\n",
    "\n",
    "        int iKey;\n",
    "        do {\n",
    "            iKey = cv::waitKey(1);\n",
    "            if (getWindowProperty(VIDEO_FILE, cv::WND_PROP_VISIBLE) == 0) {\n",
    "                return 0;\n",
    "            }\n",
    "            if (iKey == (int)'q' || iKey == (int)'Q') {\n",
    "                return 0;\n",
    "            }\n",
    "        } while (iKey == (int)' ');\n",
    "\n",
    "        iFrame++;\n",
    "        cout << iFrame << \"  From  \" << cFrames << endl;\n",
    "    }\n",
    "    video.release();\n",
    "    destroyAllWindows();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Segmentation\n",
    "\n",
    "We've seen that color-based segmentation in HSV space using a generative machine learning model can be effective but has limitations when the objects to be segmented from the background have color distributions similar to the background. The best accuracy we could get for our floor/obstacle model last week was around 95%, but hat was with occasional errors grouping in large enough regions that would preclude our robot from navigating safely in the indoor environment.\n",
    "\n",
    "Semantic segmentation attempts to address this issue using a more sophisticated model to separate the scene into its constituent regions more effectively.\n",
    "\n",
    "Typical semantic segmentation models typically use a lot of resources. For example, the state of the art models published for the MIT ADE20K dataset in their [GitHub repository](https://github.com/CSAILVision/semantic-segmentation-pytorch) are very accurate and run at 2-17 FPS on a NVIDIA Pascal Titan Xp GPU. They also run well on a GTX 1080TI. But on an i7 CPU, I found that they take 9-11 SECONDS PER FRAME, and on an NVIDIA Jetson Nano's GPU, they take 12-42 SEDONDS PER FRAME!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lighter semantic segmentation models\n",
    "\n",
    "We would like to experiment with some semantic segmenation models that have a hope of running in real time on a small embedded system such as the Jetson Nano.\n",
    "\n",
    "NVIDIA has published a very useful repository <link>[Jetson Inference](https://github.com/dusty-nv/jetson-inference)</link> that contains versions of two semantic segmentation models: SegNet and UNet.\n",
    "\n",
    "I ran the SegNet model in this repository using FCN-ResNet18 trained on Pascal VOC with 320x320 input images. It takes a while to load the model into memory and so on, but inference time once all is ready is very fast: less than 70 ms.\n",
    "\n",
    "So it's fast! Unfortunately, it doesn't work particularly well out of the box:\n",
    "\n",
    "<img src=\"img/lab03-3.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we shouldn't expect it to, considering that the Pascal VOC dataset \"only\" contains 20 classes plus the \"background\" (the black label), and others of which are \"bottle\" (the purple label) and \"person\" (the tan label).\n",
    "\n",
    "That's on the NVIDIA Jetson Nano. The model itself was built with PyTorch, but it has been exported in ONNX (Open Neural Network Exchange) format, and the Jetson Nano executes it on TensorRT. You can download the <link>[ONNX model from AIT](https://www.cs.ait.ac.th/~mdailey/class/vision/fcn_resnet18.onnx)</link> (I just copied it from the excellent Jetson Inference repository).\n",
    "\n",
    "If you'd like to understand the structure of the model represented in this ONNX file, download the <link>[Netron 4.3.4 AppImage for Linux](https://github.com/lutzroeder/netron)</link>, run the AppImage, and load the file. You'll see that it\n",
    "\n",
    "1. Takes as input a 320x320 3-channel image\n",
    "2. Performs 64 7x7 convolutions\n",
    "3. Does batch normalization, ReLU, and MaxPool\n",
    "4. Runs a residual block with the following\n",
    "    1. 64 3x3 convolutions then BN then ReLU\n",
    "    2. 64 3x3 convolutions then BN\n",
    "5. ReLU\n",
    "6. Residual block (same structure as above)\n",
    "7. ReLU\n",
    "8. Residual block with the following\n",
    "    1. 128 3x3 convolutions then BN then ReLU\n",
    "    2. 128 3x3 convolutions then BN\n",
    "    3. Residual add using 128 1x1 convs to expand feature map\n",
    "9. Etc. (several more residual blocks and downscaling by a factor of 32)\n",
    "10. Final 21 1x1 convolution to obtain output 1x21x10x10 tensor\n",
    "\n",
    "Since this model is relatively small and at least generates some output, let's see if we can get it running on our OpenCV video stream. A model that runs nice and fast on an Intel CPU and on a Jetson GPU, in C++/OpenCV as well as Python, would be perfect.\n",
    "\n",
    "<img src=\"img/lab03-10.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a segmentation model using OpenCV DNN\n",
    "\n",
    "Let's see if we can get this ONNX model running in OpenCV with its DNN functionality.\n",
    "\n",
    "First, let's initialize with the Pascal VOC classes and read an image:\n",
    "\n",
    "### C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string aStringClasses[] = {\n",
    "    \"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\n",
    "    \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
    "};\n",
    "\n",
    "cv::Vec3b aColorClasses[] = {\n",
    "        { 0, 0, 0 }, { 255, 0, 0 }, { 0, 255, 0 }, { 0, 255, 120 }, { 0, 0, 255 }, { 255, 0, 255 }, { 70, 70, 70 },\n",
    "        { 102, 102, 156 }, { 190, 153, 153 }, { 180, 165, 180 }, { 150, 100, 100 }, { 153, 153, 153 },\n",
    "        { 250, 170, 30 }, { 220, 220, 0 }, { 107, 142, 35 }, { 192, 128, 128 }, { 70, 130, 180 }, { 220, 20, 60 },\n",
    "        { 0, 0, 142 }, { 0, 0, 70 }, { 119, 11, 32 }\n",
    "};\n",
    "\n",
    "int nClasses = sizeof(aColorClasses) / 3;\n",
    "\n",
    "// Read CNN definition\n",
    "\n",
    "auto net = cv::dnn::readNetFromONNX(ONNX_NETWORK_DEFINITION);\n",
    "\n",
    "// Read input image\n",
    "\n",
    "cv::Mat matFrame = cv::imread(IMAGE_FILE);\n",
    "if (matFrame.empty()) {\n",
    "    cerr << \"Cannot open image file \" << IMAGE_FILE << endl;\n",
    "    return -1;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aStringClasses = [\n",
    "    \"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\n",
    "    \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
    "]\n",
    "\n",
    "aColorClasses = [\n",
    "        ( 0, 0, 0 ), ( 255, 0, 0 ), ( 0, 255, 0 ), ( 0, 255, 120 ), ( 0, 0, 255 ), ( 255, 0, 255 ), ( 70, 70, 70 ),\n",
    "        ( 102, 102, 156 ), ( 190, 153, 153 ), ( 180, 165, 180 ), ( 150, 100, 100 ), ( 153, 153, 153 ),\n",
    "        ( 250, 170, 30 ), ( 220, 220, 0 ), ( 107, 142, 35 ), ( 192, 128, 128 ), ( 70, 130, 180 ), ( 220, 20, 60 ),\n",
    "        ( 0, 0, 142 ), ( 0, 0, 70 ), ( 119, 11, 32 )\n",
    "]\n",
    "\n",
    "nClasses = len(aColorClasses)\n",
    "\n",
    "# Read CNN definition\n",
    "net = cv2.dnn.readNetFromONNX(cv2.ONNX_NETWORK_DEFINITION)\n",
    "\n",
    "# Read input image\n",
    "matFrame = cv2.imread(IMAGE_FILE);\n",
    "if (matFrame is None):\n",
    "    print(\"Cannot open image file \", IMAGE_FILE)\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run an image through the network\n",
    "\n",
    "Once the input image is preprocessed to form a tensor suitable for input to our DNN model, we can just set the input layer of the network to point to the newly preprocessed image tensor, then we can do a forward pass through the network model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Propagate the matInputTensor through the FCN model\n",
    "\n",
    "net.setInput(matInputTensor);\n",
    "cv::Mat matScore = net.forward();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propagate the matInputTensor through the FCN model\n",
    "net.setInput(matInputTensor)\n",
    "matScore = net.forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK but how to do that preprocessing before we feed the image to the model?\n",
    "\n",
    "Here's the thing: most CNN models (even object detection and image segmentation models) are based on a classification model as the \"backbone\" of the model. In the case of FCN-ResNet-18, the backbone is of course ResNet-18.\n",
    "\n",
    "Usually, training a model based on a classifier begins by loading weights trained for classification on ImageNet or another dataset then further training and/or \"fine tuning\" the model on a more specific dataset. We do this because we don't want to spend the week of GPU time it takes to get a model that analyzes image edges, puts them together into higher-level shapes, and gradually extracts a set of coarsely localized features that describe objects of interest.\n",
    "\n",
    "When data scientists train a model on ImageNet, they almost always perform a few common steps:\n",
    "\n",
    "1. Scale the input image's R, G, and B intensities (normally in the range 0-255) to the range 0-1.\n",
    "2. Scale the input image to the size needed for the classifier, or sample a patch from the input with the size required by the classifier.\n",
    "3. Subtract expected mean values for the R, G, and B channels. The magic values for ImageNet are 0.485 for R, 0.456 for G, and 0.406 for B.\n",
    "4. Divide by expected standard deviations for the R, G, and B channels. The magic values for ImageNet are 0.229 for R, 0.224 for G, and 0.225 for B.\n",
    "\n",
    "There is an OpenCV function <code>cv::dnn::blobFromImage()</code> that performs some of these steps but not all. Check the documentation and add the necessary code to prepare the image for presentation for the pre-trained network.\n",
    "\n",
    "Next, you'll want to use the result of the semantic segmentation model to color the input image and display for examination by the user, and perhaps add some information about CPU time required for the inference:\n",
    "\n",
    "### C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Colorize the image and display\n",
    "\n",
    "cv::Mat matColored;\n",
    "colorizeSegmentation(matFrame, matScore, matColored, aColorClasses, aStringClasses, nClasses);\n",
    "\n",
    "// Add timing information\n",
    "\n",
    "std::vector<double> layersTimes;\n",
    "double freq = cv::getTickFrequency() / 1000;\n",
    "double t = net.getPerfProfile(layersTimes) / freq;\n",
    "std::string label = cv::format(\"Inference time: %.2f ms\", t);\n",
    "cv::putText(matColored, label, cv::Point(10,30),\n",
    "        cv::FONT_HERSHEY_SIMPLEX, 1.0, cv::Scalar(0, 255, 0));\n",
    "\n",
    "// Display\n",
    "\n",
    "cv::namedWindow(WINDOW_NAME, WINDOW_FLAGS);\n",
    "cv::imshow(WINDOW_NAME, matColored);\n",
    "cv::waitKey(0);\n",
    "\n",
    "return 0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colorize the image and display\n",
    "\n",
    "matColored = cv2.colorizeSegmentation(matFrame, matScore, aColorClasses, aStringClasses, nClasses)\n",
    "\n",
    "# Add timing information\n",
    "layersTimes = 0\n",
    "freq = cv2.getTickFrequency() / 1000;\n",
    "t = net.getPerfProfile(layersTimes) / freq;\n",
    "label = \"Inference time: \" + str(t) + \" ms\"\n",
    "cv2.putText(matColored, label, (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0))\n",
    "\n",
    "# Display\n",
    "cv2.namedWindow(WINDOW_NAME, WINDOW_FLAGS)\n",
    "cv2.imshow(WINDOW_NAME, matColored)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll have to figure out colorizeSegmentation for yourself. See if you can get a result similar to the image above. It won't be exactly the same, as the jetson inference code scales the input image slightly differently from <code>blobFromImage()</code>. I got the following:\n",
    "\n",
    "<img src=\"img/lab03-11.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python example code file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os.path\n",
    "from os import path\n",
    "import sys\n",
    "\n",
    "\n",
    "def colorizeSegmentation_Naja(matFrame, matScore, aColorClasses, aStringClasses, nClasses):\n",
    "    colorizedFrame = matFrame.copy()\n",
    "    # print(matScore)\n",
    "    # print(\"====================================================\")\n",
    "    #https://en.wikipedia.org/wiki/Sigmoid_function\n",
    "    scores = 1.0/ (1.0+np.exp(-matScore))\n",
    "    # print(scores[0])\n",
    "    # print('matScore size:', matScore.shape)\n",
    "    qq = scores[0 ,  0: ,:,:]\n",
    "    # print(\"====================================================\")\n",
    "    # classeses = qq.argmax(0) + 1 #find max from matrix 10*10 of each class\n",
    "    classes = np.argmax(qq,axis=0) #find max from matrix 10*10 of each class\n",
    "    # print(\"====================================================\")\n",
    "    # print(classes)\n",
    "    # print(classeses)\n",
    "    color_mask = np.zeros((3,40,40))\n",
    "    for iClass in range(nClasses):\n",
    "        mask = classes ==iClass\n",
    "        # print(mask)\n",
    "        colors = np.array(( aColorClasses[iClass][0] * np.ones((40,40)),\n",
    "                            aColorClasses[iClass][1] * np.ones((40,40)),\n",
    "                            aColorClasses[iClass][2] * np.ones((40,40)) ))\n",
    "        color_mask += mask * colors\n",
    "        # print(aColorClasses[iClass][0]  * np.ones((1,10,10)) )\n",
    "        # print('Class = ',iClass,\" Max = \",scores[0,iClass,:,:].max()  )\n",
    "        \n",
    "    # print(color_mask)\n",
    "    ii = color_mask.transpose(1,2,0) # postion define (1,2,0) 1 = change position 1 to 0\n",
    "    # print(ii)\n",
    "    color_mask = cv2.resize(  color_mask.transpose(1,2,0) , (matFrame.shape[1],matFrame.shape[0]) ,cv2.INTER_NEAREST)\n",
    "    colorizedFrame = 0.5*matFrame+ 0.5*color_mask\n",
    "    colorizedFrame = np.array(colorizedFrame ,dtype=np.uint8)\n",
    "    return colorizedFrame\n",
    "\n",
    "def f_fcn_resnet50(matFrame , net):\n",
    "    # ===================================================\n",
    "    tensorframe = cv2.dnn.blobFromImage(matFrame, IMAGE_SCAleFactor, (320,320), IMAGENET_MEAN , True,False )\n",
    "    # Propagate the matInputTensor through the FCN model\n",
    "    net.setInput(tensorframe)\n",
    "    matScore = net.forward()\n",
    "\n",
    "    matColored=colorizeSegmentation_Naja(matFrame, matScore, aColorClasses, aStringClasses, nClasses)\n",
    "    # Add timing information\n",
    "    freq = cv2.getTickFrequency() / 1000\n",
    "    t, layersTimes = net.getPerfProfile()\n",
    "    layersTimes /= freq\n",
    "\n",
    "    label = \"Inference time: \" + str(layersTimes[0]) + \" ms\"\n",
    "    cv2.putText(matColored, label, (20,70), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 2)\n",
    "\n",
    "    return matColored\n",
    "\n",
    "#=================== variable ====================================\n",
    "#Prepare blob for input image\n",
    "#ImageNet 0= (color/255  - mean) / std\n",
    "#Opencv 0 =(color - mean) * scaleFactor\n",
    "#mean must be (0.485 0.456 0.406) *255\n",
    "#scaleFactor must be 1/255.0/std where std is avg of 0.229 0.224 0.225\n",
    "IMAGENET_MEAN = (123.67, 116.28, 103.53)\n",
    "IMAGE_SCAleFactor = (1.0/255.0/0.226)\n",
    "IMAGE_SCAleFactor = (1.0/(255.0*0.226) )\n",
    "WINDOW_NAME = \"fcn_resnet50\"\n",
    "VIDEO_FILE = \"Lab04-robot-video.mp4\"\n",
    "\n",
    "aStringClasses = [\n",
    "    \"obstacle\", \"floor\"\n",
    "]\n",
    "aColorClasses = [\n",
    "         ( 255, 0, 0 ), ( 0, 255, 0 )\n",
    "]\n",
    "\n",
    "nClasses = len(aColorClasses)\n",
    "\n",
    "\n",
    "#==================================================================\n",
    "\n",
    "\n",
    "# Read CNN definition\n",
    "net = cv2.dnn.readNetFromONNX(\"fcn_resnet50.onnx\")\n",
    "\n",
    "\n",
    "# IMAGE_FILE  = \"original/frame-015.png\"\n",
    "\n",
    "print(\"Strat\")\n",
    "videoCapture = cv2.VideoCapture(VIDEO_FILE)\n",
    "_num = 0\n",
    "FirstFrame =True\n",
    "cv2.namedWindow(WINDOW_NAME, cv2.WINDOW_GUI_EXPANDED)\n",
    "\n",
    "width  = videoCapture.get(cv2.CAP_PROP_FRAME_WIDTH)   # float `width`\n",
    "height = videoCapture.get(cv2.CAP_PROP_FRAME_HEIGHT)  # float `height`\n",
    "\n",
    "size = ((int)(width)*2, (int)(height))\n",
    "fourcc  = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "# video   = cv2.VideoWriter( 'ATC_LKPR_output.avi', fourcc, 30, size ) # fps = 30, size = ( 1024, 512 )\n",
    "result = cv2.VideoWriter('Lab4Python.avi' , fourcc , 30 , size)\n",
    "FirstFrame =True\n",
    "\n",
    "#============================= Homo============================\n",
    "H_n =[]\n",
    "if path.exists(\"homography.yml\"):\n",
    "    #================================================= Load homography to YML ==============================================\n",
    "    cv_file = cv2.FileStorage(\"homography.yml\", cv2.FILE_STORAGE_READ)\n",
    "    matrix = cv_file.getNode(\"homo\").mat()\n",
    "    cv_file.release()\n",
    "    H_n = matrix\n",
    "    print(\"Estimated Homography Matrix is:\")\n",
    "    print(H_n)\n",
    "    print(\"===================================================\")\n",
    "else:\n",
    "    sys.exit(\"Cannot fond files YML Naja\") \n",
    "#==============================================================\n",
    "if not videoCapture.isOpened():\n",
    "    sys.exit(\"ERROR! Unable to open input video file \", VIDEO_FILE) \n",
    "\n",
    "while (True):  \n",
    "    ## Read input image\n",
    "    # matFrame = cv2.imread(IMAGE_FILE)\n",
    "    # if (matFrame is None):\n",
    "    #     print(\"Cannot open image file \", IMAGE_FILE)\n",
    "    ret, matFrame = videoCapture.read()\n",
    "\n",
    "    if matFrame is None:   # no more frame capture from the video\n",
    "        break\n",
    "    \n",
    "    # perspective transform operation using transform matrix\n",
    "    h, w, ch = matFrame.shape\n",
    "    \n",
    "    matColored = f_fcn_resnet50( matFrame=matFrame , net=net)\n",
    "    matHomo = cv2.warpPerspective(matColored, H_n, (w, h), cv2.INTER_LINEAR)\n",
    "\n",
    "    if ret == True:\n",
    "        R_img = cv2.hconcat([matColored, matHomo])\n",
    "        result.write(R_img)\n",
    "\n",
    "    # Display\n",
    "    if FirstFrame:\n",
    "        R_img = cv2.hconcat([matColored, matHomo])\n",
    "        cv2.imshow(WINDOW_NAME, R_img)\n",
    "        cv2.waitKey(0)\n",
    "        FirstFrame = False\n",
    "\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    _num = _num+1\n",
    "    print(\"Number of Frame = \",_num)\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Finish\")\n",
    "result.release()\n",
    "videoCapture.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning on our own dataset\n",
    "\n",
    "Since the stock model was trained on VOC 2012's 21 classes, it is unable to give a recognizable segmentation on our data set.\n",
    "So here, we will want to fine-tune the FCN-ResNet-18 model on our own floor/obstacle dataset. We will load the existing weights, throw away the 21-class output layer of the existing model, and replace it with our own two-class output layer. We'll keep the weights for all but this last layer, then start training on our dataset.\n",
    "\n",
    "Also, since we will be training on a medium-sized dataset (VOC), we should set up our machine learning model development environment to use a powerful GPU server rather than our poor little laptops.\n",
    "\n",
    "## Torchvision versioning\n",
    "\n",
    "The standard PyTorch Vision module does not have FCN-ResNet-18 -- it only has the larger models like FCN-ResNet-50.\n",
    "\n",
    "The original FCN-ResNet-18 patch for torchvision 0.3.0 is at [this GitHub repository](https://github.com/dusty-nv/vision).\n",
    "\n",
    "By now, depending on your environment, you may be running torchvision 0.6.0, 0.9.1, or higher.\n",
    "\n",
    "If you want to patch your local install of torchvision 0.9.1 for FCN-ResNet-18, [try this Gist](https://gist.github.com/cheadrian/f8ea250d78c2bb9bc913aa89f18f8e21).\n",
    "This patch also works for torchvision 0.6.0 to 0.9.0 but you'll also need the segmentation model code\n",
    "in `lraspp.py`, `mobilenetv2.py`, and `mobilenetv3.py` from version 0.9.1.\n",
    "\n",
    "Luckily, we've already set this up for you on the DS&AI GPU server. See below.\n",
    "\n",
    "## GPU server setup\n",
    "\n",
    "This lab will work fine on the DS&AI JupyterHub\n",
    "server. [Here is the login for AIT](https://puffer.cs.ait.ac.th/hub/login?next=).\n",
    "Use the `CV-Class` image.\n",
    "\n",
    "You might instead want to create a custom environment according to\n",
    "[the RTML GPU setup guide](https://github.com/dsai-asia/RTML/blob/main/Labs/01-Setup/01-Setup.ipynb),\n",
    "but that would require quite a bit of additional work!\n",
    "\n",
    "## Training Scripts\n",
    "\n",
    "Download the [training scripts and data for this lab](https://drive.google.com/file/d/1ihaFWQLTsFPpzAfAHfhrt1cR4WEwhH7r/view).\n",
    "\n",
    "Move the code and data into your project directory and take a look at `train.py` and the code it uses.\n",
    "The code was originally from torchvision then modified by Dustin Franklin at NVIDIA for smaller models like\n",
    "FCN-ResNet-18.\n",
    "\n",
    "The floor data are from an earlier experiment using a high-end mobile phone camera attached to the robot. The autofocus and depth of field are\n",
    "much better on this magic camera than on the Raspberry Pi camera we use with the Jetson Nano. Once you get the training working, you'll want to\n",
    "replace or augment this dataset with the dataset we created in Lab 03.\n",
    "\n",
    "## Train FCN-ResNet-18 from scratch on Pascal VOC\n",
    "\n",
    "Use train.py to learn an initial model from the Pascal VOC 2012 (21 class) dataset. You'll want to train for about 100 epochs and take the model with the best IoU on the validation set. Expect about 56% IoU.\n",
    "\n",
    "The standard location of the VOC trainval dataset is at Oxford. To have `train.py` try to download it for you, uncomment line 99 and comment out line 100.\n",
    "However, we find that the Oxford site has been unavailable recently. If you find it so, you can also manually download VOC from\n",
    "[Joseph Redmond's old mirror](https://pjreddie.com/projects/pascal-voc-dataset-mirror/). You want the VOC 2012 train/validation dataset. Unpack it in your data\n",
    "directory.\n",
    "\n",
    "With that, you should be able to run `train.py`. Check the options. You can change batch size, number of workers, etc. to get the best runtime\n",
    "on your GPU. The model weights after each iteration as well as the best model will be saved in your working directory.\n",
    "\n",
    "## Transfer learning\n",
    "\n",
    "Once we have a good model on VOC, let's fine tune on our robot floor dataset.\n",
    "Take a look at <code>retrain.py</code>. This script loads the pre-trained FCN-ResNet-18 we built in the previous step and fine tunes it on the robot floor data set.\n",
    "\n",
    "Note that the fine tuning script expects your best VOC model weights to be in a file called `fcn_resnet18_voc_best.pth`.\n",
    "\n",
    "Take a look at the code and play around with it, only fine tuning the fresh output layer or also tuning the layers already trained on VOC. You'll also want to experiment with the relative weighting of floor vs. obstacle classes in the loss function. If you don't give obstacles (the rare class) a high weight, the model\n",
    "may learn to classify everything as floor!\n",
    "\n",
    "I didn't put the code to save the model weights here. To get your model saved, read and understand the two scripts `train.py` and `fine_tune.py` and add the code to save your model to the fine tuning script.\n",
    "\n",
    "Give some thought to what criteria you should use for the \"best model so far.\" If you only cared about per-pixel accuracy, you would probably just classify all pixels as \"floor.\" What you probably want to maximize is the mean of the IoU for the two classes.\n",
    "\n",
    "## Export to ONNX and run under OpenCV DNN\n",
    "\n",
    "Once that's all working, export the model to ONNX using the provided script and see if you can get it running on OpenCV DNN.\n",
    "\n",
    "## Put it all together!\n",
    "\n",
    "Finally, you should be able to display, frame by frame, the input image with floor/non-floor pixels identified and a birds-eye view map of the\n",
    "obstacle-free space around the robot (using the homography you saved in Lab 02 or 03).\n",
    "\n",
    "## Try another video\n",
    "\n",
    "[Here's another video from the same environment](https://drive.google.com/file/d/1LKH5zPhZRPKSHF287apsaOL5ZMN3c7JB/view?usp=sharing)\n",
    "taken during the daytime, with different lighting and reflection challenges. Does your model work? Probably not!\n",
    "\n",
    "## The report\n",
    "\n",
    "Post a video of your result to Piazza before the next lab, and turn in a report describing your experiments and results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
