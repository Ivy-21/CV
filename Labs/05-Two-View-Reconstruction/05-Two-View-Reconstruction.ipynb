{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision, Lab 5: Two-View Reconstruction in the Real World\n",
    "\n",
    "Today we'll take a look at how to perform real-world 3D reconstruction of a scene using point correspondences between two calibrated views of that scene.\n",
    "\n",
    "The theory we explored in lecture thus far has had one glaring omission: how to get correspondences between two or more views of a scene?\n",
    "\n",
    "We'll explore keypoint detection and matching, estimating the essential matrix, estimating the camera rotation and translation, and resolving the scale ambiguity using extrinsic camera parameters.\n",
    "\n",
    "## Data needed for the lab\n",
    "\n",
    "**Alternative 1**: Use the camera you calibrated in Lab 03 to get two views of the same scene of interest. Make sure the scene is not purely planar.\n",
    "\n",
    "**Alternative 2**: Use one of the two home robot navigation videos we've already worked with. You may also want to just use the sequence of frames\n",
    "we segmented from. You'll also need to calibrate the camera to get the intrinsic parameters of the camera.\n",
    "- [Video 1 (nighttime, LED lighting)](https://drive.google.com/file/d/1K2EjcMJifDUOkSP_amlg8wcHmv_jh44V/view?usp=sharing)\n",
    "- [Video 2 (daytime, ambient lighting)](https://drive.google.com/file/d/1LKH5zPhZRPKSHF287apsaOL5ZMN3c7JB/view?usp=sharing)\n",
    "- [Segmented frames from video 1](https://drive.google.com/drive/folders/1V0GyVhnrO9NgXLRzJNFVpLOqFJJScUg2?usp=sharing)\n",
    "- [Calibration images for the camera used for all video/image data](https://github.com/dsai-asia/CV/tree/master/Labs/05-Calibration/sample-calib-images-jetson-rpicam)\n",
    "\n",
    "## Feature matching: AKAZE vs. ORB?\n",
    "\n",
    "We'll look at two feature point matchers today. They are both similar to the original idea\n",
    "of wide baseline matching with SIFT, first invented by David Lowe at the University of British Colombia around 2000.\n",
    "SIFT (and its faster successor SURF) are free for academic or individual use, but they are patent protected, so you have\n",
    "to license the algorithms if you want to make money with them! For that reason, the OpenCV community has implemented quite\n",
    "a few other feature point detectors and matchers so you have wide range of choices that are patent-free. We'll look at\n",
    "AKAZE and ORB a bit.\n",
    "\n",
    "Reference: [Comparing ORB and AKAZE for visual odometry\n",
    "of unmanned aerial vehicles](http://www.epacis.net/ccis2016/papers/paper_121.pdf)\n",
    "\n",
    "In **ORB**, the detection step is based on the FAST keypoint detector,\n",
    "which is an efficient corner detector suitable for real-time applications due\n",
    "to its computation properties. Since FAST does not include an orientation\n",
    "operator, ORB adds an orientation component to it, which\n",
    "is called oFAST (oriented FAST).\n",
    "\n",
    "**AKAZE** makes use of a \"Fast Explicit Diffusion\" (FED) scheme embedded in a pyramidal framework in order to build an accelerated feature detection system in nonlinear scale spaces. By means of FED schemes, a nonlinear scale space can be built much faster than with any other kind of discretization scheme.\n",
    "\n",
    "## Keypoint detection and matching\n",
    "\n",
    "Study the [ORB/AKAZE OpenCV tutorial](https://docs.opencv.org/4.3.0/dc/d16/tutorial_akaze_tracking.html).\n",
    "It shows us how to do the following:\n",
    "\n",
    "- Detect and describe keypoints on the first frame, manually set object boundaries\n",
    "- For every next frame:\n",
    "  1. Detect and describe keypoints\n",
    "  2. Match them using bruteforce matcher\n",
    "  3. Estimate homography transformation using RANSAC\n",
    "  4. Filter out the outliers among the matches\n",
    "  5. Apply homography transformation to the bounding box to find the object\n",
    "  6. Draw bounding box and inliers and compute the inlier ratio as an evaluation metric\n",
    "\n",
    "While this is useful for tracking a 2D planar object with a fixed camera, the keypoint\n",
    "matching method is appropriate for full 3D point correspondence estimation, estimation of F or E,\n",
    "and so on. We'll just have to replace the homography transformation with F or E.\n",
    "\n",
    "First we'll talk about feature matching a bit, get the tutorial code running (code is replicated below and nicely translated\n",
    "into Python by Alisa), then we'll\n",
    "get AKAZE and ORB keypoints from the first two frames with motion in the sequence of frames from Video 1.\n",
    "\n",
    "When you adapt the tutorial code to our situation,\n",
    "note that the it has some things such as setting the ROI and tracking from a video that are not relevant for us.\n",
    "Focus on the keypoint detector setup and keypoint matcher setup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C++ / main.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <opencv2/opencv.hpp>\n",
    "#include <vector>\n",
    "#include <iostream>\n",
    "#include <iomanip>\n",
    "#include \"stats.h\" // Stats structure definition\n",
    "#include \"utils.h\" // Drawing and printing functions\n",
    "using namespace std;\n",
    "using namespace cv;\n",
    "const double akaze_thresh = 3e-4; // AKAZE detection threshold set to locate about 1000 keypoints\n",
    "const double ransac_thresh = 2.5f; // RANSAC inlier threshold\n",
    "const double nn_match_ratio = 0.8f; // Nearest-neighbour matching ratio\n",
    "const int bb_min_inliers = 100; // Minimal number of inliers to draw bounding box\n",
    "const int stats_update_period = 10; // On-screen statistics are updated every 10 frames\n",
    "namespace example {\n",
    "    class Tracker\n",
    "    {\n",
    "    public:\n",
    "        Tracker(Ptr<Feature2D> _detector, Ptr<DescriptorMatcher> _matcher) :\n",
    "            detector(_detector),\n",
    "            matcher(_matcher)\n",
    "        {}\n",
    "        void setFirstFrame(const Mat frame, vector<Point2f> bb, string title, Stats& stats);\n",
    "        Mat process(const Mat frame, Stats& stats);\n",
    "        Ptr<Feature2D> getDetector() {\n",
    "            return detector;\n",
    "        }\n",
    "    protected:\n",
    "        Ptr<Feature2D> detector;\n",
    "        Ptr<DescriptorMatcher> matcher;\n",
    "        Mat first_frame, first_desc;\n",
    "        vector<KeyPoint> first_kp;\n",
    "        vector<Point2f> object_bb;\n",
    "    };\n",
    "    void Tracker::setFirstFrame(const Mat frame, vector<Point2f> bb, string title, Stats& stats)\n",
    "    {\n",
    "        cv::Point* ptMask = new cv::Point[bb.size()];\n",
    "        const Point* ptContain = { &ptMask[0] };\n",
    "        int iSize = static_cast<int>(bb.size());\n",
    "        for (size_t i = 0; i < bb.size(); i++) {\n",
    "            ptMask[i].x = static_cast<int>(bb[i].x);\n",
    "            ptMask[i].y = static_cast<int>(bb[i].y);\n",
    "        }\n",
    "        first_frame = frame.clone();\n",
    "        cv::Mat matMask = cv::Mat::zeros(frame.size(), CV_8UC1);\n",
    "        cv::fillPoly(matMask, &ptContain, &iSize, 1, cv::Scalar::all(255));\n",
    "        detector->detectAndCompute(first_frame, matMask, first_kp, first_desc);\n",
    "\n",
    "        Mat res;\n",
    "        drawKeypoints(first_frame, first_kp, res, Scalar(255, 0, 0), DrawMatchesFlags::DRAW_RICH_KEYPOINTS);\n",
    "        imshow(\"key points\", res);\n",
    "        waitKey(0);\n",
    "        destroyWindow(\"key points\");\n",
    "\n",
    "        stats.keypoints = (int)first_kp.size();\n",
    "        drawBoundingBox(first_frame, bb);\n",
    "        putText(first_frame, title, Point(0, 60), FONT_HERSHEY_PLAIN, 5, Scalar::all(0), 4);\n",
    "        object_bb = bb;\n",
    "        delete[] ptMask;\n",
    "    }\n",
    "    Mat Tracker::process(const Mat frame, Stats& stats)\n",
    "    {\n",
    "        TickMeter tm;\n",
    "        vector<KeyPoint> kp;\n",
    "        Mat desc;\n",
    "        tm.start();\n",
    "        detector->detectAndCompute(frame, noArray(), kp, desc);\n",
    "        stats.keypoints = (int)kp.size();\n",
    "        vector< vector<DMatch> > matches;\n",
    "        vector<KeyPoint> matched1, matched2;\n",
    "        matcher->knnMatch(first_desc, desc, matches, 2);\n",
    "        for (unsigned i = 0; i < matches.size(); i++) {\n",
    "            if (matches[i][0].distance < nn_match_ratio * matches[i][1].distance) {\n",
    "                matched1.push_back(first_kp[matches[i][0].queryIdx]);\n",
    "                matched2.push_back(kp[matches[i][0].trainIdx]);\n",
    "            }\n",
    "        }\n",
    "        stats.matches = (int)matched1.size();\n",
    "        Mat inlier_mask, homography;\n",
    "        vector<KeyPoint> inliers1, inliers2;\n",
    "        vector<DMatch> inlier_matches;\n",
    "        if (matched1.size() >= 4) {\n",
    "            homography = findHomography(Points(matched1), Points(matched2),\n",
    "                RANSAC, ransac_thresh, inlier_mask);\n",
    "        }\n",
    "        tm.stop();\n",
    "        stats.fps = 1. / tm.getTimeSec();\n",
    "        if (matched1.size() < 4 || homography.empty()) {\n",
    "            Mat res;\n",
    "            hconcat(first_frame, frame, res);\n",
    "            stats.inliers = 0;\n",
    "            stats.ratio = 0;\n",
    "            return res;\n",
    "        }\n",
    "        for (unsigned i = 0; i < matched1.size(); i++) {\n",
    "            if (inlier_mask.at<uchar>(i)) {\n",
    "                int new_i = static_cast<int>(inliers1.size());\n",
    "                inliers1.push_back(matched1[i]);\n",
    "                inliers2.push_back(matched2[i]);\n",
    "                inlier_matches.push_back(DMatch(new_i, new_i, 0));\n",
    "            }\n",
    "        }\n",
    "        stats.inliers = (int)inliers1.size();\n",
    "        stats.ratio = stats.inliers * 1.0 / stats.matches;\n",
    "        vector<Point2f> new_bb;\n",
    "        perspectiveTransform(object_bb, new_bb, homography);\n",
    "        Mat frame_with_bb = frame.clone();\n",
    "        if (stats.inliers >= bb_min_inliers) {\n",
    "            drawBoundingBox(frame_with_bb, new_bb);\n",
    "        }\n",
    "        Mat res;\n",
    "        drawMatches(first_frame, inliers1, frame_with_bb, inliers2,\n",
    "            inlier_matches, res,\n",
    "            Scalar(255, 0, 0), Scalar(255, 0, 0));\n",
    "        return res;\n",
    "    }\n",
    "}\n",
    "int main(int argc, char** argv)\n",
    "{\n",
    "    string video_name = \"robot.mp4\";\n",
    "    VideoCapture video_in;\n",
    "    video_in.open(video_name);\n",
    "    if (!video_in.isOpened()) {\n",
    "        cerr << \"Couldn't open \" << video_name << endl;\n",
    "        return 1;\n",
    "    }\n",
    "    Stats stats, akaze_stats, orb_stats;\n",
    "    Ptr<AKAZE> akaze = AKAZE::create();\n",
    "    akaze->setThreshold(akaze_thresh);\n",
    "    Ptr<ORB> orb = ORB::create();\n",
    "    Ptr<DescriptorMatcher> matcher = DescriptorMatcher::create(\"BruteForce-Hamming\");\n",
    "    example::Tracker akaze_tracker(akaze, matcher);\n",
    "    example::Tracker orb_tracker(orb, matcher);\n",
    "    Mat frame;\n",
    "    namedWindow(video_name, WINDOW_NORMAL);\n",
    "    cout << \"\\nPress any key to stop the video and select a bounding box\" << endl;\n",
    "    while (waitKey(1) < 1)\n",
    "    {\n",
    "        video_in >> frame;\n",
    "        cv::resizeWindow(video_name, frame.size());\n",
    "        imshow(video_name, frame);\n",
    "    }\n",
    "    vector<Point2f> bb;\n",
    "    cv::Rect uBox = cv::selectROI(video_name, frame);\n",
    "    bb.push_back(cv::Point2f(static_cast<float>(uBox.x), static_cast<float>(uBox.y)));\n",
    "    bb.push_back(cv::Point2f(static_cast<float>(uBox.x + uBox.width), static_cast<float>(uBox.y)));\n",
    "    bb.push_back(cv::Point2f(static_cast<float>(uBox.x + uBox.width), static_cast<float>(uBox.y + uBox.height)));\n",
    "    bb.push_back(cv::Point2f(static_cast<float>(uBox.x), static_cast<float>(uBox.y + uBox.height)));\n",
    "    akaze_tracker.setFirstFrame(frame, bb, \"AKAZE\", stats);\n",
    "    orb_tracker.setFirstFrame(frame, bb, \"ORB\", stats);\n",
    "    Stats akaze_draw_stats, orb_draw_stats;\n",
    "    Mat akaze_res, orb_res, res_frame;\n",
    "    int i = 0;\n",
    "    for (;;) {\n",
    "        i++;\n",
    "        bool update_stats = (i % stats_update_period == 0);\n",
    "        video_in >> frame;\n",
    "        // stop the program if no more images\n",
    "        if (frame.empty()) break;\n",
    "        akaze_res = akaze_tracker.process(frame, stats);\n",
    "        akaze_stats += stats;\n",
    "        if (update_stats) {\n",
    "            akaze_draw_stats = stats;\n",
    "        }\n",
    "        orb->setMaxFeatures(stats.keypoints);\n",
    "        orb_res = orb_tracker.process(frame, stats);\n",
    "        orb_stats += stats;\n",
    "        if (update_stats) {\n",
    "            orb_draw_stats = stats;\n",
    "        }\n",
    "        drawStatistics(akaze_res, akaze_draw_stats);\n",
    "        drawStatistics(orb_res, orb_draw_stats);\n",
    "        vconcat(akaze_res, orb_res, res_frame);\n",
    "        cv::imshow(video_name, res_frame);\n",
    "        if (waitKey(1) == 27) break; //quit on ESC button\n",
    "    }\n",
    "    akaze_stats /= i - 1;\n",
    "    orb_stats /= i - 1;\n",
    "    printStatistics(\"AKAZE\", akaze_stats);\n",
    "    printStatistics(\"ORB\", orb_stats);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C++ / stats.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ifndef STATS_H\n",
    "#define STATS_H\n",
    "\n",
    "struct Stats\n",
    "{\n",
    "    int matches;\n",
    "    int inliers;\n",
    "    double ratio;\n",
    "    int keypoints;\n",
    "    double fps;\n",
    "\n",
    "    Stats() : matches(0),\n",
    "        inliers(0),\n",
    "        ratio(0),\n",
    "        keypoints(0),\n",
    "        fps(0.)\n",
    "    {}\n",
    "\n",
    "    Stats& operator+=(const Stats& op) {\n",
    "        matches += op.matches;\n",
    "        inliers += op.inliers;\n",
    "        ratio += op.ratio;\n",
    "        keypoints += op.keypoints;\n",
    "        fps += op.fps;\n",
    "        return *this;\n",
    "    }\n",
    "    Stats& operator/=(int num)\n",
    "    {\n",
    "        matches /= num;\n",
    "        inliers /= num;\n",
    "        ratio /= num;\n",
    "        keypoints /= num;\n",
    "        fps /= num;\n",
    "        return *this;\n",
    "    }\n",
    "};\n",
    "\n",
    "#endif // STATS_H#pragma once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C++ / utils.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ifndef UTILS_H\n",
    "#define UTILS_H\n",
    "\n",
    "#include <opencv2/opencv.hpp>\n",
    "#include <vector>\n",
    "#include \"stats.h\"\n",
    "\n",
    "using namespace std;\n",
    "using namespace cv;\n",
    "\n",
    "void drawBoundingBox(Mat image, vector<Point2f> bb);\n",
    "void drawStatistics(Mat image, const Stats& stats);\n",
    "void printStatistics(string name, Stats stats);\n",
    "vector<Point2f> Points(vector<KeyPoint> keypoints);\n",
    "Rect2d selectROI(const String& video_name, const Mat& frame);\n",
    "\n",
    "void drawBoundingBox(Mat image, vector<Point2f> bb)\n",
    "{\n",
    "    for (unsigned i = 0; i < bb.size() - 1; i++) {\n",
    "        line(image, bb[i], bb[i + 1], Scalar(0, 0, 255), 2);\n",
    "    }\n",
    "    line(image, bb[bb.size() - 1], bb[0], Scalar(0, 0, 255), 2);\n",
    "}\n",
    "\n",
    "void drawStatistics(Mat image, const Stats& stats)\n",
    "{\n",
    "    static const int font = FONT_HERSHEY_PLAIN;\n",
    "    stringstream str1, str2, str3, str4;\n",
    "\n",
    "    str1 << \"Matches: \" << stats.matches;\n",
    "    str2 << \"Inliers: \" << stats.inliers;\n",
    "    str3 << \"Inlier ratio: \" << setprecision(2) << stats.ratio;\n",
    "    str4 << \"FPS: \" << std::fixed << setprecision(2) << stats.fps;\n",
    "\n",
    "    putText(image, str1.str(), Point(0, image.rows - 120), font, 2, Scalar::all(255), 3);\n",
    "    putText(image, str2.str(), Point(0, image.rows - 90), font, 2, Scalar::all(255), 3);\n",
    "    putText(image, str3.str(), Point(0, image.rows - 60), font, 2, Scalar::all(255), 3);\n",
    "    putText(image, str4.str(), Point(0, image.rows - 30), font, 2, Scalar::all(255), 3);\n",
    "}\n",
    "\n",
    "void printStatistics(string name, Stats stats)\n",
    "{\n",
    "    cout << name << endl;\n",
    "    cout << \"----------\" << endl;\n",
    "\n",
    "    cout << \"Matches \" << stats.matches << endl;\n",
    "    cout << \"Inliers \" << stats.inliers << endl;\n",
    "    cout << \"Inlier ratio \" << setprecision(2) << stats.ratio << endl;\n",
    "    cout << \"Keypoints \" << stats.keypoints << endl;\n",
    "    cout << \"FPS \" << std::fixed << setprecision(2) << stats.fps << endl;\n",
    "    cout << endl;\n",
    "}\n",
    "\n",
    "vector<Point2f> Points(vector<KeyPoint> keypoints)\n",
    "{\n",
    "    vector<Point2f> res;\n",
    "    for (unsigned i = 0; i < keypoints.size(); i++) {\n",
    "        res.push_back(keypoints[i].pt);\n",
    "    }\n",
    "    return res;\n",
    "}\n",
    "#endif // UTILS_H#pragma once\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python / stats.py\n",
    "\n",
    "Some quick tips:\n",
    " - Use a multiline comment (\"\"\" some data \"\"\") after a class namde or function name declaration to make your intellisense checker happy.\n",
    " - use <tt>:type</tt> to define the type of a parameter to a function or method.\n",
    " - You can overload operators like <tt>+</tt>, <tt>-</tt>, <tt>*</tt>, and <tt>/</tt> yourself. Just declare a method with <tt>__method__(self,...)</tt>.\n",
    "   Try it, it's very useful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Stats:\n",
    "    \"\"\"\n",
    "    Statistic class\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    matches=0 (int):\n",
    "        total number of matching\n",
    "\n",
    "    inliers=0 (int):\n",
    "        number of inliner matching\n",
    "\n",
    "    ratio=0. (float):\n",
    "        Nearest-neighbour matching ratio\n",
    "\n",
    "    keypoints=0 (int):\n",
    "        Wall\n",
    "\n",
    "    fps=0. (float):\n",
    "        frame per 1 sec\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    add(Stats) - overload + function:\n",
    "        plus the information into this class\n",
    "\n",
    "    divide(Stats) - overload + function:\n",
    "        divide the information into this class\n",
    "    \"\"\"\n",
    "    matches:int\n",
    "    inliers:int\n",
    "    ratio:float\n",
    "    keypoints:int\n",
    "    fps:float\n",
    "\n",
    "    def __init__(self, matches = 0, inliers = 0, ratio = 0., keypoints = 0, fps = 0.):\n",
    "        self.matches = matches\n",
    "        self.inliers = inliers\n",
    "        self.ratio = ratio\n",
    "        self.keypoints = keypoints\n",
    "        self.fps = fps\n",
    "\n",
    "    def __add__(self, op:\"Stats\") -> \"Stats\":\n",
    "        self.matches += op.matches\n",
    "        self.inliers += op.inliers\n",
    "        self.ratio += op.ratio\n",
    "        self.keypoints += op.keypoints\n",
    "        self.fps += op.fps\n",
    "        return self\n",
    "\n",
    "    def __truediv__(self, num:int) -> \"Stats\":\n",
    "        self.matches //= num\n",
    "        self.inliers //= num\n",
    "        self.ratio /= num\n",
    "        self.keypoints //= num\n",
    "        self.fps /= num\n",
    "        return self\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"matches({0}) inliner({1}) ratio({2:.2f}) keypoints({3}) fps({4:.2f})\".format(self.matches, self.inliers, self.ratio, self.keypoints, self.fps)\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "    def to_strings(self):\n",
    "        \"\"\"\n",
    "        Convert to string set of matches, inliners, ratio, and fps\n",
    "        \"\"\"\n",
    "        str1 = \"Matches: {0}\".format(self.matches)\n",
    "        str2 = \"Inliers: {0}\".format(self.inliers)\n",
    "        str3 = \"Inlier ratio: {0:.2f}\".format(self.ratio)\n",
    "        str4 = \"Keypoints: {0}\".format(self.keypoints)\n",
    "        str5 = \"FPS: {0:.2f}\".format(self.fps)\n",
    "        return str1, str2, str3, str4, str5\n",
    "\n",
    "    def copy(self):\n",
    "        return Stats(self.matches, self.inliers, self.ratio, self.keypoints, self.fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches(7) inliner(3) ratio(9.00) keypoints(12) fps(10.50)\n",
      "matches(2) inliner(1) ratio(3.00) keypoints(4) fps(3.50)\n"
     ]
    }
   ],
   "source": [
    "# test the class\n",
    "\n",
    "#from stats import Stats\n",
    "\n",
    "test1 = Stats(5, 2, 9, 4, 1.5)\n",
    "test2 = Stats(2, 1, 0, 8, 9)\n",
    "\n",
    "test1 + test2\n",
    "print(test1)\n",
    "test1 / 3\n",
    "print(test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python / Utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stats import Stats\n",
    "import cv2\n",
    "from typing import List #use it for :List[...]\n",
    "\n",
    "def drawBoundingBox(image, bb):\n",
    "    \"\"\"\n",
    "    Draw the bounding box from the points set\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        image (array):\n",
    "            image which you want to draw\n",
    "        bb (List):\n",
    "            points array set\n",
    "    \"\"\"\n",
    "    color = (0, 0, 255)\n",
    "    for i in range(len(bb) - 1):\n",
    "        b1 = (int(bb[i][0]), int(bb[i][1]))\n",
    "        b2 = (int(bb[i + 1][0]), int(bb[i + 1][1]))\n",
    "        cv2.line(image, b1, b2, color, 2)\n",
    "    b1 = (int(bb[len(bb) - 1][0]), int(bb[len(bb) - 1][1]))\n",
    "    b2 = (int(bb[0][0]), int(bb[0][1]))\n",
    "    cv2.line(image, b1, b2, color, 2)\n",
    "\n",
    "def drawStatistics(image, stat: Stats):\n",
    "    \"\"\"\n",
    "    Draw the statistic to images\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        image (array):\n",
    "            image which you want to draw\n",
    "        stat (Stats):\n",
    "            statistic values\n",
    "    \"\"\"\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "    str1, str2, str3, str4, str5 = stat.to_strings()\n",
    "\n",
    "    shape = image.shape\n",
    "\n",
    "    cv2.putText(image, str1, (0, shape[0] - 120), font, 2, (0, 0, 255), 3)\n",
    "    cv2.putText(image, str2, (0, shape[0] - 90), font, 2, (0, 0, 255), 3)\n",
    "    cv2.putText(image, str3, (0, shape[0] - 60), font, 2, (0, 0, 255), 3)\n",
    "    cv2.putText(image, str5, (0, shape[0] - 30), font, 2, (0, 0, 255), 3)\n",
    "\n",
    "def printStatistics(name: str, stat: Stats):\n",
    "    \"\"\"\n",
    "    Print the statistic\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        name (str):\n",
    "            image which you want to draw\n",
    "        stat (Stats):\n",
    "            statistic values\n",
    "    \"\"\"\n",
    "    print(name)\n",
    "    print(\"----------\")\n",
    "    str1, str2, str3, str4, str5 = stat.to_strings()\n",
    "    print(str1)\n",
    "    print(str2)\n",
    "    print(str3)\n",
    "    print(str4)\n",
    "    print(str5)\n",
    "    print()\n",
    "\n",
    "def Points(keypoints):\n",
    "    res = []\n",
    "    for i in keypoints:\n",
    "        res.append(i)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python / main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from stats import Stats\n",
    "from utils import drawBoundingBox, drawStatistics, printStatistics, Points\n",
    "\n",
    "akaze_thresh:float = 3e-4 # AKAZE detection threshold set to locate about 1000 keypoints\n",
    "ransac_thresh:float = 2.5 # RANSAC inlier threshold\n",
    "nn_match_ratio:float = 0.8 # Nearest-neighbour matching ratio\n",
    "bb_min_inliers:int = 100 # Minimal number of inliers to draw bounding box\n",
    "stats_update_period:int = 10 # On-screen statistics are updated every 10 frames\n",
    "\n",
    "class Tracker:\n",
    "    def __init__(self, detector, matcher):\n",
    "        self.detector = detector\n",
    "        self.matcher = matcher\n",
    "\n",
    "    def setFirstFrame(self, frame, bb, title:str):\n",
    "        iSize = len(bb)\n",
    "        stat = Stats()\n",
    "        ptContain = np.zeros((iSize, 2))\n",
    "        i = 0\n",
    "        for b in bb:\n",
    "            #ptMask[i] = (b[0], b[1])\n",
    "            ptContain[i, 0] = b[0]\n",
    "            ptContain[i, 1] = b[1]\n",
    "            i += 1\n",
    "        \n",
    "        self.first_frame = frame.copy()\n",
    "        matMask = np.zeros(frame.shape, dtype=np.uint8)\n",
    "        cv2.fillPoly(matMask, np.int32([ptContain]), (255,0,0))\n",
    "\n",
    "        # cannot use in ORB\n",
    "        # self.first_kp, self.first_desc = self.detector.detectAndCompute(self.first_frame, matMask)\n",
    "\n",
    "        # find the keypoints with ORB\n",
    "        kp = self.detector.detect(self.first_frame,None)\n",
    "        # compute the descriptors with ORB\n",
    "        self.first_kp, self.first_desc = self.detector.compute(self.first_frame, kp)\n",
    "\n",
    "        # print(self.first_kp[0].pt[0])\n",
    "        # print(self.first_kp[0].pt[1])\n",
    "        # print(self.first_kp[0].angle)\n",
    "        # print(self.first_kp[0].size)\n",
    "        res = cv2.drawKeypoints(self.first_frame, self.first_kp, None, color=(255,0,0), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "        \n",
    "        stat.keypoints = len(self.first_kp)\n",
    "        drawBoundingBox(self.first_frame, bb);\n",
    "\n",
    "        cv2.imshow(\"key points of {0}\".format(title), res)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyWindow(\"key points of {0}\".format(title))\n",
    "\n",
    "        cv2.putText(self.first_frame, title, (0, 60), cv2.FONT_HERSHEY_PLAIN, 5, (0,0,0), 4)\n",
    "        self.object_bb = bb\n",
    "        return stat\n",
    "\n",
    "    def process(self, frame):\n",
    "        stat = Stats()\n",
    "        start_time = time.time()\n",
    "        kp, desc = self.detector.detectAndCompute(frame, None)\n",
    "        stat.keypoints = len(kp)\n",
    "        matches = self.matcher.knnMatch(self.first_desc, desc, k=2)\n",
    "\n",
    "        matched1 = []\n",
    "        matched2 = []\n",
    "        matched1_keypoints = []\n",
    "        matched2_keypoints = []\n",
    "        good = []\n",
    "\n",
    "        for i,(m,n) in enumerate(matches):\n",
    "            if m.distance < nn_match_ratio * n.distance:\n",
    "                good.append(m)\n",
    "                matched1_keypoints.append(self.first_kp[matches[i][0].queryIdx])\n",
    "                matched2_keypoints.append(kp[matches[i][0].trainIdx])\n",
    "\n",
    "        matched1 = np.float32([ self.first_kp[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "        matched2 = np.float32([ kp[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "\n",
    "        stat.matches = len(matched1)\n",
    "        homography = None\n",
    "        if (len(matched1) >= 4):\n",
    "            homography, inlier_mask = cv2.findHomography(matched1, matched2, cv2.RANSAC, ransac_thresh)\n",
    "        dt = time.time() - start_time\n",
    "        stat.fps = 1. / dt\n",
    "        if (len(matched1) < 4 or homography is None):\n",
    "            res = cv2.hconcat([self.first_frame, frame])\n",
    "            stat.inliers = 0\n",
    "            stat.ratio = 0\n",
    "            return res, stat\n",
    "        inliers1 = []\n",
    "        inliers2 = []\n",
    "        inliers1_keypoints = []\n",
    "        inliers2_keypoints = []\n",
    "        for i in range(len(good)):\n",
    "            if (inlier_mask[i] > 0):\n",
    "                new_i = len(inliers1)\n",
    "                inliers1.append(matched1[i])\n",
    "                inliers2.append(matched2[i])\n",
    "                inliers1_keypoints.append(matched1_keypoints[i])\n",
    "                inliers2_keypoints.append(matched2_keypoints[i])\n",
    "        inlier_matches = [cv2.DMatch(_imgIdx=0, _queryIdx=idx, _trainIdx=idx,_distance=0) for idx in range(len(inliers1))]\n",
    "        inliers1 = np.array(inliers1, dtype=np.float32)\n",
    "        inliers2 = np.array(inliers2, dtype=np.float32)\n",
    "\n",
    "        stat.inliers = len(inliers1)\n",
    "        stat.ratio = stat.inliers * 1.0 / stat.matches\n",
    "        bb = np.array([self.object_bb], dtype=np.float32)\n",
    "        new_bb = cv2.perspectiveTransform(bb, homography)\n",
    "        frame_with_bb = frame.copy()\n",
    "        if (stat.inliers >= bb_min_inliers):\n",
    "            drawBoundingBox(frame_with_bb, new_bb[0])\n",
    "\n",
    "        res = cv2.drawMatches(self.first_frame, inliers1_keypoints, frame_with_bb, inliers2_keypoints, inlier_matches, None, matchColor=(255, 0, 0), singlePointColor=(255, 0, 0))\n",
    "        return res, stat\n",
    "\n",
    "    def getDetector(self):\n",
    "        return self.detector\n",
    "\n",
    "def main():\n",
    "    video_name = \"robot.mp4\"\n",
    "    video_in = cv2.VideoCapture()\n",
    "    video_in.open(video_name)\n",
    "    if (not video_in.isOpened()):\n",
    "        print(\"Couldn't open \", video_name)\n",
    "        return -1\n",
    "\n",
    "    akaze_stats = Stats()\n",
    "    orb_stats = Stats()\n",
    "\n",
    "    akaze = cv2.AKAZE_create()\n",
    "    akaze.setThreshold(akaze_thresh)\n",
    "\n",
    "    orb = cv2.ORB_create()\n",
    "\n",
    "    matcher = cv2.DescriptorMatcher_create(\"BruteForce-Hamming\")\n",
    "\n",
    "    akaze_tracker = Tracker(akaze, matcher)\n",
    "    orb_tracker = Tracker(orb, matcher)\n",
    "\n",
    "    cv2.namedWindow(video_name, cv2.WINDOW_NORMAL);\n",
    "    print(\"\\nPress any key to stop the video and select a bounding box\")\n",
    "\n",
    "    key = -1\n",
    "\n",
    "    while(key < 1):\n",
    "        _, frame = video_in.read()\n",
    "        w, h, ch = frame.shape\n",
    "        cv2.resizeWindow(video_name, (h, w))\n",
    "        cv2.imshow(video_name, frame)\n",
    "        key = cv2.waitKey(1)\n",
    "\n",
    "    print(\"Select a ROI and then press SPACE or ENTER button!\")\n",
    "    print(\"Cancel the selection process by pressing c button!\")\n",
    "    uBox = cv2.selectROI(video_name, frame);\n",
    "    bb = []\n",
    "    bb.append((uBox[0], uBox[1]))\n",
    "    bb.append((uBox[0] + uBox[2], uBox[0] ))\n",
    "    bb.append((uBox[0] + uBox[2], uBox[0] + uBox[3]))\n",
    "    bb.append((uBox[0], uBox[0] + uBox[3]))\n",
    "\n",
    "    stat_a = akaze_tracker.setFirstFrame(frame, bb, \"AKAZE\",);\n",
    "    stat_o = orb_tracker.setFirstFrame(frame, bb, \"ORB\");\n",
    "\n",
    "    akaze_draw_stats = stat_a.copy()\n",
    "    orb_draw_stats = stat_o.copy()\n",
    "\n",
    "    i = 0\n",
    "    video_in.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "    while True:\n",
    "        i += 1\n",
    "        update_stats = (i % stats_update_period == 0)\n",
    "        _, frame = video_in.read()\n",
    "        if frame is None:\n",
    "            # End of video\n",
    "            break\n",
    "        akaze_res, stat = akaze_tracker.process(frame)\n",
    "        akaze_stats + stat\n",
    "        if (update_stats):\n",
    "            akaze_draw_stats = stat\n",
    "        orb.setMaxFeatures(stat.keypoints)\n",
    "        orb_res, stat = orb_tracker.process(frame)\n",
    "        orb_stats + stat\n",
    "        if (update_stats):\n",
    "            orb_draw_stats = stat\n",
    "        drawStatistics(akaze_res, akaze_draw_stats)\n",
    "        drawStatistics(orb_res, orb_draw_stats)\n",
    "        res_frame = cv2.vconcat([akaze_res, orb_res])\n",
    "        # cv2.imshow(video_name, akaze_res)\n",
    "        cv2.imshow(video_name, res_frame)\n",
    "        if (cv2.waitKey(1) == 27): # quit on ESC button\n",
    "            break\n",
    "\n",
    "    akaze_stats / (i - 1)\n",
    "    orb_stats / (i - 1)\n",
    "    printStatistics(\"AKAZE\", akaze_stats);\n",
    "    printStatistics(\"ORB\", orb_stats);\n",
    "    return 0\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### ORB/AKAZE Tutorial\n",
    "\n",
    "Get the tutorial running and play with it.\n",
    "\n",
    "### Feature points\n",
    "\n",
    "Select a pair of frames with motion from the Video 1 frame sequence.\n",
    "\n",
    "Detect ORB and AKAZE features and use\n",
    "the OpenCV [<code>drawKeypoints()</code>](https://docs.opencv.org/4.3.0/d4/d5d/group__features2d__draw.html#ga5d2bafe8c1c45289bc3403a40fb88920) function to display the keypoints detected in the two images. Your result should look something like this:\n",
    "\n",
    "<img src=\"img/lab06-1.png\" width=\"600\"/>\n",
    "\n",
    "### Undistortion\n",
    "\n",
    "Using the parameters you got and saved in Lab 05,\n",
    "use <tt>undistortPoints()</tt> to obtain \"ideal\" undistorted points for each of the input point sets.\n",
    "\n",
    "Be careful about the Mat object resulting from <code>undistortPoints()</code>. It is a Nx1 2 channel, 64-bit image, so to access it, you use code such as\n",
    "(C++):\n",
    "\n",
    "    // Example use of undistortPoints function\n",
    "\n",
    "    Mat xy_undistorted;  // leave empty, opencv will fill it.\n",
    "    undistortPoints(match_points, xy_undistorted, camera_matrix, dist_coeffs);\n",
    "\n",
    "    Point2f point;\n",
    "    for (int i = 0;i<nPoints;i++)\n",
    "    {\n",
    "        point.x = xy_undistorted.at<cv::Vec2d>(i, 0)[0];\n",
    "        point.y = xy_undistorted.at<cv::Vec2d>(i, 0)[1];\n",
    "        // do something\n",
    "    }\n",
    "    \n",
    "It's easier in Python:\n",
    "\n",
    "    xy_undistorted = cv2.undistortPoints(match_points, camera_matrix, dist_coeffs)\n",
    "\n",
    "    x = xy_undistorted[i][0]\n",
    "    y = xy_undistorted[i][1]\n",
    "\n",
    "Knowing this in advance will save you some time.\n",
    "\n",
    "### Feature point matching\n",
    "\n",
    "Next, get matches using the brute force Hamming matcher, remove indistinct matches (matches for which the ratio of distances for the first and second match is greater than 0.8) and use the OpenCV [<code>drawMatches()</code>](https://docs.opencv.org/4.3.0/d4/d5d/group__features2d__draw.html#gad8f463ccaf0dc6f61083abd8717c261a) function to display the result for AKAZE and ORB.\n",
    "\n",
    "In your report, discuss which keypoint detector seems to work best in terms of number of matches and number of accurate matches.\n",
    "\n",
    "### Essential matrix\n",
    "\n",
    "Next, let's find an essential matrix relating these two images using the better keypoint matching algorithm from the previous experiment.\n",
    "Use <code>findEssentialMat</code> to get an essential matrix with RANSAC.\n",
    "Check carefully about normalization of the point correspondences.\n",
    "After that, replot your correspondences with inliers only, obtaining something like the following:\n",
    "\n",
    "<img src=\"img/lab06-2.png\" width=\"600\"/>\n",
    "\n",
    "Pick two pairs of corresponding points in the two images and verify that $X^T K^{-T} E K^{-1} X' = 0$, approximately.\n",
    "\n",
    "Hint: you can tell <code>drawMatches</code> to only draw inliers by constructing a vector of vector of char like this:\n",
    "\n",
    "    std::vector<std::vector<char> > vvMatchesMask;\n",
    "    for (int i = 0, j = 0; i < matched1.size(); i++) {\n",
    "        if (vMatched[i]) {\n",
    "            if (inlier_mask.at<uchar>(j)) {\n",
    "                vvMatchesMask.push_back( { 1, 0 } );\n",
    "            } else {\n",
    "                vvMatchesMask.push_back( { 0, 0 });\n",
    "            }\n",
    "            j++;\n",
    "        } else {\n",
    "            vvMatchesMask.push_back( { 0, 0 });\n",
    "        }\n",
    "    }\n",
    "    \n",
    "Here's the Python:\n",
    "\n",
    "    matchesMask = []\n",
    "    j = 0\n",
    "    for i in range(len(good)):\n",
    "        if vMatched[i]:\n",
    "            if inlier_mask[j] > 0:\n",
    "                matchesMask.append( ( 1, 0 ) )\n",
    "            else:\n",
    "                matchesMask.append( ( 0, 0 ) )\n",
    "            j += 1\n",
    "        else:\n",
    "            matchesMask.append( ( 0, 0 ))\n",
    "\n",
    "Here <code>vMatched</code> is a vector of <code>bool</code> that I constructed while selecting matches according to the distance ratio.\n",
    "\n",
    "Using undistorted images and undistorted points (see note above about how to access the undistorted point array) you should get something like this:\n",
    "\n",
    "<img src=\"img/lab06-3.png\" width=\"600\"/>\n",
    "\n",
    "### Epipolar lines\n",
    "\n",
    "Finally, draw a couple corresponding epipolar lines in each undistorted image. You should get something like this:\n",
    "\n",
    "For frame 1:\n",
    "\n",
    "<img src=\"img/lab06-4.png\" width=\"600\"/>\n",
    "\n",
    "For frame 2:\n",
    "\n",
    "<img src=\"img/lab06-5.png\" width=\"600\"/>\n",
    "\n",
    "Next, perform factorization of E to get R and t.\n",
    "\n",
    "In your report, show your analysis of the number of keypoints, matched keypoints, matched unique keypoints (those that pass the distance ratio test), and inliers according to the estimated essential matrix.\n",
    "\n",
    "### Recover relative pose\n",
    "\n",
    "Use <code>correctMatches()</code> and <code>recoverPose()</code> to \"clean up\" your image points (adjust each corresponding pair of points to be on corresponding epipolar lines according to E/F) and get the rotation and translation between the two camera frames. Understand the rotation and translation vectors you get and the scale ambiguity inherent in a metric 3D reconstruction.\n",
    "\n",
    "Construct the two projection matrices and use <code>triangulatePoints()</code> to obtain 3D points from the corrected 2D points. Visualize the 3D point cloud in Octave to see if it is sensible.\n",
    "\n",
    "You should get something similar to this:\n",
    "\n",
    "<img src=\"img/lab06-6.png\" width=\"600\"/>\n",
    "\n",
    "Here the points have been transformed from the first camera's coordinate frame to the robot frame for the first camera, using the rotation matrix and translation matrix from the extrinsic calibration.\n",
    "\n",
    "### Find absolute scale\n",
    "\n",
    "We know that after scaling then transforming the 3D points into the world coordinate system, the points with the smallest 'Z' values should be the ones on the floor. Can you come up with a scale factor that pushes the \"bottom\" of the point cloud to the floor (Z=0) in the world frame?\n",
    "For that you'll need the extrinsic parameters of the camera. We'll provide them.\n",
    "Show your solution and a visualization of the points.\n",
    "\n",
    "After scaling the points in the camera frame (or re-triangulating after scaling the translation vector from <code>recoverPose()</code>), you should have a structure similar to what's shown in [this video](https://drive.google.com/file/d/16lwooQ4rIGJJ1cLM-hUxb_m-tmmyWddY/view)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
