{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision, Lab 7:  Structure from Motion (SfM)\n",
    "\n",
    "reference:\n",
    "- https://www.opensfm.org/docs/building.html\n",
    "- https://github.com/mapillary/OpenSfM\n",
    "- Mastering OpenCV3 (Packtpub)\n",
    "- Mastering OpenCV4 (Packtpub)\n",
    "\n",
    "In this lab, we'll experiment with Structure from Motion (SfM) using the open source OpenSFM framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction of SFM\n",
    "\n",
    "The following flow diagram describes the process in the SfM pipeline we will implement.\n",
    "\n",
    "<img src=\"img/lab07-11.png\" width=\"800\"/>\n",
    "\n",
    "The first discrimination we should make is the difference between stereo (or indeed any multiview) and 3D reconstruction using calibrated rigs and SfM. A rig of two or more cameras assumes that we already know the motion between the cameras, while in SfM, we don't know what this motion is and we wish to find it. Calibrated rigs, from a simplistic point of view, allow a much more accurate reconstruction of 3D geometry because there is no error in estimating the distance and rotation between the cameras, it is already known. The first step in implementing an SfM system is finding the motion between the cameras.\n",
    "\n",
    "In most cases, we wish to obtain the geometry of the scene, for example, where objects are in relation to the camera and what their form is. Having found the motion between the cameras picturing the same scene, from a reasonably similar point of view, we would now like to reconstruct the geometry. In Computer Vision jargon, this is known as **triangulation**, and there are plenty of ways to go about it. It may be done by way of ray intersection, where we construct two rays-one from each camera's center of projection and a point on each of the image planes. The intersection of these rays in space will, ideally, intersect at one 3D point in the real world that is imaged in each camera, as shown in the following diagram:\n",
    "\n",
    "<img src=\"img/lab07-12.jfif\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we set out to actually find the motion between two cameras, let's examine the inputs and the tools we have at hand to perform this operation. First, we have two images of the same scene from different positions in space. This is a powerful asset, and we will make sure that we use it. As for tools, we should take a look at mathematical objects that impose constraints over our images, cameras, and the scene.\n",
    "\n",
    "Two very useful mathematical objects are the fundamental matrix (denoted by $F$) and the essential matrix (denoted by $E$), which impose a constraint over corresponding 2D points in two images of the scene. They are mostly similar, except that the essential matrix is assuming usage of calibrated cameras; this is the case for us, so we will choose it.\n",
    "\n",
    "The essential matrix $E$ is a 3x3 sized matrix, which imposes the following constraint on a point $x$ in one image and a point and a point $x'$ corresponding image:\n",
    "\n",
    "$x' K T E K x = 0$\n",
    "\n",
    "$K$ is the calibration matrix. Another important fact we use is that the essential matrix is all we need in order to recover the two cameras' positions from our images, although only up to an arbitrary unit of scale. So, if we obtain the essential matrix, we know where each camera is positioned in space, and where it is looking. We can easily calculate the matrix if we have enough of those constraint equations, simply because each equation can be used to solve for a small part of the matrix. In fact, OpenCV internally calculates it using just five point-pairs, but through the **Random Sample Consensus algorithm (RANSAC)**, many more pairs can be used and they make for a more robust solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point matching using rich feature descriptors\n",
    "\n",
    "Feature extraction and descriptor matching is an essential process in Computer Vision, and is used in many methods to perform all sorts of operations, for example, detecting the position and orientation of an object in an image or searching a big database of images for similar images through a given query. In essence, **feature extraction** means selecting points in the image that would make for good features and computing a descriptor for them. A **descriptor** is a vector of numbers that describes the surrounding environment around a feature point in an image. Different methods have different length and data types for their descriptor vectors. **Descriptor Matching** is the process of finding a corresponding feature of one set in another using its descriptor. OpenCV provides very easy and powerful methods to support feature extraction and matching.\n",
    "\n",
    "Our goal is to obtain three elements: feature points for two images (ORB, AKAZE, SIFT, SURF), descriptors for them, and a matching between the two sets of features (bruteforce). \n",
    "\n",
    "In OpenCV, Brute-force matcher, is cross-check filtering. That is, a match is considered true if a feature of the first image matches a feature of the second image, and the reverse check also matches the feature of the second image with the feature of the first image. Another common filtering mechanism, used in the provided code, is to filter based on the fact that the two images are of the same scene and have a certain stereo-view relationship between them. In practice, the filter tries to robustly calculate the fundamental or essential matrix which we will learn about in the **Finding camera matrices** section and retain those feature pairs that correspond with this calculation with small errors.\n",
    "\n",
    "An alternative to using rich features, such as ORB, is to use optical flow. The following information box provides a short overview of optical flow. It is possible to use optical flow instead of descriptor matching to find the required point matching between two images, while the rest of the SfM pipeline remains the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding camera matrices\n",
    "\n",
    "Now that we have obtained matches between keypoints, we can calculate the essential matrix. However, we must first align our matching points into two arrays, where an index in one array corresponds to the same index in the other. This is required by the findEssentialMat function as we've seen in the **Estimating Camera Motion** section.\n",
    "\n",
    "<img src=\"img/lab07-13.jfif\" width=\"800\"/>\n",
    "\n",
    "Now we are ready to find the camera matrices. This process is described at length in a chapter of H&Z's book; however, the new OpenCV 3 API makes things very easy for us by introducing the <code>recoverPose</code> function. First, we will briefly examine the structure of the camera matrix we are going to use $P=[R|t]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the image pair to use first\n",
    "\n",
    "Given we have more than just two image views of the scene, we must choose which two views we will start the reconstruction from and picking the two views that have the least number of **homography inliers**. A homography is a relationship between two images or sets of points that lie on a plane; the **homography matrix** defines the transformation from one plane to another. In case of an image or a set of 2D points, the homography matrix is of size 3x3.\n",
    "\n",
    "When we look for the lowest inlier ratio, they essentially suggest that you calculate the homography matrix between all pairs of images and pick the pair whose points mostly do not correspond with the homography matrix. This means that the geometry of the scene in these two views is not planar, or at least, not the same plane in both views, which helps when doing 3D reconstruction. For reconstruction, it is best to look at a complex scene with non-planar geometry, with things closer and farther away from the camera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstructing the scene\n",
    "\n",
    "Next, we look into the matter of recovering the 3D structure of the scene from the information we have acquired so far. As we had done before, we should look at the tools and information we have at hand to achieve this. In the preceding section, we obtained two camera matrices from the essential matrix; we already discussed how these tools would be useful for obtaining the 3D position of a point in space. Then, we can go back to our matched point pairs to fill in our equations with numerical data. The point pairs will also be useful in calculating the error we get from all our approximate calculations.\n",
    "\n",
    "Remember we had two key equations arising from the 2D point matching and $P$ matrices: $x=PX$ and $x'= P'X$, where $x$ and $x'$ are matching 2D points and X is a real-world 3D point imaged by the two cameras. If we examine these equations, we will see that the $x$ vector that represents a 2D point should be of size (3x1) and $X$ that represents a 3D point should be (4x1). Both points received an extra entry in the vector; this is called Homogeneous Coordinates. We use these coordinates to streamline the triangulation process.\n",
    "\n",
    "The equation $x = PX$ is missing a crucial element: the camera calibration parameters matrix, $K$. The matrix $K$ is used to transform 2D image points from pixel coordinates to normalized coordinates (in the [-1, 1] range) removing the dependency on the size of the image in pixels, which is absolutely necessary. For example, a 2D point $x_1 = (160, 120)$ in a 320x240 image, may transform to $x_1' = (0, 0)$ under certain circumstances. To that end, we use the <code>undistortPoints</code> function.\n",
    "\n",
    "<img src=\"img/lab07-14.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction from many views\n",
    "\n",
    "Now that we know how to recover the motion and scene geometry from two cameras, it would seem simple to get the parameters of additional cameras and more scene points simply by applying the same process. This matter is in fact not so simple, as we can only get a reconstruction that is upto scale, and each pair of pictures has a different scale.\n",
    "\n",
    "There are a number of ways to correctly reconstruct the 3D scene data from multiple views. One way to achieve **camera pose estimation** or **camera resectioning**, is the **Perspective N-Point(PnP)** algorithm, where we try to solve for the position of a new camera using N 3D scene points, which we have already found and their respective 2D image points. Another way is to triangulate more points and see how they fit into our existing scene geometry; this will tell us the position of the new camera by means of **point cloud registration**. In this section, we will discuss using OpenCV's <code>solvePnP</code> functions that implements the first method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building OpenSFM\n",
    "\n",
    "First you'll want to build the library. This will take a while and so should be done in advance of the laboratory session. The following should work if you're on Ubuntu 20.04 with Python 3.8. If you're using virtualenv (PyCharm will set up virtualenv for you automatically), your Python executable and pip executable might be <code>python</code> and <code>pip</code> instead of <code>python3</code> and <code>pip3</code>.\n",
    "\n",
    "<code>\n",
    "$ git clone --recursive https://github.com/mapillary/OpenSfM\n",
    "$ sudo apt-get install build-essential cmake git libatlas-base-dev \\\n",
    "       libeigen3-dev libgoogle-glog-dev libsuitesparse-dev python3-dev \\\n",
    "       curl vim python3-yaml\n",
    "$ mkdir -p source ; cd source ; curl -L http://ceres-solver.org/ceres-solver-1.14.0.tar.gz \\\n",
    "       | tar xz ; cd ceres-solver-1.14.0 ; mkdir -p build ; cd build ; cmake .. \\\n",
    "       -DCMAKE_C_FLAGS=-fPIC -DCMAKE_CXX_FLAGS=-fPIC -DBUILD_EXAMPLES=OFF \\\n",
    "       -DBUILD_TESTING=OFF ; make -j4 ; sudo make install ; cd ../../..\n",
    "$ mkdir -p source ; cd source ; git clone https://github.com/paulinus/opengv.git ; \\\n",
    "       cd opengv ; git submodule update --init --recursive ; mkdir -p build ; cd build ; \\\n",
    "       cmake .. -DBUILD_TESTS=OFF -DBUILD_PYTHON=ON -DPYBIND11_PYTHON_VERSION=3.8 \\\n",
    "       -DPYTHON_INSTALL_DIR=/usr/local/lib/python3.8/dist-packages/ ; \\\n",
    "       make -j4 ; sudo make install ; cd ../../..\n",
    "$ pip3 install scipy exifread==2.1.2 gpxpy==1.1.2 networkx==1.11 numpy \\\n",
    "       pyproj pytest==3.0.7 python-dateutil==2.6.0 PyYAML==3.12 xmltodict==0.10.2 \\\n",
    "       loky repoze.lru wheel opencv-python pillow joblib\n",
    "$ cd OpenSfM ; python3 setup.py build\n",
    "</code>\n",
    "\n",
    "## Dr. Matt, I cannot do it :'(\n",
    "\n",
    "All machines that I have in Ubuntu20.04 (2 machines) and Windows10 (1 machines) cause errors. Some are pyfeatures.py cannot be imported, and some are path not found! I used this step\n",
    "- <code>git clone --recursive https://github.com/mapillary/OpenSfM</code>\n",
    "- <code>cd OpenSfM</code>\n",
    "- <code>git submodule update --init --recursive</code>\n",
    "- use dockerfile from the link https://stackoverflow.com/questions/42742067/opensfm-setup-py-file-returning-error-after-building-via-docker I checked and it says Ubuntu20.04\n",
    "- <code>docker build -t mapillary/opensfm .</code>\n",
    "- <code>docker run -ti mapillary/opensfm /bin/sh -c \"bin/opensfm_run_all data/berlin\"</code>\n",
    "\n",
    "Everything looks fine until I go to build\n",
    "- <code>bin/opensfm_run_all data/berlin</code>      I found error here\n",
    "- I did not try <code>python3 -m http.server</code>\n",
    "\n",
    "Another way, I used install dependencies\n",
    "- Python3\n",
    "- ceres solver (by hand) http://ceres-solver.org/installation.html and download from here https://github.com/ceres-solver/ceres-solver/releases\n",
    "- Finally, install NumPy, SciPy, Networkx, PyYAML, exifread, openMP\n",
    "- <code>git clone --recursive https://github.com/mapillary/OpenSfM</code>\n",
    "- cd OpenSfM\n",
    "- python3 setup.py build\n",
    "\n",
    "**Not work at all** (Died here: subprocess expression)\n",
    "\n",
    "**No compatable with Windows10** I have tried, yes, bad news.\n",
    "\n",
    "So... I have 3 plans, if some students cannot run OpenSfM.\n",
    "\n",
    "1. Do OpenSfM follow from your new instruction.\n",
    "2. Use OpenCV SfM (at the bottom of the lab instruction)\n",
    "3. Use SfMedu demo (python) from https://github.com/Alisa-Kunapinun/sfmedu_python3 I had modify them from python2 to python3 and make sure that both Windows and Linux can be run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that's done, if all is well, you should be able to view the three images of the Berlin Cathedral in data/berlin. Then you can execute the SfM pipeline:\n",
    "\n",
    "<code>OpenSfM$ ./bin/opensfm_run_all data/berlin</code>\n",
    "\n",
    "It will take a while to run. If successful, you'll get many messages, ending with something like\n",
    "\n",
    "<code>2020-07-06 10:00:28,312 INFO: Merging depthmaps</code>\n",
    "\n",
    "and the result will be a JSON file data/berlin/reconstruction.meshed.json. You can visualize the reconstructed point cloud with\n",
    "\n",
    "<code>OpenSfM$ python3 -m http.server</code>\n",
    "\n",
    "then you can visit http://localhost:8000/viewer/reconstruction.html#file=/data/berlin/reconstruction.meshed.json to see the reconstructed cameras and point cloud.\n",
    "\n",
    "If you see something like the image below, all is successful.\n",
    "\n",
    "<img src=\"img/lab07-1.jpg\" width=\"600\"/>\n",
    "\n",
    "A lot more detail is at the OpenSfM documentation page. We will use this information to see how to use OpenSfM properly in the rest of the session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get video\n",
    "\n",
    "Use the sample video for the lab. Extract images\n",
    "\n",
    "<code>\n",
    "OpenSfM$ cd data\n",
    "OpenSfM/data$ mkdir lab7-frames\n",
    "OpenSfM/data$ cd lab7-frames\n",
    "OpenSfM/data/lab7-frames$ ffmpeg -i ~/Downloads/robot.mp4 frame%03d.png\n",
    "</code>\n",
    "You should get 868 frames from the original video with names such as <code>frame001.png</code>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset for SfM\n",
    "Now you'll want to select a sequence of keyframes from the video. We'll think about how to do this automatically later, but here let's do it manually. Use two viewers to step through the sequence and get a subset of the frames that contain motion with perhaps 80% overlap between subsequent frames:\n",
    "\n",
    "<code>\n",
    "OpenSfM/data/lab7-frames$ eog frame001.png &\n",
    "OpenSfM/data/lab7-frames$ mkdir ../lab7-keyframes/\n",
    "OpenSfM/data/lab7-frames$ cp frame010.png ../lab7-keyframes/\n",
    "OpenSfM/data/lab7-frames$ eog ../lab7-keyframes/frame010.png &\n",
    "OpenSfM/data/lab7-frames$ cp frame120.png ../lab7-keyframes/\n",
    "OpenSfM/data/lab7-frames$ ...\n",
    "</code>\n",
    "\n",
    "I got 55 frames in this way. Take note of the motion blur, compression artifacts, reflections off the windows, and other issues. You may notice that image quality is much lower and that more frames per second are needed when the robot is turning.\n",
    "\n",
    "Next, copy the Berlin dataset configuration to your project directory:\n",
    "\n",
    "<code>\n",
    "OpenSfM/data/lab7-frames$ cd ../lab7-keyframes\n",
    "OpenSfM/data/lab7-keyframes$ cp ../berlin/config.yaml .\n",
    "OpenSfM/data/lab7-keyframes$ mkdir images\n",
    "OpenSfM/data/lab7-keyframes$ mv *.png images/\n",
    "OpenSfM/data/lab7-keyframes$ cd ../..\n",
    "OpenSfM$\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now you should be ready to try a run:\n",
    "\n",
    "OpenSfM$ ./bin/opensfm_run_all data/lab6-keyframes\n",
    "It will take a few minutes to obtain the reconstruction. You'll see that the system does the following:\n",
    "\n",
    "1. Extracts keypoints from every keyframe\n",
    "2. Tries to match keypoints between every pair of frames. You will probably observe the best matching for adjacent frames in the sequence.\n",
    "3. Builds up a 3D reconstruction, beginning with two frames that have good matches then performing resectioning for the remaining frames.\n",
    "4. Computes depth maps for each frame based on the 3D points' projections into the images.\n",
    "\n",
    "The resulting 3D reconstruction can then be viewed.\n",
    "\n",
    "<code>\n",
    "OpenSfM$ python3 -m http.server\n",
    "</code>\n",
    "\n",
    "And then you can visit http://localhost:8000/viewer/reconstruction.html#file=/data/lab6-keyframes/reconstruction.json\n",
    "\n",
    "Do you get a reasonable result? If not, try subsequences of the total keyframe sequence until you get something reasonable.\n",
    "\n",
    "Once you have a reasonable sparse reconstruction, you can view the dense one:\n",
    "\n",
    "<code>\n",
    "OpenSfM$ sudo apt-get install meshlab\n",
    "OpenSfM$ meslab data/lab6-keyframes/undistorted/depthmaps/merged.ply\n",
    "</code>\n",
    "\n",
    "You should be able to see something like this:\n",
    "\n",
    "<img src=\"img/lab07-2.png\" width=\"600\"/>\n",
    "\n",
    "The dense mesh makes clear that the reconstruction works better for objects that are far from the camera. Reflections seem to wreak havoc and lead to multiple false matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements\n",
    "\n",
    "Perform some further experiments in attempts to get better 3D reconstructions from OpenSfM. Some things to try:\n",
    "\n",
    "1. Change the parameter depthmap_min_consistent_views to 3 or more.\n",
    "2. Check the undistorted versions of the images in data/lab6-keyframes/undistorted/images/. Do you think that the automatically extracted distortion parameters are working well? Try using OpenCV to undistort the images in advance using the results from intrinsic calibration we got in previous labs. Do you get better results?\n",
    "3. Check the estimated camera parameters in data/lab6-keyframes/camera_models.json. Does it seem reasonable? Would giving your own parameters be better? Note that OpenSfM only uses a single focal length, which is assumed the same for x and y, while our calibration generated a camera matrix with fx = 807 and fy = 804. Is this a big enough difference to affect OpenSfM? Redo the undistortion but this time output undistorted images with a single focal length, and then provide this focal length to OpenSfM by copying camera_models.json to camera_models_overrides.json prior to reconstruction. Note that the OpenSfM focal length fsfm is related to the OpenCV focal length fcv by fsfm = 2 * fcv / w, where w is the image width (1920 in our case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thought experiments\n",
    "\n",
    "Consider the following, discuss in your report, and make some recommendations:\n",
    "\n",
    "1. The biggest bottleneck to the reconstruction was extracting keyframes. Think about how you could write a program to automatically extract keyframes using, for example, the Lucas and Kanade optical flow method we looked at in the early labs. What would you require of the optical flow between keyframe i and keyframe i+1 ? For some ideas, see our VISAPP paper on the subject and other references you find online.\n",
    "\n",
    "2. Reflections on the floor seem to cause trouble for reconstruction, and the textureless floor surface means we almost never get good matches for points on the floor. Discuss how we could use our semantic segmentation model for floor/no-floor to automatically ignore the floor in the SfM (think about the masks provided for the Berlin Cathedral dataset), and also think about how, once we have the extrinsic parameters and scene scale, we could reconstruct a dense 3D mesh for the floor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing SfM in OpenCV\n",
    "\n",
    "From: Mastering OpenCV4 Packtpub\n",
    "\n",
    "*Note: Can try in Linux only*\n",
    "\n",
    "OpenCV has an abundance of tools to implement a full-fledged SfM pipeline from first principles. However, such a task is very demanding and beyond the scope of this chapter. The former edition of this book presented just a small taste of what building such a system will entail, but luckily now we have at our disposal a tried and tested technique integrated right into OpenCV's API. Although the sfm module allows us to get away with simply providing a non-parametric function with a list of images to crunch and receive a fully reconstructed scene with a sparse point cloud and camera poses, we will not take that route. Instead, we will see in this section some useful methods that will allow us to have much more control over the reconstruction and exemplify some of the topics we discussed in the last section, as well as be more robust to noise.\n",
    "\n",
    "This section will begin with the very basics of SfM: matching images using key points and feature descriptors. We will then advance to finding tracks, and multiple views of similar features through the image set, using a match graph. We proceed with 3D reconstruction, 3D visualization, and finally MVS with OpenMVS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image feature matching\n",
    "\n",
    "SfM, as presented in the last section, relies on the understanding of the geometric relationship between images as it pertains to the visible objects in them. We saw that we can calculate the exact motion between two images with sufficient information on how the objects in the images move. The essential or fundamental matrices, which can be estimated linearly from image features, can be decomposed to the rotation and translation elements that define a 3D rigid transform. Thereafter, this transform can help us triangulate the 3D position of the objects, from the 3D-2D projection equations or from a dense stereo matching over the rectified epilines. It all begins with image feature matching, so we will see how to obtain robust and noise-free matching.\n",
    "\n",
    "OpenCV has an extensive offering of 2D feature detectors (also called extractors) and descriptors. Features are designed to be invariant to image deformations so they can be matched through translation, rotation, scaling, and other more complicated transformations (affine, projective) of the objects in the scene. One of the latest additions to OpenCV's APIs is the AKAZE feature extractor and detector, which presents a very good compromise between speed of calculation and robustness to transformation. AKAZE was shown to outperform other prominent features, such as ORB (short for Oriented BRIEF) and SURF (short for Speeded Up Robust Features).\n",
    "\n",
    "The following snippet will extract an AKAZE key point, calculate AKAZE features for each of the images we collect in imagesFilenames, and save them in the keypoints and descriptors arrays respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto detector = AKAZE::create();\n",
    "auto extractor = AKAZE::create();\n",
    "\n",
    "for (const auto& i : imagesFilenames) {\n",
    "    Mat grayscale;\n",
    "    cvtColor(images[i], grayscale, COLOR_BGR2GRAY);\n",
    "    detector->detect(grayscale, keypoints[i]);\n",
    "    extractor->compute(grayscale, keypoints[i], descriptors[i]);\n",
    "\n",
    "    CV_LOG_INFO(TAG, \"Found \" + to_string(keypoints[i].size()) + \" \n",
    "    keypoints in \" + i);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we also convert the images to grayscale; however, this step may be omitted and the results will not suffer.\n",
    "\n",
    "Here's a visualization of the detected features in two adjacent images. Notice how many of them repeat; this is known as feature repeatability, which is one of the most desired functions in a good feature extractor:\n",
    "\n",
    "<img src=\"img/lab07-3.png\" width=\"800\"/>\n",
    "\n",
    "Next up is matching the features between every pair of images. OpenCV provides an excellent feature matching suite. AKAZE feature descriptors are binary, meaning they cannot be regarded as binary-encoded numbers when matched; they must be compared on the bit level with bit-wise operators. OpenCV offers a Hamming distance metric for binary feature matchers, which essentially count the number of incorrect matches between the two-bit sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector<DMatch> matchWithRatioTest(const DescriptorMatcher& matcher, \n",
    "                                  const Mat& desc1, \n",
    "                                  const Mat& desc2) \n",
    "{\n",
    "    // Raw match\n",
    "    vector< vector<DMatch> > nnMatch;\n",
    "    matcher.knnMatch(desc1, desc2, nnMatch, 2);\n",
    "\n",
    "    // Ratio test filter\n",
    "    vector<DMatch> ratioMatched;\n",
    "    for (size_t i = 0; i < nnMatch.size(); i++) {\n",
    "        const DMatch first = nnMatch[i][0];\n",
    "        const float dist1 = nnMatch[i][0].distance;\n",
    "        const float dist2 = nnMatch[i][1].distance;\n",
    "\n",
    "        if (dist1 < MATCH_RATIO_THRESHOLD * dist2) {\n",
    "            ratioMatched.push_back(first);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return ratioMatched;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding function not only invokes our matcher (for example, a BFMatcher(NORM_HAMMING)) regularly, it also performs the ratio test. This simple test is a very fundamental concept in many computer vision algorithms that rely on feature matching (such as SfM, panorama stitching, sparse tracking, and more). Instead of looking for a single match for a feature from image A in image B, we look for two matches in image B and make sure there is no confusion. Confusion in matching may arise if two potential matching-feature descriptors are too similar (in terms of their distance metric) and we cannot tell which of them is the correct match for the query, so we discard them both to prevent confusion.\n",
    "\n",
    "Next, we implement a reciprocity filter. This filter only allows feature matches that match (with a ratio test) in A to B, as well as B to A. Essentially, this is making sure there's a one-to-one match between features in image A and those in image B: a symmetric match. The reciprocity filter removes even more ambiguity and contributes to a cleaner, more robust match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Match with ratio test filter\n",
    "vector<DMatch> match = matchWithRatioTest(matcher, descriptors[imgi], descriptors[imgj]);\n",
    "\n",
    "// Reciprocity test filter\n",
    "vector<DMatch> matchRcp = matchWithRatioTest(matcher, descriptors[imgj], descriptors[imgi]);\n",
    "vector<DMatch> merged;\n",
    "for (const DMatch& dmrecip : matchRcp) {\n",
    "    bool found = false;\n",
    "    for (const DMatch& dm : match) {\n",
    "        // Only accept match if 1 matches 2 AND 2 matches 1.\n",
    "        if (dmrecip.queryIdx == dm.trainIdx and dmrecip.trainIdx == \n",
    "        dm.queryIdx) {\n",
    "            merged.push_back(dm);\n",
    "            found = true;\n",
    "            break;\n",
    "        }\n",
    "    }\n",
    "    if (found) {\n",
    "        continue;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we apply the epipolar constraint. Every two images that have a valid rigid transformation between them would comply with the epipolar constraint over their feature points, , and those who don't pass this test (with sufficient success) are likely not a good match and may contribute to noise. We achieve this by calculating the fundamental matrix with a voting algorithm (RANSAC) and checking for the ratio of inliers to outliers. We apply a threshold to discard matches with a low survival rate with respect to the original match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Fundamental matrix filter\n",
    "vector<uint8_t> inliersMask(merged.size());\n",
    "vector<Point2f> imgiPoints, imgjPoints;\n",
    "for (const DMatch& m : merged) {\n",
    "    imgiPoints.push_back(keypoints[imgi][m.queryIdx].pt);\n",
    "    imgjPoints.push_back(keypoints[imgj][m.trainIdx].pt);\n",
    "}\n",
    "findFundamentalMat(imgiPoints, imgjPoints, inliersMask);\n",
    "\n",
    "vector<DMatch> final;\n",
    "for (size_t m = 0; m < merged.size(); m++) {\n",
    "    if (inliersMask[m]) {\n",
    "        final.push_back(merged[m]);\n",
    "    }\n",
    "}\n",
    "\n",
    "if ((float)final.size() / (float)match.size() < PAIR_MATCH_SURVIVAL_RATE) {\n",
    "    CV_LOG_INFO(TAG, \"Final match '\" + imgi + \"'->'\" + imgj + \"' has less than \"+to_string(PAIR_MATCH_SURVIVAL_RATE)+\" inliers from orignal. Skip\");\n",
    "    continue;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the effect of each of the filtering steps, raw match, ratio, reciprocity, and epipolar, in the following figure:\n",
    "\n",
    "<img src=\"img/lab07-4.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding feature tracks\n",
    "\n",
    "The concept of feature tracks was introduced in SfM literature as early as 1992 in Tomasi and Kanade's work (Shape and Motion from Image Streams, 1992) and made famous in Snavely and Szeliski's seminal photo tourism work from 2007 for large-scale unconstrained reconstructions. Tracks are simply the 2D positions of a single scene feature, an interesting point, over a number of views. Tracks are important since they maintain consistency across frames than can be composed into a global optimization problem, as Snavely suggested. Tracks are specifically important to us since OpenCV's sfm module allows to reconstruct a scene by providing just the 2D tracks across all the views:\n",
    "\n",
    "<img src=\"img/lab07-5.png\" width=\"800\"/>\n",
    "\n",
    "Having already found a pair-wise match between all views, we have the required information to find tracks within those matched features. If we follow feature i in the first image to the second image through the match, then from the second image to the third image through their own match, and so on, we would end up with its track. This sort of bookkeeping can easily become too hard to implement in a straightforward fashion with standard data structures. However, it can be simply done if we represent all the matches in a match graph. Each node in the graph would be a feature detected in a single image, and edges would be the matches we recovered. From the feature nodes of the first image, we would have many edges to the feature nodes of the second image, third image, fourth image, and so on (for matches not discarded by our filters). Since our matches are reciprocal (symmetric), the graph can be undirected. Moreover, the reciprocity test ensures that for feature i in the first image, there is only one matching feature j in the second image, and vice versa: feature j will only match back to feature i.\n",
    "\n",
    "The following is a visual example of such a match graph. The node colors represent the image from which the feature point (node) has originated. Edges represent a match between image features. We can notice the very strong pattern of a feature matching chain from the first image to the last:\n",
    "\n",
    "<img src=\"img/lab07-6.png\" width=\"800\"/>\n",
    "\n",
    "To code the match graph, we can use the Boost Graph Library (BGL), which has an extensive API for graph processing and algorithms. Constructing the graph is straightforward; we simply augment the nodes with the image ID and feature ID, so later we can trace back the origin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace boost;\n",
    "\n",
    "struct ImageFeature {\n",
    "    string image;\n",
    "    size_t featureID;\n",
    "};\n",
    "typedef adjacency_list < listS, vecS, undirectedS, ImageFeature > Graph;\n",
    "typedef graph_traits < Graph >::vertex_descriptor Vertex;\n",
    "map<pair<string, int>, Vertex> vertexByImageFeature;\n",
    "\n",
    "Graph g;\n",
    "\n",
    "// Add vertices - image features\n",
    "for (const auto& imgi : keypoints) {\n",
    "    for (size_t i = 0; i < imgi.second.size(); i++) {\n",
    "        Vertex v = add_vertex(g);\n",
    "        g[v].image = imgi.first;\n",
    "        g[v].featureID = i;\n",
    "        vertexByImageFeature[make_pair(imgi.first, i)] = v;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Add edges - feature matches\n",
    "for (const auto& match : matches) {\n",
    "    for (const DMatch& dm : match.second) {\n",
    "        Vertex& vI = vertexByImageFeature[make_pair(match.first.first, dm.queryIdx)];\n",
    "        Vertex& vJ = vertexByImageFeature[make_pair(match.first.second, dm.trainIdx)];\n",
    "        add_edge(vI, vJ, g);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at a visualization of the resulting graph (using boost::write_graphviz()), we can see many cases where our matching is erroneous. A bad match chain will involve more than one feature from the same image in the chain. We marked a few such instances in the following figure; notice some chains have two or more nodes with the same color:\n",
    "\n",
    "<img src=\"img/lab07-7.png\" width=\"800\"/>\n",
    "\n",
    "We can notice the chains are essentially connected components in the graph. Extracting the components is simple using boost::connected_components():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Get connected components\n",
    "std::vector<int> component(num_vertices(gFiltered), -1);\n",
    "int num = connected_components(gFiltered, &component[0]);\n",
    "map<int, vector<Vertex> > components;\n",
    "for (size_t i = 0; i != component.size(); ++i) {\n",
    "    if (component[i] >= 0) {\n",
    "        components[component[i]].push_back(i);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter out the bad components (with more than one feature from any one image) to get a clean match graph.\n",
    "\n",
    "### 3D reconstruction and visualization\n",
    "\n",
    "Having obtained the tracks in principle, we need to align them in a data structure that OpenCV's SfM module expects. Unfortunately, the sfm module is not very well documented, so this part we have to figure out on our own from the source code. We will be invoking the following function under the cv::sfm:: namespace, which can be found in opencv_contrib/modules/sfm/include/opencv2/sfm/reconstruct.hpp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "void reconstruct(InputArrayOfArrays points2d, OutputArray Ps, OutputArray points3d, InputOutputArray K, bool is_projective = false);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The opencv_contrib/modules/sfm/src/simple_pipeline.cpp file has a major hint as to what that function expects as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static void\n",
    "parser_2D_tracks( const std::vector<Mat> &points2d, libmv::Tracks &tracks )\n",
    "{\n",
    "  const int nframes = static_cast<int>(points2d.size());\n",
    "  for (int frame = 0; frame < nframes; ++frame) {\n",
    "    const int ntracks = points2d[frame].cols;\n",
    "    for (int track = 0; track < ntracks; ++track) {\n",
    "      const Vec2d track_pt = points2d[frame].col(track);\n",
    "      if ( track_pt[0] > 0 && track_pt[1] > 0 )\n",
    "        tracks.Insert(frame, track, track_pt[0], track_pt[1]);\n",
    "    }\n",
    "  }\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the sfm module uses a reduced version of libmv (https://developer.blender.org/tag/libmv/), which is a well-established SfM package used for 3D reconstruction for cinema production with the Blender 3D (https://www.blender.org/) graphics software.\n",
    "\n",
    "We can tell the tracks need to be placed in a vector of multiple individual cv::Mat, where each contains an aligned list of cv::Vec2d as columns, meaning it has two rows of double. We can also deduce that missing (unmatched) feature points in a track will have a negative coordinate. The following snippet will extract tracks in the desired data structure from the match graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector<Mat> tracks(nViews); // Initialize to number of views\n",
    "\n",
    "// Each component is a track\n",
    "const size_t nViews = imagesFilenames.size();\n",
    "tracks.resize(nViews);\n",
    "for (int i = 0; i < nViews; i++) {\n",
    "    tracks[i].create(2, components.size(), CV_64FC1);\n",
    "    tracks[i].setTo(-1.0); // default is (-1, -1) - no match\n",
    "}\n",
    "int i = 0;\n",
    "for (auto c = components.begin(); c != components.end(); ++c, ++i) {\n",
    "    for (const int v : c->second) {\n",
    "        const int imageID = imageIDs[g[v].image];\n",
    "        const size_t featureID = g[v].featureID;\n",
    "        const Point2f p = keypoints[g[v].image][featureID].pt;\n",
    "        tracks[imageID].at<double>(0, i) = p.x;\n",
    "        tracks[imageID].at<double>(1, i) = p.y;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow up with running the reconstruction function, collecting the sparse 3D point cloud and the color for each 3D point, and afterward, visualize the results (using functions from cv::viz::):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv::sfm::reconstruct(tracks, Rs, Ts, K, points3d, true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will produce a sparse reconstruction with a point cloud and camera positions, visualized in the following image:\n",
    "\n",
    "<img src=\"img/lab07-8.png\" width=\"800\"/>\n",
    "\n",
    "Re-projecting the 3D points back on the 2D images we can validate a correct reconstruction:\n",
    "\n",
    "<img src=\"img/lab07-9.png\" width=\"800\"/>\n",
    "\n",
    "See the entire code for reconstruction and visualization in the accompanying source repository.\n",
    "\n",
    "Notice the reconstruction is very sparse; we only see 3D points where features have matched. This doesn't make for a very appealing effect when getting the geometry of objects in the scene. In many cases, SfM pipelines do not conclude with a sparse reconstruction, which is not useful for many applications, such as 3D scanning. Next, we will see how to get a dense reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MVS for dense reconstruction\n",
    "With the sparse 3D point cloud and the positions of the cameras, we can proceed with dense reconstruction using MVS. We already learned the basic concept of MVS in the first section; however, we do not need to implement this from scratch, but rather we can use the OpenMVS project. To use OpenMVS for cloud densifying, we must save our project in a specialized format. OpenMVS provides a class for saving and loading .mvs projects, the MVS::Interface class, defined in MVS/Interface.h.\n",
    "\n",
    "Let's start with the camera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MVS::Interface interface;\n",
    "MVS::Interface::Platform p;\n",
    "\n",
    "// Add camera\n",
    "MVS::Interface::Platform::Camera c;\n",
    "c.K = Matx33d(K_); // The intrinsic matrix as refined by the bundle adjustment\n",
    "c.R = Matx33d::eye(); // Camera doesn't have any inherent rotation\n",
    "c.C = Point3d(0,0,0); // or translation\n",
    "c.name = \"Camera1\";\n",
    "const Size imgS = images[imagesFilenames[0]].size();\n",
    "c.width = imgS.width; // Size of the image, to normalize the intrinsics\n",
    "c.height = imgS.height;\n",
    "p.cameras.push_back(c);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When adding the camera poses (views), we must take care. OpenMVS expects to get the rotation and center of the camera, and not the camera pose matrix for point projection $[R|t]$. We therefore must translate the translation vector to represent the center of the camera by applying the inverse rotation $-R^tt$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Add views\n",
    "p.poses.resize(Rs.size());\n",
    "for (size_t i = 0; i < Rs.size(); ++i) {\n",
    "    Mat t = -Rs[i].t() * Ts[i]; // Camera *center*\n",
    "    p.poses[i].C.x = t.at<double>(0);\n",
    "    p.poses[i].C.y = t.at<double>(1);\n",
    "    p.poses[i].C.z = t.at<double>(2);\n",
    "    Rs[i].convertTo(p.poses[i].R, CV_64FC1);\n",
    "\n",
    "    // Add corresponding image (make sure index aligns)\n",
    "    MVS::Interface::Image image;\n",
    "    image.cameraID = 0;\n",
    "    image.poseID = i;\n",
    "    image.name = imagesFilenames[i];\n",
    "    image.platformID = 0;\n",
    "    interface.images.push_back(image);\n",
    "}\n",
    "p.name = \"Platform1\";\n",
    "interface.platforms.push_back(p);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding the point cloud to the Interface as well, we can proceed with the cloud densifying in the command line:\n",
    "\n",
    "<code>\n",
    "$ ${openMVS}/build/bin/DensifyPointCloud -i crazyhorse.mvs\n",
    "18:48:32 [App ] Command line: -i crazyhorse.mvs\n",
    "18:48:32 [App ] Camera model loaded: platform 0; camera 0; f 0.896x0.896; poses 7\n",
    "18:48:32 [App ] Image loaded 0: P1000965.JPG\n",
    "18:48:32 [App ] Image loaded 1: P1000966.JPG\n",
    "18:48:32 [App ] Image loaded 2: P1000967.JPG\n",
    "18:48:32 [App ] Image loaded 3: P1000968.JPG\n",
    "18:48:32 [App ] Image loaded 4: P1000969.JPG\n",
    "18:48:32 [App ] Image loaded 5: P1000970.JPG\n",
    "18:48:32 [App ] Image loaded 6: P1000971.JPG\n",
    "18:48:32 [App ] Scene loaded from interface format (11ms):\n",
    "7 images (7 calibrated) with a total of 5.25 MPixels (0.75 MPixels/image)\n",
    "1557 points, 0 vertices, 0 faces\n",
    "18:48:32 [App ] Preparing images for dense reconstruction completed: 7 images (125ms)\n",
    "18:48:32 [App ] Selecting images for dense reconstruction completed: 7 images (5ms)\n",
    "Estimated depth-maps 7 (100%, 1m44s705ms)\n",
    "Filtered depth-maps 7 (100%, 1s671ms)\n",
    "Fused depth-maps 7 (100%, 421ms)\n",
    "18:50:20 [App ] Depth-maps fused and filtered: 7 depth-maps, 1653963 depths, 263027 points (16%%) (1s684ms)\n",
    "18:50:20 [App ] Densifying point-cloud completed: 263027 points (1m48s263ms)\n",
    "18:50:21 [App ] Scene saved (489ms):\n",
    "7 images (7 calibrated)\n",
    "263027 points, 0 vertices, 0 faces\n",
    "18:50:21 [App ] Point-cloud saved: 263027 points (46ms)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process might take a few minutes to complete. However, once it's done, the results are very impressive. The dense point cloud has a whopping 263,027 3D points, compared to just 1,557 in the sparse cloud. We can visualize the dense OpenMVS project using the Viewer app bundled in OpenMVS:\n",
    "\n",
    "<img src=\"img/lab07-10.png\" width=\"800\"/>\n",
    "\n",
    "OpenMVS has several more functions to complete the reconstruction, such as extracting a triangular mesh from the dense point cloud.\n",
    "\n",
    "You can load image samples from <link>[here](img/crazyhorse)</link>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C++ (Full code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <iostream>\n",
    "#include <algorithm>\n",
    "#include <string>\n",
    "#include <numeric>\n",
    "\n",
    "#define CERES_FOUND true\n",
    "\n",
    "#include <opencv2/opencv.hpp>\n",
    "#include <opencv2/sfm.hpp>\n",
    "#include <opencv2/viz.hpp>\n",
    "#include <opencv2/core/utils/logger.hpp>\n",
    "#include <opencv2/core/utils/filesystem.hpp>\n",
    "#include <opencv2/xfeatures2d.hpp>\n",
    "\n",
    "#include <boost/filesystem.hpp>\n",
    "#include <boost/graph/graph_traits.hpp>\n",
    "#include <boost/graph/adjacency_list.hpp>\n",
    "#include <boost/graph/connected_components.hpp>\n",
    "#include <boost/graph/graphviz.hpp>\n",
    "\n",
    "#define _USE_OPENCV true\n",
    "#include <libs/MVS/Interface.h>\n",
    "\n",
    "using namespace cv;\n",
    "using namespace std;\n",
    "namespace fs = boost::filesystem;\n",
    "\n",
    "class StructureFromMotion {\n",
    "\n",
    "public:\n",
    "    StructureFromMotion(const string& dir,\n",
    "            const float matchSurvivalRate = 0.5f,\n",
    "            const bool viz = false,\n",
    "            const string mvs = \"\",\n",
    "            const string cloud = \"\",\n",
    "            const bool saveDebug = false)\n",
    "        :PAIR_MATCH_SURVIVAL_RATE(matchSurvivalRate),\n",
    "         visualize(viz),\n",
    "         saveMVS(mvs),\n",
    "         saveCloud(cloud),\n",
    "         saveDebugVisualizations(saveDebug)\n",
    "\n",
    "    {\n",
    "        findImagesInDiretcory(dir);\n",
    "    }\n",
    "\n",
    "    void runSfM() {\n",
    "        extractFeatures();\n",
    "        matchFeatures();\n",
    "        buildTracks();\n",
    "        reconstructFromTracks();\n",
    "        if (visualize) {\n",
    "            visualize3D();\n",
    "        }\n",
    "        if (saveMVS != \"\") {\n",
    "            saveToMVSFile();\n",
    "        }\n",
    "        if (saveCloud != \"\") {\n",
    "            CV_LOG_INFO(TAG, \"Save point cloud to: \" + saveCloud);\n",
    "            viz::writeCloud(saveCloud, pointCloud, pointCloudColor);\n",
    "        }\n",
    "    }\n",
    "\n",
    "private:\n",
    "    void findImagesInDiretcory(const string& dir) {\n",
    "        CV_LOG_INFO(TAG, \"Finding images in \" + dir);\n",
    "\n",
    "        utils::fs::glob(dir, \"*.jpg\", imagesFilenames);\n",
    "        utils::fs::glob(dir, \"*.JPG\", imagesFilenames);\n",
    "        utils::fs::glob(dir, \"*.png\", imagesFilenames);\n",
    "        utils::fs::glob(dir, \"*.PNG\", imagesFilenames);\n",
    "\n",
    "        std::sort(imagesFilenames.begin(), imagesFilenames.end());\n",
    "\n",
    "        CV_LOG_INFO(TAG, \"Found \" + std::to_string(imagesFilenames.size()) + \" images\");\n",
    "\n",
    "        CV_LOG_INFO(TAG, \"Reading images...\");\n",
    "        for (const auto& i : imagesFilenames) {\n",
    "            CV_LOG_INFO(TAG, i);\n",
    "            images[i] = imread(i);\n",
    "            imageIDs[i] = images.size() - 1;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    void extractFeatures() {\n",
    "        CV_LOG_INFO(TAG, \"Extract Features\");\n",
    "\n",
    "        auto detector = AKAZE::create();\n",
    "        auto extractor = AKAZE::create();\n",
    "\n",
    "        for (const auto& i : imagesFilenames) {\n",
    "            Mat grayscale;\n",
    "            cvtColor(images[i], grayscale, COLOR_BGR2GRAY);\n",
    "            detector->detect(grayscale, keypoints[i]);\n",
    "            extractor->compute(grayscale, keypoints[i], descriptors[i]);\n",
    "\n",
    "            CV_LOG_INFO(TAG, \"Found \" + to_string(keypoints[i].size()) + \" keypoints in \" + i);\n",
    "\n",
    "            if (saveDebugVisualizations) {\n",
    "                Mat out;\n",
    "                drawKeypoints(images[i], keypoints[i], out, Scalar(0,0,255));\n",
    "                imwrite(fs::basename(fs::path(i)) + \"_features.jpg\", out);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    vector<DMatch> matchWithRatioTest(const DescriptorMatcher& matcher, const Mat& desc1, const Mat& desc2) {\n",
    "        // Raw match\n",
    "        vector< vector<DMatch> > nnMatch;\n",
    "        matcher.knnMatch(desc1, desc2, nnMatch, 2);\n",
    "\n",
    "        // Ratio test filter\n",
    "        vector<DMatch> ratioMatched;\n",
    "        for (size_t i = 0; i < nnMatch.size(); i++) {\n",
    "            DMatch first = nnMatch[i][0];\n",
    "            float dist1 = nnMatch[i][0].distance;\n",
    "            float dist2 = nnMatch[i][1].distance;\n",
    "\n",
    "            if (dist1 < MATCH_RATIO_THRESHOLD * dist2) {\n",
    "                ratioMatched.push_back(first);\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return ratioMatched;\n",
    "    }\n",
    "\n",
    "    void matchFeatures() {\n",
    "        CV_LOG_INFO(TAG, \"Match Features\");\n",
    "\n",
    "        BFMatcher matcher(NORM_HAMMING);\n",
    "\n",
    "        for (size_t i = 0; i < imagesFilenames.size() - 1; ++i) {\n",
    "            for (size_t j = i + 1; j < imagesFilenames.size(); ++j) {\n",
    "                const string imgi = imagesFilenames[i];\n",
    "                const string imgj = imagesFilenames[j];\n",
    "\n",
    "                // Match with ratio test filter\n",
    "                vector<DMatch> match = matchWithRatioTest(matcher, descriptors[imgi], descriptors[imgj]);\n",
    "\n",
    "                // Reciprocity test filter\n",
    "                vector<DMatch> matchRcp = matchWithRatioTest(matcher, descriptors[imgj], descriptors[imgi]);\n",
    "                vector<DMatch> merged;\n",
    "                for (const DMatch& dmr : matchRcp) {\n",
    "                    bool found = false;\n",
    "                    for (const DMatch& dm : match) {\n",
    "                        // Only accept match if 1 matches 2 AND 2 matches 1.\n",
    "                        if (dmr.queryIdx == dm.trainIdx and dmr.trainIdx == dm.queryIdx) {\n",
    "                            merged.push_back(dm);\n",
    "                            found = true;\n",
    "                            break;\n",
    "                        }\n",
    "                    }\n",
    "                    if (found) {\n",
    "                        continue;\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                // Fundamental matrix filter\n",
    "                vector<uint8_t> inliersMask(merged.size());\n",
    "                vector<Point2f> imgiPoints, imgjPoints;\n",
    "                for (const auto& m : merged) {\n",
    "                    imgiPoints.push_back(keypoints[imgi][m.queryIdx].pt);\n",
    "                    imgjPoints.push_back(keypoints[imgj][m.trainIdx].pt);\n",
    "                }\n",
    "                findFundamentalMat(imgiPoints, imgjPoints, inliersMask);\n",
    "\n",
    "                vector<DMatch> final;\n",
    "                for (size_t m = 0; m < merged.size(); m++) {\n",
    "                    if (inliersMask[m]) {\n",
    "                        final.push_back(merged[m]);\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                if ((float)final.size() / (float)match.size() < PAIR_MATCH_SURVIVAL_RATE) {\n",
    "                    CV_LOG_INFO(TAG, \"Final match '\" + imgi + \"'->'\" + imgj + \"' has less than \"+to_string(PAIR_MATCH_SURVIVAL_RATE)+\" inliers from orignal. Skip\");\n",
    "                    continue;\n",
    "                }\n",
    "\n",
    "                matches[make_pair(imgi, imgj)] = final;\n",
    "\n",
    "                CV_LOG_INFO(TAG, \"Matching \" + imgi + \" and \" + imgj + \": \" + to_string(final.size()) + \" / \" + to_string(match.size()));\n",
    "\n",
    "                if (saveDebugVisualizations) {\n",
    "                    Mat out;\n",
    "                    vector<DMatch> rawmatch;\n",
    "                    matcher.match(descriptors[imgi], descriptors[imgj], rawmatch);\n",
    "                    vector<pair<string, vector<DMatch>& > > showList{\n",
    "                            {\"Raw Match\", rawmatch},\n",
    "                            {\"Ratio Test Filter\", match},\n",
    "                            {\"Reciprocal Filter\", merged},\n",
    "                            {\"Epipolar Filter\", final}\n",
    "                    };\n",
    "                    for (size_t i = 0; i< showList.size(); i++) {\n",
    "                        drawMatches(images[imgi], keypoints[imgi],\n",
    "                                    images[imgj], keypoints[imgj],\n",
    "                                    showList[i].second, out, CV_RGB(255,0,0));\n",
    "                        putText(out, showList[i].first, Point(10,50), FONT_HERSHEY_COMPLEX, 2.0, CV_RGB(255,255,255), 2);\n",
    "                        putText(out, \"# Matches: \" + to_string(showList[i].second.size()), Point(10,100), FONT_HERSHEY_COMPLEX, 1.0, CV_RGB(255,255,255));\n",
    "                        imwrite(fs::basename(fs::path(imgi)) + \"_\" + fs::basename(fs::path(imgj)) + \"_\" + to_string(i) + \".jpg\", out);\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    void buildTracks() {\n",
    "        CV_LOG_INFO(TAG, \"Build tracks\");\n",
    "\n",
    "        using namespace boost;\n",
    "\n",
    "        struct ImageFeature {\n",
    "            string image;\n",
    "            size_t featureID;\n",
    "        };\n",
    "        typedef adjacency_list < listS, vecS, undirectedS, ImageFeature > Graph;\n",
    "        typedef graph_traits < Graph >::vertex_descriptor Vertex;\n",
    "\n",
    "        map<pair<string, int>, Vertex> vertexByImageFeature;\n",
    "\n",
    "        Graph g;\n",
    "\n",
    "        // Add vertices - image features\n",
    "        for (const auto& imgi : keypoints) {\n",
    "            for (size_t i = 0; i < imgi.second.size(); i++) {\n",
    "                Vertex v = add_vertex(g);\n",
    "                g[v].image = imgi.first;\n",
    "                g[v].featureID = i;\n",
    "                vertexByImageFeature[make_pair(imgi.first, i)] = v;\n",
    "            }\n",
    "        }\n",
    "\n",
    "        // Add edges - feature matches\n",
    "        for (const auto& match : matches) {\n",
    "            for (const DMatch& dm : match.second) {\n",
    "                Vertex& vI = vertexByImageFeature[make_pair(match.first.first, dm.queryIdx)];\n",
    "                Vertex& vJ = vertexByImageFeature[make_pair(match.first.second, dm.trainIdx)];\n",
    "                add_edge(vI, vJ, g);\n",
    "            }\n",
    "        }\n",
    "\n",
    "        using Filtered  = filtered_graph<Graph, keep_all, std::function<bool(Vertex)>>;\n",
    "        Filtered gFiltered(g, keep_all{}, [&g](Vertex vd) { return degree(vd, g) > 0; });\n",
    "\n",
    "        // Get connected components\n",
    "        std::vector<int> component(num_vertices(gFiltered), -1);\n",
    "        int num = connected_components(gFiltered, &component[0]);\n",
    "        map<int, vector<Vertex> > components;\n",
    "        for (size_t i = 0; i != component.size(); ++i) {\n",
    "            if (component[i] >= 0) {\n",
    "                components[component[i]].push_back(i);\n",
    "            }\n",
    "        }\n",
    "        // Filter bad components (with more than 1 feature from a single image)\n",
    "        std::vector<int> vertexInGoodComponent(num_vertices(gFiltered), -1);\n",
    "        map<int, vector<Vertex> > goodComponents;\n",
    "        for (const auto& c : components) {\n",
    "            set<string> imagesInComponent;\n",
    "            bool isComponentGood = true;\n",
    "            for (int j = 0; j < c.second.size(); ++j) {\n",
    "                const string imgId = g[c.second[j]].image;\n",
    "                if (imagesInComponent.count(imgId) > 0) {\n",
    "                    // Image already represented in this component\n",
    "                    isComponentGood = false;\n",
    "                    break;\n",
    "                } else {\n",
    "                    imagesInComponent.insert(imgId);\n",
    "                }\n",
    "            }\n",
    "            if (isComponentGood) {\n",
    "                for (int j = 0; j < c.second.size(); ++j) {\n",
    "                    vertexInGoodComponent[c.second[j]] = 1;\n",
    "                }\n",
    "                goodComponents[c.first] = c.second;\n",
    "            }\n",
    "        }\n",
    "\n",
    "        Filtered gGoodComponents(g, keep_all{}, [&vertexInGoodComponent](Vertex vd) {\n",
    "            return vertexInGoodComponent[vd] > 0;\n",
    "        });\n",
    "\n",
    "        CV_LOG_INFO(TAG, \"Total number of components found: \" + to_string(components.size()));\n",
    "        CV_LOG_INFO(TAG, \"Number of good components: \" + to_string(goodComponents.size()));\n",
    "        const int accum = std::accumulate(goodComponents.begin(),\n",
    "                                          goodComponents.end(), 0,\n",
    "                                          [](int a, pair<const int, vector<Vertex> >& v){\n",
    "                                                    return a+v.second.size();\n",
    "                                                });\n",
    "        CV_LOG_INFO(TAG, \"Average component size: \" + to_string((float)accum / (float)(goodComponents.size())));\n",
    "\n",
    "        if (saveDebugVisualizations) {\n",
    "            struct my_node_writer {\n",
    "                my_node_writer(Graph& g_, const map<string,int>& iid_) : g (g_), iid(iid_) {};\n",
    "                void operator()(std::ostream& out, Vertex v) {\n",
    "                    const int imgId = iid[g[v].image];\n",
    "                    out << \" [label=\\\"\" << imgId << \"\\\" colorscheme=\\\"accent8\\\" fillcolor=\"<<(imgId+1)<<\" style=filled]\";\n",
    "                };\n",
    "                Graph g;\n",
    "                map<string,int> iid;\n",
    "            };\n",
    "            std::ofstream ofs(\"match_graph_good_components.dot\");\n",
    "            write_graphviz(ofs, gGoodComponents, my_node_writer(g, imageIDs));\n",
    "            std::ofstream ofsf(\"match_graph_filtered.dot\");\n",
    "            write_graphviz(ofsf, gFiltered, my_node_writer(g, imageIDs));\n",
    "        }\n",
    "\n",
    "        // Each component is a track\n",
    "        const size_t nViews = imagesFilenames.size();\n",
    "        tracks.resize(nViews);\n",
    "        for (int i = 0; i < nViews; i++) {\n",
    "            tracks[i].create(2, goodComponents.size(), CV_64FC1);\n",
    "            tracks[i].setTo(-1.0);\n",
    "        }\n",
    "        int i = 0;\n",
    "        for (auto c = goodComponents.begin(); c != goodComponents.end(); ++c, ++i) {\n",
    "            for (const int v : c->second) {\n",
    "                const int imageID = imageIDs[g[v].image];\n",
    "                const size_t featureID = g[v].featureID;\n",
    "                const Point2f p = keypoints[g[v].image][featureID].pt;\n",
    "                tracks[imageID].at<double>(0, i) = p.x;\n",
    "                tracks[imageID].at<double>(1, i) = p.y;\n",
    "            }\n",
    "        }\n",
    "\n",
    "        if (saveDebugVisualizations) {\n",
    "            vector<Scalar> colors = {CV_RGB(240, 248, 255),\n",
    "                                     CV_RGB(250, 235, 215),\n",
    "                                     CV_RGB(0, 255, 255),\n",
    "                                     CV_RGB(127, 255, 212),\n",
    "                                     CV_RGB(240, 255, 255),\n",
    "                                     CV_RGB(245, 245, 220),\n",
    "                                     CV_RGB(255, 228, 196),\n",
    "                                     CV_RGB(255, 235, 205),\n",
    "                                     CV_RGB(0, 0, 255),\n",
    "                                     CV_RGB(138, 43, 226),\n",
    "                                     CV_RGB(165, 42, 42),\n",
    "                                     CV_RGB(222, 184, 135)};\n",
    "\n",
    "            vector<Mat> imagesM;\n",
    "            for (const auto m : images) imagesM.push_back(m.second);\n",
    "            Mat out;\n",
    "            hconcat(vector<Mat>(imagesM.begin(), imagesM.begin() + 4), out);\n",
    "            RNG& rng = cv::theRNG();\n",
    "            const Size imgS = imagesM[0].size();\n",
    "            for (int tId = 0; tId < 20; tId++) {\n",
    "                const int trackId = rng(tracks[0].cols); // Randomize a track ID\n",
    "\n",
    "                // Show track over images\n",
    "                for (int i = 0; i < 3; i++) {\n",
    "                    Point2f a = Point2f(tracks[i].col(trackId));\n",
    "                    Point2f b = Point2f(tracks[i + 1].col(trackId));\n",
    "\n",
    "                    if (a.x < 0 or a.y < 0 or b.x < 0 or b.y < 0) {\n",
    "                        continue;\n",
    "                    }\n",
    "\n",
    "                    const Scalar c = colors[tId % colors.size()];\n",
    "                    a.x += imgS.width * i;\n",
    "                    b.x += imgS.width * (i + 1);\n",
    "                    circle(out, a, 7, c, FILLED);\n",
    "                    circle(out, b, 7, c, FILLED);\n",
    "                    line(out, a, b, c, 3);\n",
    "                }\n",
    "                imwrite(\"tracks.jpg\", out);\n",
    "\n",
    "                // Show track patches\n",
    "                const int patchSize = 20;\n",
    "                const Point2f patch(patchSize, patchSize);\n",
    "                for (int i = 0; i < tracks.size(); i++) {\n",
    "                    Point2f a = Point2f(tracks[i].col(trackId));\n",
    "                    if (a.x < patchSize or a.y < patchSize or\n",
    "                        a.x > imgS.width-patchSize or a.y > imgS.height-patchSize) {\n",
    "                        continue;\n",
    "                    }\n",
    "\n",
    "                    imwrite(\"track_\" + to_string(trackId) + \"_\" + to_string(i) + \".png\",\n",
    "                            imagesM[i](Rect(a - patch, a + patch)));\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    bool reconstructFromTracks() {\n",
    "        CV_LOG_INFO(TAG, \"Reconstruct from \" + to_string(tracks[0].cols) + \" tracks\");\n",
    "        const Size imgS = images.begin()->second.size();\n",
    "        const float f = std::max(imgS.width,imgS.height);\n",
    "        Mat K = Mat(Matx33f{f, 0.0, imgS.width/2.0f,\n",
    "                            0.0, f, imgS.height/2.0f,\n",
    "                            0.0, 0.0, 1.0});\n",
    "        cv::sfm::reconstruct(tracks, Rs, Ts, K, points3d, true);\n",
    "\n",
    "        K.copyTo(K_);\n",
    "\n",
    "        CV_LOG_INFO(TAG, \"Reconstruction: \");\n",
    "        CV_LOG_INFO(TAG, \"Estimated 3D points: \" + to_string(points3d.size()));\n",
    "        CV_LOG_INFO(TAG, \"Estimated cameras: \" + to_string(Rs.size()));\n",
    "        CV_LOG_INFO(TAG, \"Refined intrinsics: \");\n",
    "        CV_LOG_INFO(TAG, K_);\n",
    "\n",
    "        if (Rs.size() != imagesFilenames.size()) {\n",
    "            CV_LOG_ERROR(TAG, \"Unable to reconstruct all camera views (\" + to_string(imagesFilenames.size()) + \")\");\n",
    "            return false;\n",
    "        }\n",
    "\n",
    "        if (tracks[0].cols != points3d.size()) {\n",
    "            CV_LOG_WARNING(TAG, \"Unable to reconstruct all tracks (\" + to_string(tracks[0].cols) + \")\");\n",
    "        }\n",
    "\n",
    "        // Create the point cloud\n",
    "        pointCloud.clear();\n",
    "        for (const auto &p : points3d) pointCloud.emplace_back(Vec3f(p));\n",
    "\n",
    "        // Get the point colors\n",
    "        pointCloudColor.resize(pointCloud.size(), Vec3b(0,255,0));\n",
    "        vector<Point2f> point2d(1);\n",
    "        for (int i = 0; i < (int)pointCloud.size(); i++) {\n",
    "            for (int j = 0; j < imagesFilenames.size(); ++j) {\n",
    "                Mat point3d = Mat(pointCloud[i]).reshape(1, 1);\n",
    "                cv::projectPoints(point3d, Rs[j], Ts[j], K_, Mat(), point2d);\n",
    "                if (point2d[0].x < 0 or point2d[0].x >= imgS.width or point2d[0].y < 0 or\n",
    "                    point2d[0].y >= imgS.height) {\n",
    "                    continue;\n",
    "                }\n",
    "                pointCloudColor[i] = images[imagesFilenames[j]].at<Vec3b>(point2d[0]);\n",
    "                break;\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return true;\n",
    "    }\n",
    "\n",
    "    void visualize3D() {\n",
    "        CV_LOG_INFO(TAG, \"Visualize reconstruction\");\n",
    "\n",
    "        if (saveDebugVisualizations) {\n",
    "            // 3d point reprojections\n",
    "            Mat points2d;\n",
    "            Mat points3dM(points3d.size(), 1, CV_32FC3);\n",
    "            for (int i = 0 ; i < points3d.size(); i++) {\n",
    "                points3dM.at<Vec3f>(i) = Vec3f(points3d[i]);\n",
    "            }\n",
    "            for (int j = 0; j < imagesFilenames.size(); j++) {\n",
    "                cv::projectPoints(points3dM, Rs[j], Ts[j], K_, noArray(), points2d);\n",
    "\n",
    "                Mat out;\n",
    "                images[imagesFilenames[j]].copyTo(out);\n",
    "                for (int i = 0; i < points2d.rows; i++) {\n",
    "                    circle(out, points2d.at<Point2f>(i), 3, CV_RGB(255, 0, 0), FILLED);\n",
    "                }\n",
    "                imwrite(\"reprojection_\" + to_string(j) + \".jpg\", out);\n",
    "            }\n",
    "        }\n",
    "\n",
    "        // Create 3D windows\n",
    "        viz::Viz3d window(\"Coordinate Frame\");\n",
    "        window.setWindowSize(Size(500, 500));\n",
    "        window.setWindowPosition(Point(150, 150));\n",
    "        window.setBackgroundColor(viz::Color::white());\n",
    "\n",
    "        // Recovering cameras\n",
    "        vector<Affine3d> path;\n",
    "        for (size_t i = 0; i < Rs.size(); ++i)\n",
    "            path.push_back(Affine3d(Rs[i],Ts[i]));\n",
    "\n",
    "        // Add the pointcloud\n",
    "        viz::WCloud cloud_widget(pointCloud, pointCloudColor);\n",
    "        window.showWidget(\"point_cloud\", cloud_widget);\n",
    "        // Add cameras\n",
    "        window.showWidget(\"cameras_frames_and_lines\", viz::WTrajectory(path, viz::WTrajectory::BOTH, 0.1, viz::Color::black()));\n",
    "        window.showWidget(\"cameras_frustums\", viz::WTrajectoryFrustums(path, K_, 0.1, viz::Color::navy()));\n",
    "        window.setViewerPose(path[0]);\n",
    "\n",
    "        /// Wait for key 'q' to close the window\n",
    "        CV_LOG_INFO(TAG, \"Press 'q' to close ... \")\n",
    "\n",
    "        window.spin();\n",
    "    }\n",
    "\n",
    "    void saveToMVSFile() {\n",
    "        CV_LOG_INFO(TAG, \"Save reconstruction to MVS file: \" + saveMVS)\n",
    "\n",
    "        MVS::Interface interface;\n",
    "        MVS::Interface::Platform p;\n",
    "\n",
    "        // Add camera\n",
    "        MVS::Interface::Platform::Camera c;\n",
    "        const Size imgS = images[imagesFilenames[0]].size();\n",
    "        c.K = Matx33d(K_);\n",
    "        c.R = Matx33d::eye();\n",
    "        c.C = Point3d(0,0,0);\n",
    "        c.name = \"Camera1\";\n",
    "        c.width = imgS.width;\n",
    "        c.height = imgS.height;\n",
    "        p.cameras.push_back(c);\n",
    "\n",
    "        // Add views\n",
    "        p.poses.resize(Rs.size());\n",
    "        for (size_t i = 0; i < Rs.size(); ++i) {\n",
    "            Mat t = -Rs[i].t() * Ts[i];\n",
    "            p.poses[i].C.x = t.at<double>(0);\n",
    "            p.poses[i].C.y = t.at<double>(1);\n",
    "            p.poses[i].C.z = t.at<double>(2);\n",
    "            Mat r; Rs[i].copyTo(r);\n",
    "            Mat(r).convertTo(p.poses[i].R, CV_64FC1);\n",
    "\n",
    "            // Add corresponding image\n",
    "            MVS::Interface::Image image;\n",
    "            image.cameraID = 0;\n",
    "            image.poseID = i;\n",
    "            image.name = imagesFilenames[i];\n",
    "            image.platformID = 0;\n",
    "            interface.images.push_back(image);\n",
    "        }\n",
    "        p.name = \"Platform1\";\n",
    "        interface.platforms.push_back(p);\n",
    "\n",
    "        // Add point cloud\n",
    "        for (size_t k = 0; k < points3d.size(); ++k) {\n",
    "            MVS::Interface::Color c;\n",
    "            MVS::Interface::Vertex v;\n",
    "            v.X = Vec3f(points3d[k]);\n",
    "\n",
    "            // Reproject to see if in image bounds and get the RGB color\n",
    "            Mat point3d;\n",
    "            Mat(points3d[k].t()).convertTo(point3d, CV_32FC1);\n",
    "            for (uint32_t j = 0; j < tracks.size(); ++j) {\n",
    "                vector<Point2f> points2d(1);\n",
    "                cv::projectPoints(point3d, Rs[j], Ts[j], K_, Mat(), points2d);\n",
    "                if (points2d[0].x < 0 or points2d[0].x > imgS.width or\n",
    "                    points2d[0].y < 0 or points2d[0].y > imgS.height) {\n",
    "                    continue;\n",
    "                } else {\n",
    "                    c.c = images[imagesFilenames[j]].at<Vec3b>(points2d[0]);\n",
    "                    v.views.push_back({j, 1.0});\n",
    "                }\n",
    "            }\n",
    "\n",
    "            interface.verticesColor.push_back(c);\n",
    "            interface.vertices.push_back(v);\n",
    "        }\n",
    "\n",
    "        MVS::ARCHIVE::SerializeSave(interface, saveMVS);\n",
    "    }\n",
    "\n",
    "    vector<String> imagesFilenames;\n",
    "    map<string, int> imageIDs;\n",
    "    map<string, Mat> images;\n",
    "    map<string, vector<KeyPoint> > keypoints;\n",
    "    map<string, Mat> descriptors;\n",
    "    map<pair<string, string>, vector<DMatch> > matches;\n",
    "    vector<Mat> Rs, Ts;\n",
    "    vector<Mat> points3d;\n",
    "    vector<Mat> tracks;\n",
    "    vector<Vec3f> pointCloud;\n",
    "    vector<Vec3b> pointCloudColor;\n",
    "    Matx33f K_;\n",
    "\n",
    "    const float MATCH_RATIO_THRESHOLD = 0.8f; // Nearest neighbor matching ratio\n",
    "    const float PAIR_MATCH_SURVIVAL_RATE;     // Ratio of surviving matches for a successful stereo match\n",
    "    const bool visualize;                     // Show 3D visualization of the sprase cloud?\n",
    "    const string saveMVS;                     // Save the reconstruction in MVS format for OpenMVS?\n",
    "    const string saveCloud;                   // Save the reconstruction to a point cloud file?\n",
    "    const bool saveDebugVisualizations;       // Save debug visualizations from the reconstruction process\n",
    "\n",
    "    const string TAG = \"StructureFromMotion\";\n",
    "};\n",
    "\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "    utils::logging::setLogLevel(utils::logging::LOG_LEVEL_DEBUG);\n",
    "\n",
    "    cv::CommandLineParser parser(argc, argv,\n",
    "                                 \"{help h ? |           | help message}\"\n",
    "                                 \"{@dir     | ./images  | directory with image files for reconstruction }\"\n",
    "                                 \"{mrate    | 0.5       | Survival rate of matches to consider image pair success }\"\n",
    "                                 \"{viz      | true      | Visualize the sparse point cloud reconstruction? }\"\n",
    "                                 \"{debug    | false     | Save debug visualizations to files? }\"\n",
    "                                 \"{mvs      | recon.mvs | Save reconstruction to an .mvs file. Provide filename }\"\n",
    "                                 \"{cloud    | cloud.ply | Save reconstruction to a point cloud file (PLY, XYZ and OBJ). Provide filename}\"\n",
    "    );\n",
    "\n",
    "    if (parser.has(\"help\"))\n",
    "    {\n",
    "        parser.printMessage();\n",
    "        return 0;\n",
    "    }\n",
    "\n",
    "    StructureFromMotion sfm(parser.get<string>(\"@dir\"),\n",
    "                            parser.get<float>(\"mrate\"),\n",
    "                            parser.get<bool>(\"viz\"),\n",
    "                            parser.get<string>(\"mvs\"),\n",
    "                            parser.get<string>(\"cloud\"),\n",
    "                            parser.get<bool>(\"debug\")\n",
    "                            );\n",
    "    sfm.runSfM();\n",
    "\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
