{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision, Lab 08: ROS and ORB-SLAM2\n",
    "\n",
    "Materials for this lab were sourced from these references:\n",
    "\n",
    "- https://github.com/appliedAI-Initiative/orb_slam_2_ros\n",
    "- https://github.com/raulmur/ORB_SLAM2\n",
    "- http://wiki.ros.org/\n",
    "- https://arxiv.org/abs/1610.06475\n",
    "- https://roboticsknowledgebase.com/wiki/state-estimation/orb-slam2-setup/\n",
    "- https://medium.com/@mhamdaan/implementing-orb-slam-on-ubuntu-18-04-ros-melodic-606e668deffa\n",
    "- https://medium.com/@j.zijlmans/lsd-slam-vs-orb-slam2-a-literature-based-comparison-20732df431d\n",
    "- https://medium.com/@j.zijlmans/orb-slam-2052515bd84c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is ROS?\n",
    "\n",
    "ROS (Robot Operating System) is an open-source framework for robotics. It provides services need such as hardware abstraction, low-level device control, implementation of commonly-used functionality, message-passing between processes, and package management. It also provides development tools and libraries for building and running your own application code that executes your algorithms and interacts with the robot hardware.\n",
    "\n",
    "## Install ROS\n",
    "\n",
    "ROS is so big it really is almost like an operating system, and the development community follows a regular major release pattern similar to that of an OS.\n",
    "You can try to build ROS from source on just about any system, but the binary releases for Ubuntu are tied to specific versions. If you're running Ubuntu 20.04,\n",
    "there is only one choice for a ROS binary installation, ROS Noetic. Follow the instructions in the [ROS Noetic installation manual for Ubuntu](http://wiki.ros.org/noetic/Installation/Ubuntu). If you want to give it a shot on Windows10 (not recommended), use the <link>[ROS installation manual for Windows](http://wiki.ros.org/Installation/Windows). Here are the Ubuntu steps in summary:\n",
    "\n",
    "1. Set up your computer to accept software from packages.ros.org:\n",
    "       $ sudo sh -c 'echo \"deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main\" > /etc/apt/sources.list.d/ros-latest.list'\n",
    "2. Install the ROS repository keys:\n",
    "       $ sudo apt install curl # if you haven't already installed curl\n",
    "       $ curl -s https://raw.githubusercontent.com/ros/rosdistro/master/ros.asc | sudo apt-key add -\n",
    "3. Update the local Ubuntu package list including the new ROS package repository\n",
    "       $ sudo apt update\n",
    "4. The \"Desktop Full Install\" (Recommended) contains everything in the Desktop version plus 2D/3D simulators and 2D/3D perception packages. \n",
    "       $ sudo apt install ros-noetic-desktop-full\n",
    "   If you want to install a specific package directly:\n",
    "       $ sudo apt install ros-noetic-PACKAGE\n",
    "   For example:\n",
    "       $ sudo apt install ros-noetic-slam-gmapping\n",
    "4. Environment setup: you must source the ROS setup script in every bash terminal you use ROS in:\n",
    "       $ source /opt/ros/noetic/setup.bash\n",
    "   Alternatively, it can be convenient to automatically source this script every time a new shell is launched. If you're using bash:\n",
    "       $ echo \"source /opt/ros/noetic/setup.bash\" >> ~/.bashrc\n",
    "       $ source ~/.bashrc\n",
    "5. Finally, install the OS dependencies for building your own packages:\n",
    "       $ sudo apt install python3-rosdep python3-rosinstall python3-rosinstall-generator python3-wstool build-essential\n",
    "       $ sudo apt install python3-rosdep\n",
    "       $ sudo rosdep init\n",
    "       $ rosdep update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the catkin build system\n",
    "\n",
    "It seems catkin (needed for development) is not installed with the desktop distribution. Install it now:\n",
    "\n",
    "    $ sudo apt-get install ros-noetic-catkin\n",
    "    $ sudo apt-get install python3-catkin-tools\n",
    "    $ sudo apt-get install python3-osrf-pycommon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORB-SLAM2\n",
    "\n",
    "ORB-SLAM2 is an implementation of the algorithm described in [the original ORB-SLAM paper](https://arxiv.org/pdf/1610.06475.pdf), written by the authors themselves.\n",
    "\n",
    "### What is ORB-SLAM\n",
    "\n",
    "ORB-SLAM is a monocular SLAM algorithm. SLAM means Simultaneous Localization And Mapping, and monocular means that we can perform SLAM using a single RGB video or image sequence from a single camera moving through a (assumed static) scene.\n",
    "\n",
    "SLAM algorithms are algorithms that simultaneously track the movement of the camera (hand-held or mounted onto a robot, car, or UAV) and create a point cloud map of the surroundings that the camera moves through. They create a map of the surroundings while simultaneously localizing the camera within the map.\n",
    "\n",
    "Monocular SLAM is really just online incremental SfM. It therefore has the same limitation of SfM, namely scale ambiguity. It cannot tell the difference between small motions within a small world or big motions within a big world. This means that any online monocular SLAM method is going to have the problem that the estimated scale of the scene and thus the percieved scale of the scenery will drift over time. ORB-SLAM, like other mono SLAM methods, attempts to be fix the scale drift problem by trying to detect scenery that it has already passed through (a so-called \"loop\"). Loop detection and correction makes the algorithm work for large-scale outdoor sceneries, smaller-scale indoor scenes, and transitions between these two.\n",
    "\n",
    "### Feature-based vs. Direct SLAM\n",
    "\n",
    "Monocular slam algorithms can be divided into two groups:\n",
    " - feature-based methods\n",
    " - those that use \"direct\" methods\n",
    "\n",
    "*Feature-based* SLAM algorithms like ORB-SLAM take the image sequence, search for certain features (key-points, for instance, corners) and only use these features to estimate the camera location and point cloud. This means that they throw away a lot of possibly valuable information from the image, but this they simplify the whole process, because feature points can generally be detected quickly in a rotation invariant manner.\n",
    "\n",
    "*Direct* SLAM algorithms like LSD-SLAM do not search images for key-points but instead use the image intensities across the entire image to estimate camera motion and dense reconstruction. This means they use more information from the images and thus tend to be create more detailed maps of the surrounding, but on the other hand, they require more computational resources.\n",
    "\n",
    "<img src=\"img/lab08-4.png\" width=\"800\"/>\n",
    "\n",
    "The ORB-SLAM2 code repository contains implementations of additional methods. Besides monocular RGB sequences, it also supports stereo images (two images taken with two cameras at the same time instance) and RGBD images (single RGB images augmented with depth information, usually based on IR-range structured light or time of flight).\n",
    "\n",
    "The ORB-SLAM algorithm utilizes three threads, a tracking thread, a local mapping thread, and a loop closing thread. Here's an overview:\n",
    "\n",
    "<img src=\"img/lab08-1.png\" width=\"800\"/>\n",
    "\n",
    "### Initializing the map\n",
    "\n",
    "To initialize the map, ORB-SLAM uses either two-frame calibrated SfM using two keyframes with sufficient parallax, or a homography between two keyframes imaging a planar scene. To do this, for every candidate keyframe pair, they compute two geometrical models in parallel, one for $\\texttt{H}$ and one for $\\texttt{F}$. When the feature point motion criteria are achieved, they choose one of the two models based on a relative score. If the motion criteria are not achieved, they restart initialization using the next frame as a candidate keyframe. If the set of tracked feature points becomes too sparse before a model is selected, the entire process is restarted. Finally, once an initial keyframe pair is selected and motion is estimated, the system performs an initial bundle adjustment on the camera positions and 3D initial triangulated 3D points.\n",
    "\n",
    "### Tracking\n",
    "\n",
    "Once initialization is achieved, the tracking thread localizes the camera and decides when to insert a new keyframe. Features are matched with the previous frame, and the pose is optimized using motion-only bundle adjustment. The features extracted are FAST corners. (for resolutions up to 752x480, 1000 corners should be good, while for higher resolutions such as KITTI, which is 1241x376, 2000 corners works better). The feature point detection is done at multiple scale levels (related to each other by a factor of 1.2), and each level is divided into a grid in which up to 5 corners per cell are extracted. These FAST corners are then described using ORB. The initial pose is estimated using a constant velocity motion model. If tracking is lost, the place recognition module kicks in and tries to re-localize the camera. When the feature matching is successful, a co-visibility graph of keyframes is used to get a local map consisting of the previous keyframes that share map points with the current frame, the neighbors of these keyframes, and a reference keyframe sharing the most map points with the current frame. Through re-projection, matches of the local map are searched on the frame, then the camera pose is optimized using these matches. Finally, the thread decides whether a new keyframe needs to be created. New keyframes are inserted very frequently at first to make tracking more robust. A new keyframe is created every time at least 20 frames have passed from the last keyframe or the last global re-localization, and optionally when the frame tracks at least 50 points of which less then 90% are successfully tracked from the reference keyframe.\n",
    "\n",
    "### Local mapping\n",
    "\n",
    "The local mapping thread is responsible for integrating the information coming from the tracking thread when a new keyframe is added into the map.\n",
    "First, the new keyframe is inserted into the covisibility graph, which is a spanning tree linking each keyframe to the other keyframes with the most points in common. The local mapper also creates a bag of visual words representation of the keyframe that can be used for loop closing and relocalization later.\n",
    "\n",
    "New map points are created by triangulating features ORB from connected keyframes in the covisibility graph. The unmachted ORB features in a keyframe are compared with other unmatched ORB features in other keyframes. A match must fulfill the epipolar constraint to be valid. Candidate matches are triangulated and checked for chirality (positive depth in both frames), and the parallax, reprojection error, and scale consistency are also checked. If these checks survive, the match is projected to other connected keyframes to check if it is also in these.\n",
    "\n",
    "A new map point first needs to go through a test to increase the likelihood of these map points being valid. The point needs to be found in more than 25% of the frames it is predicted to be visible in, and it must be observed by at least three keyframes.\n",
    "\n",
    "Next, through local bundle adjustment, the current keyframe, all keyframes connected to it through the co-visibility graph, and all the map points seen by these keyframes are optimized using the keyframes that do see the map points but are not connected to the current keyframe.\n",
    "Finally keyframes that are redundant are discarded to maintain a minimal co-visibility graph. Keyframes for which 90% of the map points can be seen by three other keyframes at the same scale level are discarded.\n",
    "\n",
    "### Loop closing\n",
    "\n",
    "To detect possible loops, the loop closing thread checks the bag of words vectors for the current keyframe and its neighbors in the covisibitlity graph. The minimum similarity among the bag of words vectors is taken as a threshold, and all the keyframes with a bag of words similatrity with the current key frame greater than the threshold and all the keyframes already connected to the current keyframe are removed. Among the remaining candidates, if three consistent consecutive keyframes are found, this serious candidate for a possible loop is considered further.\n",
    "\n",
    "For such candidate loops, a 7 DOF similarity (metric) transformation is calculated using RANSAC followed by BA followed by a second correspondence search and then another BA. If the similarity between the current keyframe and the possible loop canidate is supported by having enough inliers, the loop is accepted.\n",
    "The current keyframe pose is adjusted and propagated to its neighbors, and corresponding map points are fused. Finally, a pose graph optimization is performed over the essential graph to take out loop closure errors throughout the graph. This process corrects for scale drift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install ORB-SLAM2\n",
    "\n",
    "The instructions below follow from [the Applied AI Initiative's tutorial](https://github.com/appliedAI-Initiative/orb_slam_2_ros) or the [ORB-SLAM2 GitHub Repository](https://github.com/raulmur/ORB_SLAM2).\n",
    "\n",
    "1. Create a ROS workspace:\n",
    "       $ mkdir -p ws/src\n",
    "       $ cd ws/src\n",
    "       $ source /opt/ros/noetic/setup.bash\n",
    "2. Download ORB-SLAM2:\n",
    "       $ git clone https://github.com/appliedAI-Initiative/orb_slam_2_ros.git\n",
    "3. Install ORB-SLAM2:\n",
    "       $ cd ..\n",
    "       $ rosdep install --from-paths src --ignore-src -r -y\n",
    "4. Build using catkin:\n",
    "       $ catkin build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run it on benchmark data\n",
    "\n",
    "1. Open another terminal to run roscore:\n",
    "       $ cd ws\n",
    "       $ source /opt/ros/noetic/setup.bash\n",
    "       $ roscore\n",
    "2. In the old terminal, launch ORB-SLAM. We'll run the demo of monocular SLAM with the Intel RealSense R200:\n",
    "       $ roslaunch orb_slam2_ros orb_slam2_r200_mono.launch\n",
    "\n",
    "| camera | Mono | Stereo | RGBD |\n",
    "|:----|:----|:----|:----|\n",
    "|Intel RealSense r200 | roslaunch orb_slam2_ros orb_slam2_r200_mono.launch | roslaunch orb_slam2_ros orb_slam2_r200_stereo.launch | roslaunch orb_slam2_ros orb_slam2_r200_rgbd.launch |\n",
    "|Intel RealSense d435 | roslaunch orb_slam2_ros orb_slam2_d435_mono.launch | - | roslaunch orb_slam2_ros orb_slam2_d435_rgbd.launch |\n",
    "Mynteye S | roslaunch orb_slam2_ros orb_slam2_mynteye_s_mono.launch | roslaunch orb_slam2_ros orb_slam2_mynteye_s_stereo.launch | - |\n",
    "Stereolabs ZED 2 | - | roslaunch orb_slam2_ros orb_slam2_zed2_stereo.launch | - |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run ORB-SLAM2 with your Web camera\n",
    "\n",
    "This process follows [mhamdaan's Medium.com tutorial on ORB-SLAM2](https://medium.com/@mhamdaan/implementing-orb-slam-on-ubuntu-18-04-ros-melodic-606e668deffa).\n",
    "The following should work with Noetic:\n",
    "\n",
    "1. Install the ROS USB camera driver and test run:\n",
    "       $ sudo apt install ros-melodic-usb-cam\n",
    "       $ roslaunch usb_cam usb_cam-test.launch\n",
    "2. Install the camera calibration module:\n",
    "       $ rosdep install camera_calibration\n",
    "   Once all the dependencies have been installed, you run can the camera_calibration node by giving it the required parameters. To know about each parameter and how to    use camera calibration, see [the ROS camera calibration tutorial page](http://wiki.ros.org/camera_calibration/Tutorials/MonocularCalibration). (You’ll also need a\n",
    "   checkerboard to perform the calibration.)\n",
    "   - size: checkerboard size (number of corners)\n",
    "   - square: rectangle size, in meters\n",
    "       $ rosrun camera_calibration cameracalibrator.py --size 8x6 --square 0.108 image:=/camera/image_raw camera:=/camera\n",
    "3. Convert the .ost file to a .yaml file. To do that, enter the following in the terminal:\n",
    "       $ rosrun  camera_calibration_parsers convert  <filename>.ost <filename>.yaml \n",
    "   Once that is over, a new .yaml file with the specified filename will be created.\n",
    "4. When you run the usb_cam node, it publishes two important topics that will be subscribed by your orb_slam2_ros node. One is the /camera/image_raw and /camera/camera_info. The latter is the topic that sends your camera parameters to the orb_slam2_ros node. Therefore, you need to make your usb_cam node to publish your .yaml file parameters to that topic. To do so, enter the following commands in your terminal:\n",
    "       $ roscd usb_cam\n",
    "       $ cd launch\n",
    "       $ sudo nano usb_cam-test.launch\n",
    "\n",
    "   The nano text editor will open up your launch file. Enter the highlighted line of code as shown in the image below:\n",
    "   <text><param name=\"camera_info_url\" value=\"file:///home/<your_username>/<your_file_address>/<your_filename>.yaml/></text>\n",
    "\n",
    "   <img src=\"img/lab08-2.png\" width=\"800\"/>\n",
    "     \n",
    "   Save the file and close it. Your camera and its parameters are successfully setup\n",
    "    \n",
    "5. Setting orb_slam2_ros node (same as above)\n",
    "       $ cd ~\n",
    "       $ mkdir -p orbslam2_ws/src\n",
    "       $ cd orbslam2_ws/src\n",
    "       $ git clone https://github.com/appliedAI-Initiative/orb_slam_2_ros.git\n",
    "       $ cd ..\n",
    "       $ catkin_make\n",
    "       $ source devel/setup.bash\n",
    "6. Create a new .launch file in your orb_slam_2_ros/ros/launch directory and paste the following code in it and save it. After that run <code>catkin_make</code> again in your catkin workspace directory:\n",
    "    <launch>\n",
    "      <node name=\"orb_slam2_mono\" pkg=\"orb_slam2_ros\"\n",
    "          type=\"orb_slam2_ros_mono\" output=\"screen\">\n",
    "        <param name=\"publish_pointcloud\" type=\"bool\" value=\"true\" />\n",
    "               <param name=\"publish_pose\" type=\"bool\" value=\"true\" />\n",
    "               <param name=\"localize_only\" type=\"bool\" value=\"false\" />\n",
    "               <param name=\"reset_map\" type=\"bool\" value=\"true\" />\n",
    "        <!-- static parameters -->\n",
    "               <param name=\"load_map\" type=\"bool\" value=\"false\" />\n",
    "               <param name=\"map_file\" type=\"string\" value=\"map.bin\" />\n",
    "               <param name=\"voc_file\" type=\"string\" value=\"$(find orb_slam2_ros)/orb_slam2/Vocabulary/ORBvoc.txt\" />\n",
    "        <param name=\"pointcloud_frame_id\" type=\"string\" value=\"map\" />\n",
    "               <param name=\"camera_frame_id\" type=\"string\" value=\"camera_link\" />\n",
    "               <param name=\"min_num_kf_in_map\" type=\"int\" value=\"5\" />\n",
    "        <!-- ORB parameters -->\n",
    "               <param name=\"/ORBextractor/nFeatures\" type=\"int\" value=\"2000\" />\n",
    "               <param name=\"/ORBextractor/scaleFactor\" type=\"double\" value=\"1.2\" />\n",
    "               <param name=\"/ORBextractor/nLevels\" type=\"int\" value=\"8\" />\n",
    "               <param name=\"/ORBextractor/iniThFAST\" type=\"int\" value=\"20\" />\n",
    "               <param name=\"/ORBextractor/minThFAST\" type=\"int\" value=\"7\" />\n",
    "        <!-- Camera parameters -->\n",
    "               <!-- Camera frames per second -->\n",
    "               <param name=\"camera_fps\" type=\"int\" value=\"30\" />\n",
    "               <!-- Color order of the images (0: BGR, 1: RGB. It is ignored if images are grayscale) -->\n",
    "               <param name=\"camera_rgb_encoding\" type=\"bool\" value=\"true\" />\n",
    "                <!--If the node should wait for a camera_info topic to take the camera calibration data-->\n",
    "               <param name=\"load_calibration_from_cam\" type=\"bool\" value=\"true\" />\n",
    "      </node>\n",
    "    </launch>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Creating a node to display images\n",
    "\n",
    "This is an important step because, when your orb_slam2_ros node runs, it will publish the live image from the camera that contains the currently found key points along with a status text. This will be published on the /debug_image topic. We cannot directly view those images, therefore we will have to create a ros node that can display those images to us.\n",
    "\n",
    "Create a new python file inside a package and paste the following code into it. Make sure that the package has the cv_bridge dependency. If not then, you must add that dependency to your ROS package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import rospy\n",
    "import cv2\n",
    "from cv_bridge import CvBridge\n",
    "from sensor_msgs.msg import Image\n",
    "bridge = CvBridge()\n",
    "def callback(data):\n",
    "    frame = bridge.imgmsg_to_cv2(data, \"bgr8\")\n",
    "    cv2.imshow('Video Feed', frame)\n",
    "    cv2.waitKey(1)\n",
    "    rospy.loginfo('Image feed received!')\n",
    "def listener():\n",
    "    rospy.init_node('vid_rec')\n",
    "    #first parameter is the topic you want to subcribe sensor_msgs/Image from\n",
    "    rospy.Subscriber('/orb_slam2_mono/debug_image', Image, callback)\n",
    "    rospy.spin()\n",
    "if __name__ == '__main__':\n",
    "    listener()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the file and close it. Don’t forget to make your python script executable by the running the following command:\n",
    "<code>\n",
    "    $ chmod +x <your_python_script_name>.py\n",
    "</code>\n",
    "\n",
    "Now run catkin_make in your catkin workspace directory and then source it.\n",
    "\n",
    "8. Running it all together. Make sure your camera is connected to your PC. Run each of the following commands in new terminals.\n",
    "\n",
    "<code>\n",
    "    $ roscore\n",
    "    $ roslaunch usb_cam usb_cam-test.launch\n",
    "    $ roslaunch orb_slam2_ros <your_launch_filename_you_just_created>.launch\n",
    "    $ rosrun <your_package_name> <your_python_script_name>.py\n",
    "    $ rviz\n",
    "</code>\n",
    "    \n",
    "All of these must run without any errors. If there are any errors, then run rqt_graph to see if the nodes are communicating with each other. When the RVIZ window opens up, you can add tf and PointCloud2. You must also specify the correct topic name (/map_points) for RVIZ to subscribe to the PointCloud2 message from.\n",
    "    \n",
    "<img src=\"img/lab08-3.png\" width=\"800\"/>\n",
    "    \n",
    "Once all of that has been set up, you can now see the point clouds appearing and the TF frame of the camera link move in your RVIZ window. Feel free to explore through the GitHub page which contains the source code to use all other functionality of this amazing package. Thanks to the authors (Raul Mur-Artal, Juan D. Tardos, J. M. M. Montiel and Dorian Galvez-Lopez) who developed the orb_slam2_ros package. Now, it seems like you are all so ready to use this package for your next ROS package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on a video file from our robot\n",
    "\n",
    "Try to run on a video stream from (one of the <tt>robot.mp4</tt> video files from previous labs)\n",
    "using the process described in the [ROS OpenCV video stream tutorial](http://wiki.ros.org/video_stream_opencv).\n",
    "\n",
    "## The report\n",
    "\n",
    "As always, turn in a report describing your experiments and results by the following week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
