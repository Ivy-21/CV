{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision, Lab 7:  Structure from Motion (SfM)\n",
    "\n",
    "Today we'll look at a full structure from motion pipeline, which involves keypoint computation,\n",
    "keyframe selection, two-frame reconstruction, and resectioning. Some references for today's material:\n",
    "- https://www.opensfm.org/docs/building.html\n",
    "- https://github.com/mapillary/OpenSfM\n",
    "- Mastering OpenCV3 (Packtpub)\n",
    "- Mastering OpenCV4 (Packtpub)\n",
    "\n",
    "As it's a big task to put all these things together from scratch, we'll use the open source OpenSFM framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Flow\n",
    "\n",
    "The following flow diagram describes the processing in the SfM pipeline we will implement.\n",
    "\n",
    "<img src=\"img/lab07-11.png\" width=\"800\"/>\n",
    "\n",
    "### Calibrated rigs vs. SfM\n",
    "\n",
    "The first distinction we should make is the difference between stereo (or indeed any multiview) and 3D reconstruction using calibrated rigs and SfM. A rig of two or more cameras assumes that we already know the motion between the cameras, while in SfM, we do not know what the motion is, and we need to find it. Calibrated rigs allow more accurate reconstruction of 3D geometry because there is no error in estimating the distances and rotations between the cameras, which are already known, and there is no scale ambiguity, which we already know exists in\n",
    "any SfM. The first step in SfM, on the other hand, is finding the motion between the cameras.\n",
    "\n",
    "### Triangulation\n",
    "\n",
    "In most cases, we wish to obtain the geometry of the scene, for example, where objects are in relation to the cameras and what their form is. Having found the motion between the cameras picturing the same scene, we would then like to reconstruct the geometry, initially in the form of a point cloud by triangulation. Conceptualy, the rays through a camera's optical center and a point in the idealized image plane for different 2D projections of the same 3D point will intersect at the correct 3D point in the real world that is imaged in each camera. In practice, however, with noisy correspondences, the two rays will not strictly intersect (they will be \"skew\"),\n",
    "and we will have to minimize some cost function to find the 3D point minimizing that cost. The situation is shown in the following diagram.\n",
    "\n",
    "<img src=\"img/lab07-12.jfif\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-view epiolar geometry\n",
    "\n",
    "Assume we have two images of the same scene from different positions in space. Two useful mathematical objects are the fundamental matrix (denoted by $\\mathtt{F}$) and the essential matrix (denoted by $\\mathtt{E}$), which impose constraints on the positions of corresponding 2D points in two images of the scene. The essential matrix is actually a fundamental matrix for the special case of normalized points with calibrated cameras, and using the essential matrix rather than the fundamental matrix for unnomralized points will give us a metric rather than projective reconstruction of the scene. Assuming the same calibration matrix $\\mathtt{K}$ for our two cameras and two corresponding (unnormalized) points $\\mathbf{x}$ and $\\mathbf{x}'$ represented by homogeneous 3-vectors, we have\n",
    "$$\\mathbf{x}^\\top\\mathtt{K}^{-\\top} \\mathtt{E} \\mathtt{K}^{-1} \\mathbf{x}' = \n",
    "  \\hat{\\mathbf{x}}^\\top \\mathtt{E} \\hat{\\mathbf{x}}' = 0.$$\n",
    "As we've learned from exercises in class, OpenCV implments Nister's 5-point algorithm for $\\mathtt{E}$ as the inner loop for RANSAC. This is encapsulated in the <tt>findEssentialMat()</tt> function. Factoring $\\mathtt{E}$ and testing the four possible solutions for the rotation and translation of the two cameras will give us the rotation between the two cameras and the translation vector *scaled to unit length*. Remember that this scaling of the unknown translation to one unit represents the scale ambiguity inherent in structure from motion. To resolve it, we would have to find an additional real-world distance or other constraint on the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point matching using rich feature descriptors\n",
    "\n",
    "Feature extraction and descriptor matching is useful for many computer vision tasks, e.g., detecting the position and orientation of an object in an image or searching a big database of images for similar images given a query image. **Feature extraction** means selecting points in the image that can be uniquely identified in other views of the same scene and computing a descriptor for each of them. A descriptor is just a vector of numbers that describes the texture around a feature point in an image. Different methods give different length and data types for descriptor vectors. **Descriptor matching** is the process of finding a corresponding feature for one set in another using the descriptor. OpenCV provides powerful methods to support feature extraction and matching.\n",
    "\n",
    "If we're using mathematical feature point extractors,\n",
    "given an image sequence, we will obtain three elements: feature points for two images (ORB, AKAZE, SIFT, SURF), descriptors for them, and a matching between the two sets of features (bruteforce).\n",
    "\n",
    "In OpenCV, the brute-force matcher performs cross-check filtering. A match is considered true if a feature of the first image matches a feature of the second image, and the reverse check also matches the feature of the second image with the feature of the first image. Another common filtering mechanism, used in the provided code, is to filter based on the fact that the two images are of the same scene and have a certain stereo-view relationship between them. In practice, the filter tries to robustly calculate the fundamental or essential matrix which we will learn about in the \"Finding camera matrices\" section and retain those feature pairs that correspond with this calculation with small errors.\n",
    "\n",
    "An alternative to using rich features, such as ORB, is to use optical flow. It is possible to use optical flow instead of descriptor matching to find the required point matching between two images, while the rest of the SfM pipeline remains the same.\n",
    "\n",
    "Likewise, yet another alternative to the use of rich features such as ORB or optical flow is to use a deep learning\n",
    "model for correspondence estimation. Deep learning models trained on large correspondence datasets have the potential to outperform the algorithmic correspondence estimation methods but may need fine tuning to perform well\n",
    "in environments they were not trained upon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding camera matrices\n",
    "\n",
    "Once we have obtained matches between keypoints, we can calculate the essential matrix. However, we must first align our matching points into two arrays, where an index in one array corresponds to the same index in the other. This is required by the <tt>findEssentialMat()</tt> function mentioned in the \"Two-view epipolar geometry\" section above. Once we've used the essential matrix to rule out outlier correspondences, we are left with just the inlier correspondences. Here is an example:\n",
    "\n",
    "<img src=\"img/lab07-13.jfif\" width=\"800\"/>\n",
    "\n",
    "Next, given $\\mathtt{E}$, we can obtain the relative pose of the two cameras, giving us\n",
    "the camera matrices. We've already discussed the mathematics at length in class, but the OpenCV API makes things very easy for us\n",
    "with the <code>recoverPose()</code> function, which we've already seen how to use to construct the camera matrix $\\mathtt{P} = \\mathtt{K} [\\mathtt{R} \\mid \\mathbf{t}]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the image pair to use first\n",
    "\n",
    "Given we have more than two views of the scene, we must choose which of the two views we will start reconstruction from, picking two views that have many essential matrix inliers but relatively few *homography inliers*. If all of the matches between two views of a scene can be explained by a single homography, we essentially have a set of planar points and no 3D structure. For this reason, we'll compute a homography $\\mathtt{H}$ between each pair of views and select the one with the lowest ratio of homography inliers to essential matrix inliers.\n",
    "\n",
    "When we process a video sequentially, the process of finding a good pair of initial frames is part of the *keyframe selection* process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstructing the scene\n",
    "\n",
    "Next, we look at triangulation. In the class exercies, we used the DLT algorithm for $\\mathbf{X}$ given $\\mathtt{P}, \\mathtt{P}', \\mathbf{x},$ and $\\mathbf{x}'$.\n",
    "This was fine because we didn't have an noise in the correspondences. When we have noise, it is important to minimize reprojection error iteratively using DLT as a starting point followed by Levenburg-Marquardt nonlinear least squares.\n",
    "\n",
    "Another deviation from the class exercise is that today we'll use normalized points throughout, so we don't have to worry about the image size and field of view.\n",
    "For example, a 2D point $\\mathbf{x}_1 = (160, 120, 1)^\\top$ in a 320$\\times$240 image might transform to $\\hat{\\mathbf{x}}_1 = (0, 0, 1)^\\top$ depending on what $\\mathtt{K}$ is. The OpenCV function <code>undistortPoints()</code> will perform this translation for us.\n",
    "\n",
    "<img src=\"img/lab07-14.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction from many views\n",
    "\n",
    "Now that we know how to recover the motion and scene geometry from two cameras, it would seem simple to get the parameters of additional cameras and more scene points simply by applying the same process. This matter is in fact not so simple, as we can only get a reconstruction that is upto scale, and each pair of pictures has a different scale.\n",
    "\n",
    "There are a number of ways to correctly reconstruct the 3D scene data from multiple views. One way to achieve **camera pose estimation** or **camera resectioning**, is the **Perspective N-Point(PnP)** algorithm, where we try to solve for the position of a new camera using N 3D scene points, which we have already found and their respective 2D image points. Another way is to triangulate more points and see how they fit into our existing scene geometry; this will tell us the position of the new camera by means of **point cloud registration**. In this section, we will discuss using OpenCV's <code>solvePnP</code> functions that implements the first method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building OpenSFM\n",
    "\n",
    "Let's build the OpenSFM library. This will take a while.\n",
    "The following should work if you're on Ubuntu 20.04 with Python 3.8.\n",
    "\n",
    "    $ sudo apt-get install build-essential cmake git libatlas-base-dev \\\n",
    "           libeigen3-dev libgoogle-glog-dev libsuitesparse-dev python3-dev \\\n",
    "           curl vim python3-yaml libopencv-dev\n",
    "    $ mkdir -p source ; cd source ; curl -L http://ceres-solver.org/ceres-solver-1.14.0.tar.gz \\\n",
    "           | tar xz ; cd ceres-solver-1.14.0 ; mkdir -p build ; cd build ; cmake \\\n",
    "           -DCMAKE_C_FLAGS=-fPIC -DCMAKE_CXX_FLAGS=-fPIC -DBUILD_EXAMPLES=OFF \\\n",
    "           -DBUILD_TESTING=OFF .. ; make -j4 ; sudo make install ; cd ../..\n",
    "    $ git clone --recursive https://github.com/paulinus/opengv.git ; \\\n",
    "           cd opengv ; mkdir -p build ; cd build ; \\\n",
    "           cmake -DBUILD_TESTS=OFF -DBUILD_PYTHON=ON -DPYBIND11_PYTHON_VERSION=3.8 \\\n",
    "           -DPYTHON_INSTALL_DIR=/usr/local/lib/python3.8/dist-packages/ .. ; \\\n",
    "           make -j4 ; sudo make install ; cd ../..\n",
    "    $ pip3 install scipy sphinx fpdf opencv-python pillow\n",
    "    $ git clone --recursive https://github.com/mapillary/OpenSfM\n",
    "    $ cd OpenSfM ; python3 setup.py build\n",
    "\n",
    "You may be missing some Python modules that I already had installed, so let me know if I'm missing some requirements.\n",
    "\n",
    "Once that's done, if all is well, you should be able to view the three images of the Berlin Cathedral in data/berlin. Then you can execute the SfM pipeline:\n",
    "\n",
    "<code>OpenSfM$ ./bin/opensfm_run_all data/berlin</code>\n",
    "\n",
    "It will take a while to run. If successful, you'll get many messages, ending with something like\n",
    "\n",
    "<code>2021-07-09 06:25:43,545 INFO: Merging depthmaps</code>\n",
    "\n",
    "and the result will be a JSON file data/berlin/reconstruction.meshed.json. You can visualize the reconstructed point cloud with\n",
    "\n",
    "<code>OpenSfM$ python3 -m http.server</code>\n",
    "\n",
    "then you can visit http://localhost:8000/viewer/legacy/reconstruction.html#file=/data/berlin/reconstruction.meshed.json to see the reconstructed cameras and point cloud.\n",
    "\n",
    "If you see something like the image below, all is successful.\n",
    "\n",
    "<img src=\"img/lab07-1.jpg\" width=\"600\"/>\n",
    "\n",
    "A lot more detail is at the OpenSfM documentation page. We will use this information to see how to use OpenSfM properly in the rest of the session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get video\n",
    "\n",
    "Use one of the sample videos for the lab. You may prefer the daytime or nighttime video.\n",
    "Extract the individual frames as follows:\n",
    "<code>\n",
    "OpenSfM$ cd data\n",
    "OpenSfM/data$ mkdir lab7-frames\n",
    "OpenSfM/data$ cd lab7-frames\n",
    "OpenSfM/data/lab7-frames$ ffmpeg -i ~/Downloads/robot.mp4 frame%03d.png\n",
    "</code>\n",
    "You should get a bunch of frames from the original video with names such as <code>frame001.png</code>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a dataset for SfM\n",
    "\n",
    "Now you'll want to select a sequence of keyframes from the video. We'll think about how to do this automatically later, but here let's do it manually. Use two viewers to step through the sequence and get a subset of the frames that contain motion with perhaps 80% overlap between subsequent frames:\n",
    "<code>\n",
    "OpenSfM/data/lab7-frames$ eog frame001.png &\n",
    "OpenSfM/data/lab7-frames$ mkdir ../lab7-keyframes/\n",
    "OpenSfM/data/lab7-frames$ cp frame010.png ../lab7-keyframes/\n",
    "OpenSfM/data/lab7-frames$ eog ../lab7-keyframes/frame010.png &\n",
    "OpenSfM/data/lab7-frames$ cp frame120.png ../lab7-keyframes/\n",
    "OpenSfM/data/lab7-frames$ ...\n",
    "</code>\n",
    "I got 55 frames in this way. Take note of the motion blur, compression artifacts, reflections off the windows, and other issues. You may notice that image quality is much lower and that more frames per second are needed when the robot is turning.\n",
    "\n",
    "Next, copy the Berlin dataset configuration to your project directory:\n",
    "\n",
    "<code>\n",
    "OpenSfM/data/lab7-frames$ cd ../lab7-keyframes\n",
    "OpenSfM/data/lab7-keyframes$ cp ../berlin/config.yaml .\n",
    "OpenSfM/data/lab7-keyframes$ mkdir images\n",
    "OpenSfM/data/lab7-keyframes$ mv *.png images/\n",
    "OpenSfM/data/lab7-keyframes$ cd ../..\n",
    "OpenSfM$\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try a run:\n",
    "\n",
    "OpenSfM$ ./bin/opensfm_run_all data/lab7-keyframes\n",
    "It will take a few minutes to obtain the reconstruction. You'll see that the system does the following:\n",
    "\n",
    "1. Extracts keypoints from every keyframe\n",
    "2. Tries to match keypoints between every pair of frames. You will probably observe the best matching for adjacent frames in the sequence.\n",
    "3. Builds up a 3D reconstruction, beginning with two frames that have good matches then performing resectioning for the remaining frames.\n",
    "4. Computes depth maps for each frame based on the 3D points' projections into the images.\n",
    "\n",
    "The resulting 3D reconstruction can then be viewed.\n",
    "\n",
    "<code>\n",
    "OpenSfM$ python3 -m http.server\n",
    "</code>\n",
    "\n",
    "And then you can visit http://localhost:8000/viewer/legacy/reconstruction.html#file=/data/lab7-keyframes/reconstruction.json\n",
    "\n",
    "Do you get a reasonable result? If not, try subsequences of the total keyframe sequence until you get something reasonable.\n",
    "\n",
    "Once you have a reasonable sparse reconstruction, you can view the dense one:\n",
    "\n",
    "<code>\n",
    "OpenSfM$ sudo apt-get install meshlab\n",
    "OpenSfM$ meslab data/lab7-keyframes/undistorted/depthmaps/merged.ply\n",
    "</code>\n",
    "\n",
    "You should be able to see something like this:\n",
    "\n",
    "<img src=\"img/lab07-2.png\" width=\"600\"/>\n",
    "\n",
    "The dense mesh makes clear that the reconstruction works better for objects that are far from the camera. Reflections seem to wreak havoc and lead to multiple false matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements\n",
    "\n",
    "Perform some further experiments in attempts to get better 3D reconstructions from OpenSfM. Some things to try:\n",
    "\n",
    "1. Change the parameter depthmap_min_consistent_views to 3 or more.\n",
    "2. Check the undistorted versions of the images in data/lab7-keyframes/undistorted/images/. Do you think that the automatically extracted distortion parameters are working well? Try using OpenCV to undistort the images in advance using the results from intrinsic calibration we got in previous labs. Do you get better results?\n",
    "3. Check the estimated camera parameters in data/lab7-keyframes/camera_models.json. Does it seem reasonable? Would giving your own parameters be better? Note that OpenSfM only uses a single focal length, which is assumed the same for x and y, while our calibration generated a camera matrix with fx = 807 and fy = 804. Is this a big enough difference to affect OpenSfM? Redo the undistortion but this time output undistorted images with a single focal length, and then provide this focal length to OpenSfM by copying camera_models.json to camera_models_overrides.json prior to reconstruction. Note that the OpenSfM focal length fsfm is related to the OpenCV focal length fcv by fsfm = 2 * fcv / w, where w is the image width (1920 in our case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thought experiments\n",
    "\n",
    "Consider the following, discuss in your report, and make some recommendations:\n",
    "\n",
    "1. The biggest bottleneck to the reconstruction was extracting keyframes. Think about how you could write a program to automatically extract keyframes using, for example, the Lucas and Kanade optical flow method we looked at in the early labs. What would you require of the optical flow between keyframe i and keyframe i+1 ? For some ideas, see our VISAPP paper on the subject and other references you find online.\n",
    "\n",
    "2. Reflections on the floor seem to cause trouble for reconstruction, and the textureless floor surface means we almost never get good matches for points on the floor. Discuss how we could use our semantic segmentation model for floor/no-floor to automatically ignore the floor in the SfM (think about the masks provided for the Berlin Cathedral dataset), and also think about how, once we have the extrinsic parameters and scene scale, we could reconstruct a dense 3D mesh for the floor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing SfM in OpenCV\n",
    "\n",
    "To further understand what OpenSFM is doing, we'll also go into the SfM process in more detail using OpenCV. This material is from: *Mastering OpenCV4* by\n",
    "Packtpub. Note that images and some of the text are taken directly from the original source. This tutorial seems to only work in Linux.\n",
    "\n",
    "OpenCV has an abundance of tools to implement a full-fledged SfM pipeline from first principles. The \"sfm\" module allows us to get away with simply providing a non-parametric function with a list of images to crunch and receive a fully reconstructed scene with a sparse point cloud and camera poses. However, we will not take that route -- instead, we will see some useful methods that will allow us to have much more control over the reconstruction and exemplify some of the topics discussed in class and in previous sections of this lab manual.\n",
    "\n",
    "This section will begin with the very basics of SfM: matching images using key points and feature descriptors. We will then advance to finding tracks, and multiple views of similar features through the image set, using a match graph. We proceed with 3D reconstruction, 3D visualization, and finally MVS with OpenMVS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image feature matching\n",
    "\n",
    "SfM, as presented above, relies on the understanding of the geometric relationship between images as it pertains to the visible objects in them. We saw that we can calculate the exact motion between two images with sufficient information on how the objects in the images move. The essential matrix, which can be estimated linearly from image features, can be decomposed to obtain the rotation and translation elements that define a 3D rigid transform. Thereafter, this transform can help us triangulate 3D positions of points, from the 3D-2D projection equations or from a dense stereo matching over rectified epilines. It all begins with image feature matching, so we will see how to obtain robust and noise-free matching.\n",
    "\n",
    "OpenCV has an extensive offering of 2D feature detectors (also called extractors) and descriptors. Features are designed to be invariant to image deformations so they can be matched through translation, rotation, scaling, and other more complicated transformations (affine, projective) of the objects in the scene. One of the latest additions to OpenCV's APIs is the AKAZE feature extractor and detector, which presents a very good compromise between speed of calculation and robustness to transformation. AKAZE was shown to outperform other prominent features, such as ORB (short for Oriented BRIEF) and SURF (short for Speeded Up Robust Features).\n",
    "\n",
    "The following snippet will extract a set of AKAZE keypoints and calculate AKAZE feature descriptors for each of the images we collect in <tt>imagesFilenames</tt>, and save them in the keypoints and descriptors arrays respectively:\n",
    "\n",
    "    auto detector = AKAZE::create();\n",
    "    auto extractor = AKAZE::create();\n",
    "\n",
    "    for (const auto& i : imagesFilenames) {\n",
    "        Mat grayscale;\n",
    "        cvtColor(images[i], grayscale, COLOR_BGR2GRAY);\n",
    "        detector->detect(grayscale, keypoints[i]);\n",
    "        extractor->compute(grayscale, keypoints[i], descriptors[i]);\n",
    "\n",
    "        CV_LOG_INFO(TAG, \"Found \" + to_string(keypoints[i].size()) + \" \n",
    "        keypoints in \" + i);\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we also convert the images to grayscale; however, this step may be omitted and the results will not suffer.\n",
    "\n",
    "Here's a visualization of the detected features in two adjacent images. Notice how many of them repeat; this is known as feature repeatability, which is one of the most desired functions in a good feature extractor:\n",
    "\n",
    "<img src=\"img/lab07-3.png\" width=\"800\"/>\n",
    "\n",
    "Next up is matching the features between every pair of images. OpenCV provides an excellent feature matching suite. AKAZE feature descriptors are binary, meaning they cannot be regarded as binary-encoded numbers when matched; they must be compared on the bit level with bit-wise operators. OpenCV offers a Hamming distance metric for binary feature matchers, which essentially count the number of incorrect matches between the two-bit sequences:\n",
    "\n",
    "    vector<DMatch> matchWithRatioTest(const DescriptorMatcher& matcher, \n",
    "                                      const Mat& desc1, \n",
    "                                      const Mat& desc2) \n",
    "    {\n",
    "        // Raw match\n",
    "        vector< vector<DMatch> > nnMatch;\n",
    "        matcher.knnMatch(desc1, desc2, nnMatch, 2);\n",
    "\n",
    "        // Ratio test filter\n",
    "        vector<DMatch> ratioMatched;\n",
    "        for (size_t i = 0; i < nnMatch.size(); i++) {\n",
    "            const DMatch first = nnMatch[i][0];\n",
    "            const float dist1 = nnMatch[i][0].distance;\n",
    "            const float dist2 = nnMatch[i][1].distance;\n",
    "\n",
    "            if (dist1 < MATCH_RATIO_THRESHOLD * dist2) {\n",
    "                ratioMatched.push_back(first);\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return ratioMatched;\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding function not only invokes our matcher (for example, a BFMatcher(NORM_HAMMING)) regularly, it also performs the ratio test. This simple test is a very fundamental concept in many computer vision algorithms that rely on feature matching (such as SfM, panorama stitching, sparse tracking, and more). Instead of looking for a single match for a feature from image A in image B, we look for two matches in image B and make sure there is no confusion. Confusion in matching may arise if two potential matching-feature descriptors are too similar (in terms of their distance metric) and we cannot tell which of them is the correct match for the query, so we discard them both to prevent confusion.\n",
    "\n",
    "Next, we implement a reciprocity filter. This filter only allows feature matches that match (with a ratio test) in A to B, as well as B to A. Essentially, this is making sure there's a one-to-one match between features in image A and those in image B: a symmetric match. The reciprocity filter removes even more ambiguity and contributes to a cleaner, more robust match:\n",
    "\n",
    "    // Match with ratio test filter\n",
    "    vector<DMatch> match = matchWithRatioTest(matcher, descriptors[imgi], descriptors[imgj]);\n",
    "\n",
    "    // Reciprocity test filter\n",
    "    vector<DMatch> matchRcp = matchWithRatioTest(matcher, descriptors[imgj], descriptors[imgi]);\n",
    "    vector<DMatch> merged;\n",
    "    for (const DMatch& dmrecip : matchRcp) {\n",
    "        bool found = false;\n",
    "        for (const DMatch& dm : match) {\n",
    "            // Only accept match if 1 matches 2 AND 2 matches 1.\n",
    "            if (dmrecip.queryIdx == dm.trainIdx and dmrecip.trainIdx == \n",
    "            dm.queryIdx) {\n",
    "                merged.push_back(dm);\n",
    "                found = true;\n",
    "                break;\n",
    "            }\n",
    "        }\n",
    "        if (found) {\n",
    "            continue;\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we apply the epipolar constraint. Every two images that have a valid rigid transformation between them would comply with the epipolar constraint over their feature points, , and those who don't pass this test (with sufficient success) are likely not a good match and may contribute to noise. We achieve this by calculating the fundamental matrix with a voting algorithm (RANSAC) and checking for the ratio of inliers to outliers. We apply a threshold to discard matches with a low survival rate with respect to the original match:\n",
    "\n",
    "    // Fundamental matrix filter\n",
    "    vector<uint8_t> inliersMask(merged.size());\n",
    "    vector<Point2f> imgiPoints, imgjPoints;\n",
    "    for (const DMatch& m : merged) {\n",
    "        imgiPoints.push_back(keypoints[imgi][m.queryIdx].pt);\n",
    "        imgjPoints.push_back(keypoints[imgj][m.trainIdx].pt);\n",
    "    }\n",
    "    findFundamentalMat(imgiPoints, imgjPoints, inliersMask);\n",
    "\n",
    "    vector<DMatch> final;\n",
    "    for (size_t m = 0; m < merged.size(); m++) {\n",
    "        if (inliersMask[m]) {\n",
    "            final.push_back(merged[m]);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if ((float)final.size() / (float)match.size() < PAIR_MATCH_SURVIVAL_RATE) {\n",
    "        CV_LOG_INFO(TAG, \"Final match '\" + imgi + \"'->'\" + imgj + \"' has less than \"+to_string(PAIR_MATCH_SURVIVAL_RATE)+\" inliers from orignal. Skip\");\n",
    "        continue;\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the effect of each of the filtering steps, raw match, ratio, reciprocity, and epipolar, in the following figure:\n",
    "\n",
    "<img src=\"img/lab07-4.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding feature tracks\n",
    "\n",
    "The concept of feature tracks was introduced in SfM literature as early as 1992 in Tomasi and Kanade's work (Shape and Motion from Image Streams, 1992) and made famous in Snavely and Szeliski's seminal photo tourism work from 2007 for large-scale unconstrained reconstructions. Tracks are simply the 2D positions of a single scene feature, an interesting point, over a number of views. Tracks are important since they maintain consistency across frames than can be composed into a global optimization problem, as Snavely suggested. Tracks are specifically important to us since OpenCV's sfm module allows to reconstruct a scene by providing just the 2D tracks across all the views:\n",
    "\n",
    "<img src=\"img/lab07-5.png\" width=\"800\"/>\n",
    "\n",
    "Having already found a pair-wise match between all views, we have the required information to find tracks within those matched features. If we follow feature i in the first image to the second image through the match, then from the second image to the third image through their own match, and so on, we would end up with its track. This sort of bookkeeping can easily become too hard to implement in a straightforward fashion with standard data structures. However, it can be simply done if we represent all the matches in a match graph. Each node in the graph would be a feature detected in a single image, and edges would be the matches we recovered. From the feature nodes of the first image, we would have many edges to the feature nodes of the second image, third image, fourth image, and so on (for matches not discarded by our filters). Since our matches are reciprocal (symmetric), the graph can be undirected. Moreover, the reciprocity test ensures that for feature i in the first image, there is only one matching feature j in the second image, and vice versa: feature j will only match back to feature i.\n",
    "\n",
    "The following is a visual example of such a match graph. The node colors represent the image from which the feature point (node) has originated. Edges represent a match between image features. We can notice the very strong pattern of a feature matching chain from the first image to the last:\n",
    "\n",
    "<img src=\"img/lab07-6.png\" width=\"800\"/>\n",
    "\n",
    "To code the match graph, we can use the Boost Graph Library (BGL), which has an extensive API for graph processing and algorithms. Constructing the graph is straightforward; we simply augment the nodes with the image ID and feature ID, so later we can trace back the origin:\n",
    "\n",
    "    using namespace boost;\n",
    "\n",
    "    struct ImageFeature {\n",
    "        string image;\n",
    "        size_t featureID;\n",
    "    };\n",
    "    typedef adjacency_list < listS, vecS, undirectedS, ImageFeature > Graph;\n",
    "    typedef graph_traits < Graph >::vertex_descriptor Vertex;\n",
    "    map<pair<string, int>, Vertex> vertexByImageFeature;\n",
    "\n",
    "    Graph g;\n",
    "\n",
    "    // Add vertices - image features\n",
    "    for (const auto& imgi : keypoints) {\n",
    "        for (size_t i = 0; i < imgi.second.size(); i++) {\n",
    "            Vertex v = add_vertex(g);\n",
    "            g[v].image = imgi.first;\n",
    "            g[v].featureID = i;\n",
    "            vertexByImageFeature[make_pair(imgi.first, i)] = v;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Add edges - feature matches\n",
    "    for (const auto& match : matches) {\n",
    "        for (const DMatch& dm : match.second) {\n",
    "            Vertex& vI = vertexByImageFeature[make_pair(match.first.first, dm.queryIdx)];\n",
    "            Vertex& vJ = vertexByImageFeature[make_pair(match.first.second, dm.trainIdx)];\n",
    "            add_edge(vI, vJ, g);\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at a visualization of the resulting graph (using boost::write_graphviz()), we can see many cases where our matching is erroneous. A bad match chain will involve more than one feature from the same image in the chain. We marked a few such instances in the following figure; notice some chains have two or more nodes with the same color:\n",
    "\n",
    "<img src=\"img/lab07-7.png\" width=\"800\"/>\n",
    "\n",
    "We can notice the chains are essentially connected components in the graph. Extracting the components is simple using boost::connected_components():\n",
    "\n",
    "    // Get connected components\n",
    "    std::vector<int> component(num_vertices(gFiltered), -1);\n",
    "    int num = connected_components(gFiltered, &component[0]);\n",
    "    map<int, vector<Vertex> > components;\n",
    "    for (size_t i = 0; i != component.size(); ++i) {\n",
    "        if (component[i] >= 0) {\n",
    "            components[component[i]].push_back(i);\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter out the bad components (with more than one feature from any one image) to get a clean match graph.\n",
    "\n",
    "### 3D reconstruction and visualization\n",
    "\n",
    "Having obtained the tracks in principle, we need to align them in a data structure that OpenCV's SfM module expects. Unfortunately, the sfm module is not very well documented, so this part we have to figure out on our own from the source code. We will be invoking the following function under the cv::sfm:: namespace, which can be found in opencv_contrib/modules/sfm/include/opencv2/sfm/reconstruct.hpp:\n",
    "\n",
    "    void reconstruct(InputArrayOfArrays points2d, OutputArray Ps, OutputArray points3d, InputOutputArray K, bool is_projective = false);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The opencv_contrib/modules/sfm/src/simple_pipeline.cpp file has a major hint as to what that function expects as input:\n",
    "\n",
    "    static void\n",
    "    parser_2D_tracks( const std::vector<Mat> &points2d, libmv::Tracks &tracks )\n",
    "    {\n",
    "      const int nframes = static_cast<int>(points2d.size());\n",
    "      for (int frame = 0; frame < nframes; ++frame) {\n",
    "        const int ntracks = points2d[frame].cols;\n",
    "        for (int track = 0; track < ntracks; ++track) {\n",
    "          const Vec2d track_pt = points2d[frame].col(track);\n",
    "          if ( track_pt[0] > 0 && track_pt[1] > 0 )\n",
    "            tracks.Insert(frame, track, track_pt[0], track_pt[1]);\n",
    "        }\n",
    "      }\n",
    "    } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the sfm module uses a reduced version of libmv (https://developer.blender.org/tag/libmv/), which is a well-established SfM package used for 3D reconstruction for cinema production with the Blender 3D (https://www.blender.org/) graphics software.\n",
    "\n",
    "We can tell the tracks need to be placed in a vector of multiple individual cv::Mat, where each contains an aligned list of cv::Vec2d as columns, meaning it has two rows of double. We can also deduce that missing (unmatched) feature points in a track will have a negative coordinate. The following snippet will extract tracks in the desired data structure from the match graph:\n",
    "\n",
    "    vector<Mat> tracks(nViews); // Initialize to number of views\n",
    "\n",
    "    // Each component is a track\n",
    "    const size_t nViews = imagesFilenames.size();\n",
    "    tracks.resize(nViews);\n",
    "    for (int i = 0; i < nViews; i++) {\n",
    "        tracks[i].create(2, components.size(), CV_64FC1);\n",
    "        tracks[i].setTo(-1.0); // default is (-1, -1) - no match\n",
    "    }\n",
    "    int i = 0;\n",
    "    for (auto c = components.begin(); c != components.end(); ++c, ++i) {\n",
    "        for (const int v : c->second) {\n",
    "            const int imageID = imageIDs[g[v].image];\n",
    "            const size_t featureID = g[v].featureID;\n",
    "            const Point2f p = keypoints[g[v].image][featureID].pt;\n",
    "            tracks[imageID].at<double>(0, i) = p.x;\n",
    "            tracks[imageID].at<double>(1, i) = p.y;\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow up with running the reconstruction function, collecting the sparse 3D point cloud and the color for each 3D point, and afterward, visualize the results (using functions from cv::viz::):\n",
    "\n",
    "    cv::sfm::reconstruct(tracks, Rs, Ts, K, points3d, true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will produce a sparse reconstruction with a point cloud and camera positions, visualized in the following image:\n",
    "\n",
    "<img src=\"img/lab07-8.png\" width=\"800\"/>\n",
    "\n",
    "Re-projecting the 3D points back on the 2D images we can validate a correct reconstruction:\n",
    "\n",
    "<img src=\"img/lab07-9.png\" width=\"800\"/>\n",
    "\n",
    "See the entire code for reconstruction and visualization in the accompanying source repository.\n",
    "\n",
    "Notice the reconstruction is very sparse; we only see 3D points where features have matched. This doesn't make for a very appealing effect when getting the geometry of objects in the scene. In many cases, SfM pipelines do not conclude with a sparse reconstruction, which is not useful for many applications, such as 3D scanning. Next, we will see how to get a dense reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MVS for dense reconstruction\n",
    "With the sparse 3D point cloud and the positions of the cameras, we can proceed with dense reconstruction using MVS. We already learned the basic concept of MVS in the first section; however, we do not need to implement this from scratch, but rather we can use the OpenMVS project. To use OpenMVS for cloud densifying, we must save our project in a specialized format. OpenMVS provides a class for saving and loading .mvs projects, the MVS::Interface class, defined in MVS/Interface.h.\n",
    "\n",
    "Let's start with the camera:\n",
    "\n",
    "    MVS::Interface interface;\n",
    "    MVS::Interface::Platform p;\n",
    "\n",
    "    // Add camera\n",
    "    MVS::Interface::Platform::Camera c;\n",
    "    c.K = Matx33d(K_); // The intrinsic matrix as refined by the bundle adjustment\n",
    "    c.R = Matx33d::eye(); // Camera doesn't have any inherent rotation\n",
    "    c.C = Point3d(0,0,0); // or translation\n",
    "    c.name = \"Camera1\";\n",
    "    const Size imgS = images[imagesFilenames[0]].size();\n",
    "    c.width = imgS.width; // Size of the image, to normalize the intrinsics\n",
    "    c.height = imgS.height;\n",
    "    p.cameras.push_back(c);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When adding the camera poses (views), we must take care. OpenMVS expects to get the rotation and center of the camera, and not the camera pose matrix for point projection $[R|t]$. We therefore must translate the translation vector to represent the center of the camera by applying the inverse rotation $-R^tt$:\n",
    "\n",
    "    // Add views\n",
    "    p.poses.resize(Rs.size());\n",
    "    for (size_t i = 0; i < Rs.size(); ++i) {\n",
    "        Mat t = -Rs[i].t() * Ts[i]; // Camera *center*\n",
    "        p.poses[i].C.x = t.at<double>(0);\n",
    "        p.poses[i].C.y = t.at<double>(1);\n",
    "        p.poses[i].C.z = t.at<double>(2);\n",
    "        Rs[i].convertTo(p.poses[i].R, CV_64FC1);\n",
    "\n",
    "        // Add corresponding image (make sure index aligns)\n",
    "        MVS::Interface::Image image;\n",
    "        image.cameraID = 0;\n",
    "        image.poseID = i;\n",
    "        image.name = imagesFilenames[i];\n",
    "        image.platformID = 0;\n",
    "        interface.images.push_back(image);\n",
    "    }\n",
    "    p.name = \"Platform1\";\n",
    "    interface.platforms.push_back(p);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding the point cloud to the Interface as well, we can proceed with the cloud densifying in the command line:\n",
    "\n",
    "    $ ${openMVS}/build/bin/DensifyPointCloud -i crazyhorse.mvs\n",
    "    18:48:32 [App ] Command line: -i crazyhorse.mvs\n",
    "    18:48:32 [App ] Camera model loaded: platform 0; camera 0; f 0.896x0.896; poses 7\n",
    "    18:48:32 [App ] Image loaded 0: P1000965.JPG\n",
    "    18:48:32 [App ] Image loaded 1: P1000966.JPG\n",
    "    18:48:32 [App ] Image loaded 2: P1000967.JPG\n",
    "    18:48:32 [App ] Image loaded 3: P1000968.JPG\n",
    "    18:48:32 [App ] Image loaded 4: P1000969.JPG\n",
    "    18:48:32 [App ] Image loaded 5: P1000970.JPG\n",
    "    18:48:32 [App ] Image loaded 6: P1000971.JPG\n",
    "    18:48:32 [App ] Scene loaded from interface format (11ms):\n",
    "    7 images (7 calibrated) with a total of 5.25 MPixels (0.75 MPixels/image)\n",
    "    1557 points, 0 vertices, 0 faces\n",
    "    18:48:32 [App ] Preparing images for dense reconstruction completed: 7 images (125ms)\n",
    "    18:48:32 [App ] Selecting images for dense reconstruction completed: 7 images (5ms)\n",
    "    Estimated depth-maps 7 (100%, 1m44s705ms)\n",
    "    Filtered depth-maps 7 (100%, 1s671ms)\n",
    "    Fused depth-maps 7 (100%, 421ms)\n",
    "    18:50:20 [App ] Depth-maps fused and filtered: 7 depth-maps, 1653963 depths, 263027 points (16%%) (1s684ms)\n",
    "    18:50:20 [App ] Densifying point-cloud completed: 263027 points (1m48s263ms)\n",
    "    18:50:21 [App ] Scene saved (489ms):\n",
    "    7 images (7 calibrated)\n",
    "    263027 points, 0 vertices, 0 faces\n",
    "    18:50:21 [App ] Point-cloud saved: 263027 points (46ms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process might take a few minutes to complete. However, once it's done, the results are very impressive. The dense point cloud has a whopping 263,027 3D points, compared to just 1,557 in the sparse cloud. We can visualize the dense OpenMVS project using the Viewer app bundled in OpenMVS:\n",
    "\n",
    "<img src=\"img/lab07-10.png\" width=\"800\"/>\n",
    "\n",
    "OpenMVS has several more functions to complete the reconstruction, such as extracting a triangular mesh from the dense point cloud.\n",
    "\n",
    "You can load image samples from <link>[here](img/crazyhorse)</link>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C++ full code\n",
    "\n",
    "Here's the full C++ code:\n",
    "\n",
    "    #include <iostream>\n",
    "    #include <algorithm>\n",
    "    #include <string>\n",
    "    #include <numeric>\n",
    "\n",
    "    #define CERES_FOUND true\n",
    "\n",
    "    #include <opencv2/opencv.hpp>\n",
    "    #include <opencv2/sfm.hpp>\n",
    "    #include <opencv2/viz.hpp>\n",
    "    #include <opencv2/core/utils/logger.hpp>\n",
    "    #include <opencv2/core/utils/filesystem.hpp>\n",
    "    #include <opencv2/xfeatures2d.hpp>\n",
    "\n",
    "    #include <boost/filesystem.hpp>\n",
    "    #include <boost/graph/graph_traits.hpp>\n",
    "    #include <boost/graph/adjacency_list.hpp>\n",
    "    #include <boost/graph/connected_components.hpp>\n",
    "    #include <boost/graph/graphviz.hpp>\n",
    "\n",
    "    #define _USE_OPENCV true\n",
    "    #include <libs/MVS/Interface.h>\n",
    "\n",
    "    using namespace cv;\n",
    "    using namespace std;\n",
    "    namespace fs = boost::filesystem;\n",
    "\n",
    "    class StructureFromMotion {\n",
    "\n",
    "    public:\n",
    "        StructureFromMotion(const string& dir,\n",
    "                const float matchSurvivalRate = 0.5f,\n",
    "                const bool viz = false,\n",
    "                const string mvs = \"\",\n",
    "                const string cloud = \"\",\n",
    "                const bool saveDebug = false)\n",
    "            :PAIR_MATCH_SURVIVAL_RATE(matchSurvivalRate),\n",
    "             visualize(viz),\n",
    "             saveMVS(mvs),\n",
    "             saveCloud(cloud),\n",
    "             saveDebugVisualizations(saveDebug)\n",
    "\n",
    "        {\n",
    "            findImagesInDiretcory(dir);\n",
    "        }\n",
    "\n",
    "        void runSfM() {\n",
    "            extractFeatures();\n",
    "            matchFeatures();\n",
    "            buildTracks();\n",
    "            reconstructFromTracks();\n",
    "            if (visualize) {\n",
    "                visualize3D();\n",
    "            }\n",
    "            if (saveMVS != \"\") {\n",
    "                saveToMVSFile();\n",
    "            }\n",
    "            if (saveCloud != \"\") {\n",
    "                CV_LOG_INFO(TAG, \"Save point cloud to: \" + saveCloud);\n",
    "                viz::writeCloud(saveCloud, pointCloud, pointCloudColor);\n",
    "            }\n",
    "        }\n",
    "\n",
    "    private:\n",
    "        void findImagesInDiretcory(const string& dir) {\n",
    "            CV_LOG_INFO(TAG, \"Finding images in \" + dir);\n",
    "\n",
    "            utils::fs::glob(dir, \"*.jpg\", imagesFilenames);\n",
    "            utils::fs::glob(dir, \"*.JPG\", imagesFilenames);\n",
    "            utils::fs::glob(dir, \"*.png\", imagesFilenames);\n",
    "            utils::fs::glob(dir, \"*.PNG\", imagesFilenames);\n",
    "\n",
    "            std::sort(imagesFilenames.begin(), imagesFilenames.end());\n",
    "\n",
    "            CV_LOG_INFO(TAG, \"Found \" + std::to_string(imagesFilenames.size()) + \" images\");\n",
    "\n",
    "            CV_LOG_INFO(TAG, \"Reading images...\");\n",
    "            for (const auto& i : imagesFilenames) {\n",
    "                CV_LOG_INFO(TAG, i);\n",
    "                images[i] = imread(i);\n",
    "                imageIDs[i] = images.size() - 1;\n",
    "            }\n",
    "        }\n",
    "\n",
    "        void extractFeatures() {\n",
    "            CV_LOG_INFO(TAG, \"Extract Features\");\n",
    "\n",
    "            auto detector = AKAZE::create();\n",
    "            auto extractor = AKAZE::create();\n",
    "\n",
    "            for (const auto& i : imagesFilenames) {\n",
    "                Mat grayscale;\n",
    "                cvtColor(images[i], grayscale, COLOR_BGR2GRAY);\n",
    "                detector->detect(grayscale, keypoints[i]);\n",
    "                extractor->compute(grayscale, keypoints[i], descriptors[i]);\n",
    "\n",
    "                CV_LOG_INFO(TAG, \"Found \" + to_string(keypoints[i].size()) + \" keypoints in \" + i);\n",
    "\n",
    "                if (saveDebugVisualizations) {\n",
    "                    Mat out;\n",
    "                    drawKeypoints(images[i], keypoints[i], out, Scalar(0,0,255));\n",
    "                    imwrite(fs::basename(fs::path(i)) + \"_features.jpg\", out);\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        vector<DMatch> matchWithRatioTest(const DescriptorMatcher& matcher, const Mat& desc1, const Mat& desc2) {\n",
    "            // Raw match\n",
    "            vector< vector<DMatch> > nnMatch;\n",
    "            matcher.knnMatch(desc1, desc2, nnMatch, 2);\n",
    "\n",
    "            // Ratio test filter\n",
    "            vector<DMatch> ratioMatched;\n",
    "            for (size_t i = 0; i < nnMatch.size(); i++) {\n",
    "                DMatch first = nnMatch[i][0];\n",
    "                float dist1 = nnMatch[i][0].distance;\n",
    "                float dist2 = nnMatch[i][1].distance;\n",
    "\n",
    "                if (dist1 < MATCH_RATIO_THRESHOLD * dist2) {\n",
    "                    ratioMatched.push_back(first);\n",
    "                }\n",
    "            }\n",
    "\n",
    "            return ratioMatched;\n",
    "        }\n",
    "\n",
    "        void matchFeatures() {\n",
    "            CV_LOG_INFO(TAG, \"Match Features\");\n",
    "\n",
    "            BFMatcher matcher(NORM_HAMMING);\n",
    "\n",
    "            for (size_t i = 0; i < imagesFilenames.size() - 1; ++i) {\n",
    "                for (size_t j = i + 1; j < imagesFilenames.size(); ++j) {\n",
    "                    const string imgi = imagesFilenames[i];\n",
    "                    const string imgj = imagesFilenames[j];\n",
    "\n",
    "                    // Match with ratio test filter\n",
    "                    vector<DMatch> match = matchWithRatioTest(matcher, descriptors[imgi], descriptors[imgj]);\n",
    "\n",
    "                    // Reciprocity test filter\n",
    "                    vector<DMatch> matchRcp = matchWithRatioTest(matcher, descriptors[imgj], descriptors[imgi]);\n",
    "                    vector<DMatch> merged;\n",
    "                    for (const DMatch& dmr : matchRcp) {\n",
    "                        bool found = false;\n",
    "                        for (const DMatch& dm : match) {\n",
    "                            // Only accept match if 1 matches 2 AND 2 matches 1.\n",
    "                            if (dmr.queryIdx == dm.trainIdx and dmr.trainIdx == dm.queryIdx) {\n",
    "                                merged.push_back(dm);\n",
    "                                found = true;\n",
    "                                break;\n",
    "                            }\n",
    "                        }\n",
    "                        if (found) {\n",
    "                            continue;\n",
    "                        }\n",
    "                    }\n",
    "\n",
    "                    // Fundamental matrix filter\n",
    "                    vector<uint8_t> inliersMask(merged.size());\n",
    "                    vector<Point2f> imgiPoints, imgjPoints;\n",
    "                    for (const auto& m : merged) {\n",
    "                        imgiPoints.push_back(keypoints[imgi][m.queryIdx].pt);\n",
    "                        imgjPoints.push_back(keypoints[imgj][m.trainIdx].pt);\n",
    "                    }\n",
    "                    findFundamentalMat(imgiPoints, imgjPoints, inliersMask);\n",
    "\n",
    "                    vector<DMatch> final;\n",
    "                    for (size_t m = 0; m < merged.size(); m++) {\n",
    "                        if (inliersMask[m]) {\n",
    "                            final.push_back(merged[m]);\n",
    "                        }\n",
    "                    }\n",
    "\n",
    "                    if ((float)final.size() / (float)match.size() < PAIR_MATCH_SURVIVAL_RATE) {\n",
    "                        CV_LOG_INFO(TAG, \"Final match '\" + imgi + \"'->'\" + imgj + \"' has less than \"+to_string(PAIR_MATCH_SURVIVAL_RATE)+\" inliers from orignal. Skip\");\n",
    "                        continue;\n",
    "                    }\n",
    "\n",
    "                    matches[make_pair(imgi, imgj)] = final;\n",
    "\n",
    "                    CV_LOG_INFO(TAG, \"Matching \" + imgi + \" and \" + imgj + \": \" + to_string(final.size()) + \" / \" + to_string(match.size()));\n",
    "\n",
    "                    if (saveDebugVisualizations) {\n",
    "                        Mat out;\n",
    "                        vector<DMatch> rawmatch;\n",
    "                        matcher.match(descriptors[imgi], descriptors[imgj], rawmatch);\n",
    "                        vector<pair<string, vector<DMatch>& > > showList{\n",
    "                                {\"Raw Match\", rawmatch},\n",
    "                                {\"Ratio Test Filter\", match},\n",
    "                                {\"Reciprocal Filter\", merged},\n",
    "                                {\"Epipolar Filter\", final}\n",
    "                        };\n",
    "                        for (size_t i = 0; i< showList.size(); i++) {\n",
    "                            drawMatches(images[imgi], keypoints[imgi],\n",
    "                                        images[imgj], keypoints[imgj],\n",
    "                                        showList[i].second, out, CV_RGB(255,0,0));\n",
    "                            putText(out, showList[i].first, Point(10,50), FONT_HERSHEY_COMPLEX, 2.0, CV_RGB(255,255,255), 2);\n",
    "                            putText(out, \"# Matches: \" + to_string(showList[i].second.size()), Point(10,100), FONT_HERSHEY_COMPLEX, 1.0, CV_RGB(255,255,255));\n",
    "                            imwrite(fs::basename(fs::path(imgi)) + \"_\" + fs::basename(fs::path(imgj)) + \"_\" + to_string(i) + \".jpg\", out);\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        void buildTracks() {\n",
    "            CV_LOG_INFO(TAG, \"Build tracks\");\n",
    "\n",
    "            using namespace boost;\n",
    "\n",
    "            struct ImageFeature {\n",
    "                string image;\n",
    "                size_t featureID;\n",
    "            };\n",
    "            typedef adjacency_list < listS, vecS, undirectedS, ImageFeature > Graph;\n",
    "            typedef graph_traits < Graph >::vertex_descriptor Vertex;\n",
    "\n",
    "            map<pair<string, int>, Vertex> vertexByImageFeature;\n",
    "\n",
    "            Graph g;\n",
    "\n",
    "            // Add vertices - image features\n",
    "            for (const auto& imgi : keypoints) {\n",
    "                for (size_t i = 0; i < imgi.second.size(); i++) {\n",
    "                    Vertex v = add_vertex(g);\n",
    "                    g[v].image = imgi.first;\n",
    "                    g[v].featureID = i;\n",
    "                    vertexByImageFeature[make_pair(imgi.first, i)] = v;\n",
    "                }\n",
    "            }\n",
    "\n",
    "            // Add edges - feature matches\n",
    "            for (const auto& match : matches) {\n",
    "                for (const DMatch& dm : match.second) {\n",
    "                    Vertex& vI = vertexByImageFeature[make_pair(match.first.first, dm.queryIdx)];\n",
    "                    Vertex& vJ = vertexByImageFeature[make_pair(match.first.second, dm.trainIdx)];\n",
    "                    add_edge(vI, vJ, g);\n",
    "                }\n",
    "            }\n",
    "\n",
    "            using Filtered  = filtered_graph<Graph, keep_all, std::function<bool(Vertex)>>;\n",
    "            Filtered gFiltered(g, keep_all{}, [&g](Vertex vd) { return degree(vd, g) > 0; });\n",
    "\n",
    "            // Get connected components\n",
    "            std::vector<int> component(num_vertices(gFiltered), -1);\n",
    "            int num = connected_components(gFiltered, &component[0]);\n",
    "            map<int, vector<Vertex> > components;\n",
    "            for (size_t i = 0; i != component.size(); ++i) {\n",
    "                if (component[i] >= 0) {\n",
    "                    components[component[i]].push_back(i);\n",
    "                }\n",
    "            }\n",
    "            // Filter bad components (with more than 1 feature from a single image)\n",
    "            std::vector<int> vertexInGoodComponent(num_vertices(gFiltered), -1);\n",
    "            map<int, vector<Vertex> > goodComponents;\n",
    "            for (const auto& c : components) {\n",
    "                set<string> imagesInComponent;\n",
    "                bool isComponentGood = true;\n",
    "                for (int j = 0; j < c.second.size(); ++j) {\n",
    "                    const string imgId = g[c.second[j]].image;\n",
    "                    if (imagesInComponent.count(imgId) > 0) {\n",
    "                        // Image already represented in this component\n",
    "                        isComponentGood = false;\n",
    "                        break;\n",
    "                    } else {\n",
    "                        imagesInComponent.insert(imgId);\n",
    "                    }\n",
    "                }\n",
    "                if (isComponentGood) {\n",
    "                    for (int j = 0; j < c.second.size(); ++j) {\n",
    "                        vertexInGoodComponent[c.second[j]] = 1;\n",
    "                    }\n",
    "                    goodComponents[c.first] = c.second;\n",
    "                }\n",
    "            }\n",
    "\n",
    "            Filtered gGoodComponents(g, keep_all{}, [&vertexInGoodComponent](Vertex vd) {\n",
    "                return vertexInGoodComponent[vd] > 0;\n",
    "            });\n",
    "\n",
    "            CV_LOG_INFO(TAG, \"Total number of components found: \" + to_string(components.size()));\n",
    "            CV_LOG_INFO(TAG, \"Number of good components: \" + to_string(goodComponents.size()));\n",
    "            const int accum = std::accumulate(goodComponents.begin(),\n",
    "                                              goodComponents.end(), 0,\n",
    "                                              [](int a, pair<const int, vector<Vertex> >& v){\n",
    "                                                        return a+v.second.size();\n",
    "                                                    });\n",
    "            CV_LOG_INFO(TAG, \"Average component size: \" + to_string((float)accum / (float)(goodComponents.size())));\n",
    "\n",
    "            if (saveDebugVisualizations) {\n",
    "                struct my_node_writer {\n",
    "                    my_node_writer(Graph& g_, const map<string,int>& iid_) : g (g_), iid(iid_) {};\n",
    "                    void operator()(std::ostream& out, Vertex v) {\n",
    "                        const int imgId = iid[g[v].image];\n",
    "                        out << \" [label=\\\"\" << imgId << \"\\\" colorscheme=\\\"accent8\\\" fillcolor=\"<<(imgId+1)<<\" style=filled]\";\n",
    "                    };\n",
    "                    Graph g;\n",
    "                    map<string,int> iid;\n",
    "                };\n",
    "                std::ofstream ofs(\"match_graph_good_components.dot\");\n",
    "                write_graphviz(ofs, gGoodComponents, my_node_writer(g, imageIDs));\n",
    "                std::ofstream ofsf(\"match_graph_filtered.dot\");\n",
    "                write_graphviz(ofsf, gFiltered, my_node_writer(g, imageIDs));\n",
    "            }\n",
    "\n",
    "            // Each component is a track\n",
    "            const size_t nViews = imagesFilenames.size();\n",
    "            tracks.resize(nViews);\n",
    "            for (int i = 0; i < nViews; i++) {\n",
    "                tracks[i].create(2, goodComponents.size(), CV_64FC1);\n",
    "                tracks[i].setTo(-1.0);\n",
    "            }\n",
    "            int i = 0;\n",
    "            for (auto c = goodComponents.begin(); c != goodComponents.end(); ++c, ++i) {\n",
    "                for (const int v : c->second) {\n",
    "                    const int imageID = imageIDs[g[v].image];\n",
    "                    const size_t featureID = g[v].featureID;\n",
    "                    const Point2f p = keypoints[g[v].image][featureID].pt;\n",
    "                    tracks[imageID].at<double>(0, i) = p.x;\n",
    "                    tracks[imageID].at<double>(1, i) = p.y;\n",
    "                }\n",
    "            }\n",
    "\n",
    "            if (saveDebugVisualizations) {\n",
    "                vector<Scalar> colors = {CV_RGB(240, 248, 255),\n",
    "                                         CV_RGB(250, 235, 215),\n",
    "                                         CV_RGB(0, 255, 255),\n",
    "                                         CV_RGB(127, 255, 212),\n",
    "                                         CV_RGB(240, 255, 255),\n",
    "                                         CV_RGB(245, 245, 220),\n",
    "                                         CV_RGB(255, 228, 196),\n",
    "                                         CV_RGB(255, 235, 205),\n",
    "                                         CV_RGB(0, 0, 255),\n",
    "                                         CV_RGB(138, 43, 226),\n",
    "                                         CV_RGB(165, 42, 42),\n",
    "                                         CV_RGB(222, 184, 135)};\n",
    "\n",
    "                vector<Mat> imagesM;\n",
    "                for (const auto m : images) imagesM.push_back(m.second);\n",
    "                Mat out;\n",
    "                hconcat(vector<Mat>(imagesM.begin(), imagesM.begin() + 4), out);\n",
    "                RNG& rng = cv::theRNG();\n",
    "                const Size imgS = imagesM[0].size();\n",
    "                for (int tId = 0; tId < 20; tId++) {\n",
    "                    const int trackId = rng(tracks[0].cols); // Randomize a track ID\n",
    "\n",
    "                    // Show track over images\n",
    "                    for (int i = 0; i < 3; i++) {\n",
    "                        Point2f a = Point2f(tracks[i].col(trackId));\n",
    "                        Point2f b = Point2f(tracks[i + 1].col(trackId));\n",
    "\n",
    "                        if (a.x < 0 or a.y < 0 or b.x < 0 or b.y < 0) {\n",
    "                            continue;\n",
    "                        }\n",
    "\n",
    "                        const Scalar c = colors[tId % colors.size()];\n",
    "                        a.x += imgS.width * i;\n",
    "                        b.x += imgS.width * (i + 1);\n",
    "                        circle(out, a, 7, c, FILLED);\n",
    "                        circle(out, b, 7, c, FILLED);\n",
    "                        line(out, a, b, c, 3);\n",
    "                    }\n",
    "                    imwrite(\"tracks.jpg\", out);\n",
    "\n",
    "                    // Show track patches\n",
    "                    const int patchSize = 20;\n",
    "                    const Point2f patch(patchSize, patchSize);\n",
    "                    for (int i = 0; i < tracks.size(); i++) {\n",
    "                        Point2f a = Point2f(tracks[i].col(trackId));\n",
    "                        if (a.x < patchSize or a.y < patchSize or\n",
    "                            a.x > imgS.width-patchSize or a.y > imgS.height-patchSize) {\n",
    "                            continue;\n",
    "                        }\n",
    "\n",
    "                        imwrite(\"track_\" + to_string(trackId) + \"_\" + to_string(i) + \".png\",\n",
    "                                imagesM[i](Rect(a - patch, a + patch)));\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        bool reconstructFromTracks() {\n",
    "            CV_LOG_INFO(TAG, \"Reconstruct from \" + to_string(tracks[0].cols) + \" tracks\");\n",
    "            const Size imgS = images.begin()->second.size();\n",
    "            const float f = std::max(imgS.width,imgS.height);\n",
    "            Mat K = Mat(Matx33f{f, 0.0, imgS.width/2.0f,\n",
    "                                0.0, f, imgS.height/2.0f,\n",
    "                                0.0, 0.0, 1.0});\n",
    "            cv::sfm::reconstruct(tracks, Rs, Ts, K, points3d, true);\n",
    "\n",
    "            K.copyTo(K_);\n",
    "\n",
    "            CV_LOG_INFO(TAG, \"Reconstruction: \");\n",
    "            CV_LOG_INFO(TAG, \"Estimated 3D points: \" + to_string(points3d.size()));\n",
    "            CV_LOG_INFO(TAG, \"Estimated cameras: \" + to_string(Rs.size()));\n",
    "            CV_LOG_INFO(TAG, \"Refined intrinsics: \");\n",
    "            CV_LOG_INFO(TAG, K_);\n",
    "\n",
    "            if (Rs.size() != imagesFilenames.size()) {\n",
    "                CV_LOG_ERROR(TAG, \"Unable to reconstruct all camera views (\" + to_string(imagesFilenames.size()) + \")\");\n",
    "                return false;\n",
    "            }\n",
    "\n",
    "            if (tracks[0].cols != points3d.size()) {\n",
    "                CV_LOG_WARNING(TAG, \"Unable to reconstruct all tracks (\" + to_string(tracks[0].cols) + \")\");\n",
    "            }\n",
    "\n",
    "            // Create the point cloud\n",
    "            pointCloud.clear();\n",
    "            for (const auto &p : points3d) pointCloud.emplace_back(Vec3f(p));\n",
    "\n",
    "            // Get the point colors\n",
    "            pointCloudColor.resize(pointCloud.size(), Vec3b(0,255,0));\n",
    "            vector<Point2f> point2d(1);\n",
    "            for (int i = 0; i < (int)pointCloud.size(); i++) {\n",
    "                for (int j = 0; j < imagesFilenames.size(); ++j) {\n",
    "                    Mat point3d = Mat(pointCloud[i]).reshape(1, 1);\n",
    "                    cv::projectPoints(point3d, Rs[j], Ts[j], K_, Mat(), point2d);\n",
    "                    if (point2d[0].x < 0 or point2d[0].x >= imgS.width or point2d[0].y < 0 or\n",
    "                        point2d[0].y >= imgS.height) {\n",
    "                        continue;\n",
    "                    }\n",
    "                    pointCloudColor[i] = images[imagesFilenames[j]].at<Vec3b>(point2d[0]);\n",
    "                    break;\n",
    "                }\n",
    "            }\n",
    "\n",
    "            return true;\n",
    "        }\n",
    "\n",
    "        void visualize3D() {\n",
    "            CV_LOG_INFO(TAG, \"Visualize reconstruction\");\n",
    "\n",
    "            if (saveDebugVisualizations) {\n",
    "                // 3d point reprojections\n",
    "                Mat points2d;\n",
    "                Mat points3dM(points3d.size(), 1, CV_32FC3);\n",
    "                for (int i = 0 ; i < points3d.size(); i++) {\n",
    "                    points3dM.at<Vec3f>(i) = Vec3f(points3d[i]);\n",
    "                }\n",
    "                for (int j = 0; j < imagesFilenames.size(); j++) {\n",
    "                    cv::projectPoints(points3dM, Rs[j], Ts[j], K_, noArray(), points2d);\n",
    "\n",
    "                    Mat out;\n",
    "                    images[imagesFilenames[j]].copyTo(out);\n",
    "                    for (int i = 0; i < points2d.rows; i++) {\n",
    "                        circle(out, points2d.at<Point2f>(i), 3, CV_RGB(255, 0, 0), FILLED);\n",
    "                    }\n",
    "                    imwrite(\"reprojection_\" + to_string(j) + \".jpg\", out);\n",
    "                }\n",
    "            }\n",
    "\n",
    "            // Create 3D windows\n",
    "            viz::Viz3d window(\"Coordinate Frame\");\n",
    "            window.setWindowSize(Size(500, 500));\n",
    "            window.setWindowPosition(Point(150, 150));\n",
    "            window.setBackgroundColor(viz::Color::white());\n",
    "\n",
    "            // Recovering cameras\n",
    "            vector<Affine3d> path;\n",
    "            for (size_t i = 0; i < Rs.size(); ++i)\n",
    "                path.push_back(Affine3d(Rs[i],Ts[i]));\n",
    "\n",
    "            // Add the pointcloud\n",
    "            viz::WCloud cloud_widget(pointCloud, pointCloudColor);\n",
    "            window.showWidget(\"point_cloud\", cloud_widget);\n",
    "            // Add cameras\n",
    "            window.showWidget(\"cameras_frames_and_lines\", viz::WTrajectory(path, viz::WTrajectory::BOTH, 0.1, viz::Color::black()));\n",
    "            window.showWidget(\"cameras_frustums\", viz::WTrajectoryFrustums(path, K_, 0.1, viz::Color::navy()));\n",
    "            window.setViewerPose(path[0]);\n",
    "\n",
    "            /// Wait for key 'q' to close the window\n",
    "            CV_LOG_INFO(TAG, \"Press 'q' to close ... \")\n",
    "\n",
    "            window.spin();\n",
    "        }\n",
    "\n",
    "        void saveToMVSFile() {\n",
    "            CV_LOG_INFO(TAG, \"Save reconstruction to MVS file: \" + saveMVS)\n",
    "\n",
    "            MVS::Interface interface;\n",
    "            MVS::Interface::Platform p;\n",
    "\n",
    "            // Add camera\n",
    "            MVS::Interface::Platform::Camera c;\n",
    "            const Size imgS = images[imagesFilenames[0]].size();\n",
    "            c.K = Matx33d(K_);\n",
    "            c.R = Matx33d::eye();\n",
    "            c.C = Point3d(0,0,0);\n",
    "            c.name = \"Camera1\";\n",
    "            c.width = imgS.width;\n",
    "            c.height = imgS.height;\n",
    "            p.cameras.push_back(c);\n",
    "\n",
    "            // Add views\n",
    "            p.poses.resize(Rs.size());\n",
    "            for (size_t i = 0; i < Rs.size(); ++i) {\n",
    "                Mat t = -Rs[i].t() * Ts[i];\n",
    "                p.poses[i].C.x = t.at<double>(0);\n",
    "                p.poses[i].C.y = t.at<double>(1);\n",
    "                p.poses[i].C.z = t.at<double>(2);\n",
    "                Mat r; Rs[i].copyTo(r);\n",
    "                Mat(r).convertTo(p.poses[i].R, CV_64FC1);\n",
    "\n",
    "                // Add corresponding image\n",
    "                MVS::Interface::Image image;\n",
    "                image.cameraID = 0;\n",
    "                image.poseID = i;\n",
    "                image.name = imagesFilenames[i];\n",
    "                image.platformID = 0;\n",
    "                interface.images.push_back(image);\n",
    "            }\n",
    "            p.name = \"Platform1\";\n",
    "            interface.platforms.push_back(p);\n",
    "\n",
    "            // Add point cloud\n",
    "            for (size_t k = 0; k < points3d.size(); ++k) {\n",
    "                MVS::Interface::Color c;\n",
    "                MVS::Interface::Vertex v;\n",
    "                v.X = Vec3f(points3d[k]);\n",
    "\n",
    "                // Reproject to see if in image bounds and get the RGB color\n",
    "                Mat point3d;\n",
    "                Mat(points3d[k].t()).convertTo(point3d, CV_32FC1);\n",
    "                for (uint32_t j = 0; j < tracks.size(); ++j) {\n",
    "                    vector<Point2f> points2d(1);\n",
    "                    cv::projectPoints(point3d, Rs[j], Ts[j], K_, Mat(), points2d);\n",
    "                    if (points2d[0].x < 0 or points2d[0].x > imgS.width or\n",
    "                        points2d[0].y < 0 or points2d[0].y > imgS.height) {\n",
    "                        continue;\n",
    "                    } else {\n",
    "                        c.c = images[imagesFilenames[j]].at<Vec3b>(points2d[0]);\n",
    "                        v.views.push_back({j, 1.0});\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                interface.verticesColor.push_back(c);\n",
    "                interface.vertices.push_back(v);\n",
    "            }\n",
    "\n",
    "            MVS::ARCHIVE::SerializeSave(interface, saveMVS);\n",
    "        }\n",
    "\n",
    "        vector<String> imagesFilenames;\n",
    "        map<string, int> imageIDs;\n",
    "        map<string, Mat> images;\n",
    "        map<string, vector<KeyPoint> > keypoints;\n",
    "        map<string, Mat> descriptors;\n",
    "        map<pair<string, string>, vector<DMatch> > matches;\n",
    "        vector<Mat> Rs, Ts;\n",
    "        vector<Mat> points3d;\n",
    "        vector<Mat> tracks;\n",
    "        vector<Vec3f> pointCloud;\n",
    "        vector<Vec3b> pointCloudColor;\n",
    "        Matx33f K_;\n",
    "\n",
    "        const float MATCH_RATIO_THRESHOLD = 0.8f; // Nearest neighbor matching ratio\n",
    "        const float PAIR_MATCH_SURVIVAL_RATE;     // Ratio of surviving matches for a successful stereo match\n",
    "        const bool visualize;                     // Show 3D visualization of the sprase cloud?\n",
    "        const string saveMVS;                     // Save the reconstruction in MVS format for OpenMVS?\n",
    "        const string saveCloud;                   // Save the reconstruction to a point cloud file?\n",
    "        const bool saveDebugVisualizations;       // Save debug visualizations from the reconstruction process\n",
    "\n",
    "        const string TAG = \"StructureFromMotion\";\n",
    "    };\n",
    "\n",
    "\n",
    "    int main(int argc, char** argv) {\n",
    "        utils::logging::setLogLevel(utils::logging::LOG_LEVEL_DEBUG);\n",
    "\n",
    "        cv::CommandLineParser parser(argc, argv,\n",
    "                                     \"{help h ? |           | help message}\"\n",
    "                                     \"{@dir     | ./images  | directory with image files for reconstruction }\"\n",
    "                                     \"{mrate    | 0.5       | Survival rate of matches to consider image pair success }\"\n",
    "                                     \"{viz      | true      | Visualize the sparse point cloud reconstruction? }\"\n",
    "                                     \"{debug    | false     | Save debug visualizations to files? }\"\n",
    "                                     \"{mvs      | recon.mvs | Save reconstruction to an .mvs file. Provide filename }\"\n",
    "                                     \"{cloud    | cloud.ply | Save reconstruction to a point cloud file (PLY, XYZ and OBJ). Provide filename}\"\n",
    "        );\n",
    "\n",
    "        if (parser.has(\"help\"))\n",
    "        {\n",
    "            parser.printMessage();\n",
    "            return 0;\n",
    "        }\n",
    "\n",
    "        StructureFromMotion sfm(parser.get<string>(\"@dir\"),\n",
    "                                parser.get<float>(\"mrate\"),\n",
    "                                parser.get<bool>(\"viz\"),\n",
    "                                parser.get<string>(\"mvs\"),\n",
    "                                parser.get<string>(\"cloud\"),\n",
    "                                parser.get<bool>(\"debug\")\n",
    "                                );\n",
    "        sfm.runSfM();\n",
    "\n",
    "        return 0;\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Get the OpenCV C++ code running on the sample data from the Packt book's repository.\n",
    "2. Try the same process on the keyframes you extracted from the robot video and compare the OpenSFM / OpenCV results.\n",
    "\n",
    "## Report\n",
    "\n",
    "Provide a brief report documenting your experiments with OpenSFM and the customized OpenCV processing stream on video data from the robot, due in Google Classroom\n",
    "by next week's lab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
