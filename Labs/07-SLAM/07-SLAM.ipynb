{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "082cdb5b-4644-4440-a7ac-560a51c55734",
   "metadata": {},
   "source": [
    "# Computer Vision Lab 07: Visual SLAM\n",
    "\n",
    "Some of the information in this lab comes from:\n",
    "- https://www.cs.columbia.edu/~allen/F19/NOTES/slam_pka.pdf\n",
    "- [Github repo for PySLAM](https://github.com/luigifreda/pyslam)\n",
    "\n",
    "Today we will experiment with visual odometry / SLAM using the PySlam library.\n",
    "\n",
    "PySLAM performs simultaneous localization and mapping (SLAM) using monocular cameras.\n",
    "\n",
    "## SLAM\n",
    "\n",
    "SLAM is a method for building a map while at the same time localizing a robot / person / vehicle within that map.\n",
    "SLAM consists of two important features: mapping, and localization:\n",
    "- Mapping: What does the world look like? A map should integrate the information gathered sequentially with one or more sensors into a given representation.\n",
    "- Localization: Where am I? Localization means estimating robot / camera / person / vehicle pose relative to a given map (in SLAM, the map we are building\n",
    "  as we explore the world). Typical problems:\n",
    "    1. Pose tracking: beginning from a known initial pose\n",
    "    2. Global localization: localizing without any a priori knowledge about the starting position. Sometimes also called the \"captured robot problem.\"\n",
    "\n",
    "## Visual SLAM\n",
    "\n",
    "We can use many types of sensors for SLAM including ultrasonic sensors, laser scanners, and cameras. For cameras, we can use many different types of visual sensors, such as  monocular cameras, stereo rigs, RGB-D cameras, and so on.\n",
    "\n",
    "### Why cameras?\n",
    "- Vast information\n",
    "- Cheap and easy to use\n",
    "- Small size, weight, and power (SWaP) footprint\n",
    "- Passive\n",
    "\n",
    "### Applications of Visual SLAM\n",
    "- Low-cost robotics (e.g. a mobile robot with a cheap camera)\n",
    "- Agile robotics (e.g. drones)\n",
    "- Smartphones\n",
    "- AR/VR: inside-out tracking, gaming\n",
    "\n",
    "<img src=\"img/CV07_01.PNG\" title=\"Application\" style=\"width: 600px;\" />\n",
    "\n",
    "[add reference]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb9407a",
   "metadata": {},
   "source": [
    "### Feature-based visual odometry (VO)\n",
    "\n",
    "Visual SLAM requires some kind of association of pixels between keyframes. One method is frame-by-frame tracking of points, followed by camera pose estimation, and\n",
    "anohter is to use wide baseline matching techniques between keyframes without intermediate feature point tracking or optical flow. When we compute a sequence of camera\n",
    "poses without creating a long-term map, the process is referred to as visual odometry (VO).\n",
    "\n",
    "1. Feature detection: Detect a set of features $f_k$ at time $k$\n",
    "\n",
    "<img src=\"img/CV07_02.PNG\" title=\"Feature Detection\" style=\"width: 300px;\" />\n",
    "\n",
    "[add reference]\n",
    "\n",
    "2. Feature matching/tracking: Find correspondences between set of features $f_{k−1} , f_k$\n",
    "    - tracking: locally search each feature\n",
    "    - matching: independently detect features in each image and find correspondences on the basis of a similarity metric (exploit descriptors such SURF, SIFT, ORB, etc)\n",
    "    \n",
    "<img src=\"img/CV07_03.PNG\" title=\"Feature matching\" style=\"width: 600px;\" />\n",
    "\n",
    "[add reference]\n",
    "\n",
    "3. Motion estimation - Compute transformation Tk between two images Ik−1 and Ik from two sets of corresponding features $f_{k−1} , f_k$. The different algorithms depending on available sensor data:\n",
    "    - 2-D to 2-D: works on $f_{k−1} , f_k$ specified in 2-D image coords\n",
    "    - 3-D to 3-D: works on $X_{k−1} , X_k$ sets of 3D points corresponding to $f_{k−1} , f_k$\n",
    "    - 3-D to 2-D: works on $X_{k−1}$ set of 3D points corresponding to $f_{k−1}$, and on $f_k$ their corresponding 2-D reprojections on the image $I_k$\n",
    "    \n",
    "<img src=\"img/CV07_04.PNG\" title=\"Feature matching\" style=\"width: 400px;\" />\n",
    "\n",
    "[add reference]\n",
    "\n",
    "4. Local optimization - An iterative refinement over last m poses can be optionally performed after motion estimation to obtain a more accurate estimate of the local trajectory\n",
    "\n",
    "<img src=\"img/CV07_05.PNG\" title=\"Local optimization\" style=\"width: 600px;\" />\n",
    "\n",
    "We must minimize the reprojection error\n",
    "\n",
    "$$T_k = \n",
    "\\begin{bmatrix}\n",
    "R_{k-1,k} & t_{k-1,k} \\\\ \n",
    "0 & 1\n",
    "\\end{bmatrix}=\n",
    "\\text{argmin}_{X^i,C_k} \\sum_{i,k} ||p^i_k-g(X^i,C_k)||\n",
    "$$\n",
    "\n",
    "where $p^k_i$ is the $i$-th image point of the 3D landmark $X_i$ measured in the $k$-th image and\n",
    "$g(X_i, C_k)$ is its image reprojection according to the current camera pose $C_k$\n",
    "\n",
    "### VO vs Visual SLAM\n",
    "\n",
    "- The goal of SLAM in general is to obtain a global and consistent estimate of the robot path and the map. This is done by combining VO with a method for identifying *loop closures*. When a loop closure is detected, this information is used to reduce the drift in both the map and camera path (global bundle adjustment)\n",
    "- Conversely, VO aims at recovering a path incrementally, pose after pose, without trying to maintain a perfect global map. VO can potentially use optimization only over the last m pose path (windowed bundle adjustment)\n",
    "\n",
    "<img src=\"img/CV07_06.PNG\" title=\"close loop\" style=\"width: 600px;\" />\n",
    "\n",
    "- VO only aims at the local consistency of the trajectory, while SLAM aims to the global consistency of the trajectory and of the map\n",
    "- VO can be used as a building block for SLAM\n",
    "- VO is SLAM without loop closure\n",
    "- The choice between VO and Visual SLAM depends on the application requirements and the tradeoff between performance, consistency, and simplicity of implementation\n",
    "- VO trades off consistency for real-time performance, due to dropping the past history of the camera's motion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78f1eca",
   "metadata": {},
   "source": [
    "## PySlam main scripts\n",
    "\n",
    "- <code>main_vo.py</code> combines the simplest VO ingredients without performing any image point triangulation or windowed bundle adjustment. At each step $k$, <code>main_vo.py</code> estimates the current camera pose $C_k$ with respect to the previous one $C_{k-1}$. The inter-frame pose estimation returns $[R_{k-1,k},t_{k-1,k}]$ with $||t_{k-1,k}||=1$.\n",
    "\n",
    "- <code>main_slam.py</code> adds feature tracking along multiple frames, point triangulation, keyframe management and bundle adjustment in order to estimate the camera trajectory up-to-scale and build a map. It's still a VO pipeline but it shows some basic blocks which are necessary to develop a real visual SLAM pipeline.\n",
    "\n",
    "<img src=\"img/main-vo.png\" title=\"vo\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e591193",
   "metadata": {},
   "source": [
    "## PySlam Installation\n",
    "\n",
    "Clone this repo and its modules by running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9db792",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --recursive https://github.com/luigifreda/pyslam.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19769eea",
   "metadata": {},
   "source": [
    "### Setup for Ubuntu 20.04\n",
    "\n",
    "For Ubuntu 20.04, [follow the instructions in the Github README](https://github.com/luigifreda/pyslam#install-pyslam-under-ubuntu-2004).\n",
    "\n",
    "First, we check out the experimental branch ubuntu20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3da406",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git checkout ubuntu20  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daf3384",
   "metadata": {},
   "source": [
    "#### Basic Installation\n",
    "\n",
    "In order to run <code>main_vo.py</code> with venv, get in the root of the repository and run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ef0a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "!. pyenv-create.sh "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e8da1c",
   "metadata": {},
   "source": [
    "This will create a custom pyslam environment and will also activate it. Now, from the same terminal, you can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e16484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -O main_vo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3347b42c",
   "metadata": {},
   "source": [
    "#### Full Installation\n",
    "\n",
    "In order to run <code>main_slam.py</code> with venv, get in the root of the repository and run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd76104",
   "metadata": {},
   "outputs": [],
   "source": [
    "!. install_all_venv.sh "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127fb21b",
   "metadata": {},
   "source": [
    "This will compile the required thirdparty packages and will also activate the created pyslam environment. Now, from the same terminal, you can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74da715",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -O main_slam.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b56a28d",
   "metadata": {},
   "source": [
    "### Setup in MacOS\n",
    "\n",
    "Check the instructions in this [file](https://github.com/luigifreda/pyslam/blob/master/MAC.md).\n",
    "\n",
    "### Setup in Windows\n",
    "\n",
    "Check the instructions in this [file](https://github.com/luigifreda/pyslam/issues/51)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d85cd2b",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "You can use 4 different types of datasets:\n",
    "\n",
    "|Dataset|type in config.ini|\n",
    "|-----|-----|\n",
    "|KITTI odometry data set (grayscale, 22 GB)|type=KITTI_DATASET|\n",
    "|TUM dataset|type=TUM_DATASET|\n",
    "|video file|type=VIDEO_DATASET|\n",
    "|folder of images|type=FOLDER_DATASET|\n",
    "\n",
    "### KITTI Datasets\n",
    "pySLAM code expects the following structure in the specified KITTI path folder (specified in the section <code>[KITTI_DATASET]</code> of the file config.ini).\n",
    "\n",
    "├── sequences\n",
    "\n",
    "    ├── 00\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    ├── 21\n",
    "    \n",
    "├── poses\n",
    "\n",
    "    ├── 00.txt\n",
    "    \n",
    "        ...\n",
    "        \n",
    "    ├── 10.txt\n",
    "\n",
    "1. Download the dataset (grayscale images) from http://www.cvlibs.net/datasets/kitti/eval_odometry.php and prepare the KITTI folder as specified above\n",
    "\n",
    "2. Select the corresponding calibration settings file (parameter [KITTI_DATASET][cam_settings] in the file config.ini)\n",
    "\n",
    "### TUM Datasets\n",
    "pySLAM code expects a file associations.txt in each TUM dataset folder (specified in the section <code>[TUM_DATASET]</code> of the file config.ini).\n",
    "\n",
    "Download a sequence from http://vision.in.tum.de/data/datasets/rgbd-dataset/download and uncompress it.\n",
    "\n",
    "Associate RGB images and depth images using the python script associate.py. You can generate your associations.txt file by executing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8075d333",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python associate.py PATH_TO_SEQUENCE/rgb.txt PATH_TO_SEQUENCE/depth.txt > associations.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b11c945",
   "metadata": {},
   "source": [
    "Select the corresponding calibration settings file (parameter <code>[TUM_DATASET][cam_settings]</code> in the file config.ini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da89d702",
   "metadata": {},
   "source": [
    "### Camera Settings\n",
    "The folder settings contains the camera settings files which can be used for testing the code. These are the same used in the framework ORBSLAM2. You can easily modify one of those files for creating your own new calibration file (for your new datasets).\n",
    "\n",
    "In order to calibrate your camera, you can use the scripts in the folder calibration. In particular:\n",
    "\n",
    "1. use the script grab_chessboard_images.py to collect a sequence of images where the chessboard can be detected (set the chessboard size therein, you can use the calibration pattern calib_pattern.pdf in the same folder)\n",
    "2. use the script calibrate.py to process the collected images and compute the calibration parameters (set the chessboard size therein)\n",
    "\n",
    "If you want to use your camera, you have to:\n",
    "\n",
    "- calibrate it and configure WEBCAM.yaml accordingly\n",
    "- record a video (for instance, by using save_video.py in the folder calibration)\n",
    "- configure the <code>[VIDEO_DATASET]</code> section of config.ini in order to point to your video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d2763e",
   "metadata": {},
   "source": [
    "## Lab Homework\n",
    "\n",
    "Collect your own video with a phone / laptop / ground robot / dashboard camera and set up PySLAM for VO and SLAM with your video. Report the results and level of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21963c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
