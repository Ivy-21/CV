{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision, Lab 4: Semantic Segmentation\n",
    "\n",
    "Last time we used a simple generative model based on HSV color space features to segment our scene.\n",
    "\n",
    "Today we'll continue experimenting with methods to segment a scene with a sample ground robot in an indoor environment and the\n",
    "task of segmenting unoccupied ground plane space from obstacles.\n",
    "\n",
    "Today we'll move beyond a simple pixel-based classification to a more sophisticated CNN-based \"semantic\" segmentation model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Segmentation\n",
    "\n",
    "We've seen that color-based segmentation in HSV space using a generative machine learning model can be effective but has limitations when the objects to be segmented from the background have color distributions similar to the background. The best accuracy we could get for our floor/obstacle model last week was around 95%, but hat was with occasional errors grouping in large enough regions that would preclude our robot from navigating safely in the indoor environment.\n",
    "\n",
    "Semantic segmentation attempts to address this issue using a more sophisticated model to separate the scene into its constituent regions more effectively.\n",
    "\n",
    "Typical semantic segmentation models typically use a lot of resources. For example, the state of the art models published for the MIT ADE20K dataset in their [GitHub repository](https://github.com/CSAILVision/semantic-segmentation-pytorch) are very accurate and run at 2-17 FPS on a NVIDIA Pascal Titan Xp GPU. They also run well on a GTX 1080TI. But on an i7 CPU, I found that they take 9-11 SECONDS PER FRAME, and on an NVIDIA Jetson Nano's GPU, they take 12-42 SEDONDS PER FRAME!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lighter semantic segmentation models\n",
    "\n",
    "We would like to experiment with some semantic segmenation models that have a hope of running in real time on a small embedded system such as the Jetson Nano.\n",
    "\n",
    "NVIDIA has published a very useful repository [Jetson Inference](https://github.com/dusty-nv/jetson-inference) that contains versions of two semantic segmentation models: SegNet and UNet.\n",
    "\n",
    "I ran the SegNet model in this repository using FCN-ResNet18 trained on Pascal VOC with 320x320 input images. It takes a while to load the model into memory and so on, but inference time once all is ready is very fast: less than 70 ms.\n",
    "\n",
    "So it's fast! Unfortunately, it doesn't work particularly well out of the box:\n",
    "\n",
    "<img src=\"img/lab04-1.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we shouldn't expect it to, considering that the Pascal VOC dataset \"only\" contains 20 classes plus the \"background\" (the black label), and others of which are \"bottle\" (the purple label) and \"person\" (the tan label).\n",
    "\n",
    "That's on the NVIDIA Jetson Nano. The model itself was built with PyTorch, but it has been exported in ONNX (Open Neural Network Exchange) format, and the Jetson Nano executes it on TensorRT. You can download the [ONNX model from AIT](https://www.cs.ait.ac.th/~mdailey/class/vision/fcn_resnet18.onnx) (I just copied it from the excellent Jetson Inference repository).\n",
    "\n",
    "If you'd like to understand the structure of the model represented in this ONNX file, download the [Netron 4.3.4 AppImage for Linux](https://github.com/lutzroeder/netron) (or the .exe for Windows, etc.), run the AppImage, and load the file. You'll see that it\n",
    "\n",
    "1. Takes as input a 320x320 3-channel image\n",
    "2. Performs 64 7x7 convolutions\n",
    "3. Does batch normalization, ReLU, and MaxPool\n",
    "4. Runs a residual block with the following\n",
    "    1. 64 3x3 convolutions then BN then ReLU\n",
    "    2. 64 3x3 convolutions then BN\n",
    "5. ReLU\n",
    "6. Residual block (same structure as above)\n",
    "7. ReLU\n",
    "8. Residual block with the following\n",
    "    1. 128 3x3 convolutions then BN then ReLU\n",
    "    2. 128 3x3 convolutions then BN\n",
    "    3. Residual add using 128 1x1 convs to expand feature map\n",
    "9. Etc. (several more residual blocks and downscaling by a factor of 32)\n",
    "10. Final 21 1x1 convolution to obtain output 1x21x10x10 tensor\n",
    "\n",
    "Since this model is relatively small and at least generates some output, let's see if we can get it running on our OpenCV video stream. A model that runs nice and fast on an Intel CPU and on a Jetson GPU, in C++/OpenCV as well as Python, would be perfect.\n",
    "\n",
    "<img src=\"img/lab04-2.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a segmentation model using OpenCV DNN\n",
    "\n",
    "Let's see if we can get this ONNX model running in OpenCV with its DNN functionality.\n",
    "\n",
    "First, let's initialize with the Pascal VOC classes and read an image:\n",
    "\n",
    "### C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string aStringClasses[] = {\n",
    "    \"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\n",
    "    \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
    "};\n",
    "\n",
    "cv::Vec3b aColorClasses[] = {\n",
    "        { 0, 0, 0 }, { 255, 0, 0 }, { 0, 255, 0 }, { 0, 255, 120 }, { 0, 0, 255 }, { 255, 0, 255 }, { 70, 70, 70 },\n",
    "        { 102, 102, 156 }, { 190, 153, 153 }, { 180, 165, 180 }, { 150, 100, 100 }, { 153, 153, 153 },\n",
    "        { 250, 170, 30 }, { 220, 220, 0 }, { 107, 142, 35 }, { 192, 128, 128 }, { 70, 130, 180 }, { 220, 20, 60 },\n",
    "        { 0, 0, 142 }, { 0, 0, 70 }, { 119, 11, 32 }\n",
    "};\n",
    "\n",
    "int nClasses = sizeof(aColorClasses) / 3;\n",
    "\n",
    "// Read CNN definition\n",
    "\n",
    "auto net = cv::dnn::readNetFromONNX(ONNX_NETWORK_DEFINITION);\n",
    "\n",
    "// Read input image\n",
    "\n",
    "cv::Mat matFrame = cv::imread(IMAGE_FILE);\n",
    "if (matFrame.empty()) {\n",
    "    cerr << \"Cannot open image file \" << IMAGE_FILE << endl;\n",
    "    return -1;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aStringClasses = [\n",
    "    \"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\n",
    "    \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
    "]\n",
    "\n",
    "aColorClasses = [\n",
    "        ( 0, 0, 0 ), ( 255, 0, 0 ), ( 0, 255, 0 ), ( 0, 255, 120 ), ( 0, 0, 255 ), ( 255, 0, 255 ), ( 70, 70, 70 ),\n",
    "        ( 102, 102, 156 ), ( 190, 153, 153 ), ( 180, 165, 180 ), ( 150, 100, 100 ), ( 153, 153, 153 ),\n",
    "        ( 250, 170, 30 ), ( 220, 220, 0 ), ( 107, 142, 35 ), ( 192, 128, 128 ), ( 70, 130, 180 ), ( 220, 20, 60 ),\n",
    "        ( 0, 0, 142 ), ( 0, 0, 70 ), ( 119, 11, 32 )\n",
    "]\n",
    "\n",
    "nClasses = len(aColorClasses)\n",
    "\n",
    "# Read CNN definition\n",
    "net = cv2.dnn.readNetFromONNX(ONNX_NETWORK_DEFINITION)\n",
    "\n",
    "# Read input image\n",
    "matFrame = cv2.imread(IMAGE_FILE);\n",
    "if (matFrame is None):\n",
    "    print(\"Cannot open image file \", IMAGE_FILE)\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run an image through the network\n",
    "\n",
    "Once the input image is preprocessed to form a tensor suitable for input to our DNN model, we can just set the input layer of the network to point to the newly preprocessed image tensor, then we can do a forward pass through the network model:\n",
    "\n",
    "### C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Propagate the matInputTensor through the FCN model\n",
    "\n",
    "net.setInput(matInputTensor);\n",
    "cv::Mat matScore = net.forward();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propagate the matInputTensor through the FCN model\n",
    "\n",
    "net.setInput(matInputTensor)\n",
    "matScore = net.forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK but how to do that preprocessing before we feed the image to the model?\n",
    "\n",
    "Here's the thing: most CNN models (even object detection and image segmentation models) are based on a classification model as the \"backbone\" of the model. In the case of FCN-ResNet-18, the backbone is of course ResNet-18.\n",
    "\n",
    "Usually, training a model based on a classifier begins by loading weights trained for classification on ImageNet or another dataset then further training and/or \"fine tuning\" the model on a more specific dataset. We do this because we don't want to spend the week of GPU time it takes to get a model that analyzes image edges, puts them together into higher-level shapes, and gradually extracts a set of coarsely localized features that describe objects of interest.\n",
    "\n",
    "When data scientists train a model on ImageNet, they almost always perform a few common steps:\n",
    "\n",
    "1. Scale the input image's R, G, and B intensities (normally in the range 0-255) to the range 0-1.\n",
    "2. Scale the input image to the size needed for the classifier, or sample a patch from the input with the size required by the classifier.\n",
    "3. Subtract expected mean values for the R, G, and B channels. The magic values for ImageNet are 0.485 for R, 0.456 for G, and 0.406 for B.\n",
    "4. Divide by expected standard deviations for the R, G, and B channels. The magic values for ImageNet are 0.229 for R, 0.224 for G, and 0.225 for B.\n",
    "\n",
    "There is an OpenCV function <code>cv::dnn::blobFromImage()</code> that performs some of these steps but not all. Check the documentation and add the necessary code to prepare the image for presentation for the pre-trained network.\n",
    "\n",
    "Next, you'll want to use the result of the semantic segmentation model to color the input image and display for examination by the user, and perhaps add some information about CPU time required for the inference:\n",
    "\n",
    "### C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Colorize the image and display\n",
    "\n",
    "cv::Mat matColored;\n",
    "colorizeSegmentation(matFrame, matScore, matColored, aColorClasses, aStringClasses, nClasses);\n",
    "\n",
    "// Add timing information\n",
    "\n",
    "std::vector<double> layersTimes;\n",
    "double freq = cv::getTickFrequency() / 1000;\n",
    "double t = net.getPerfProfile(layersTimes) / freq;\n",
    "std::string label = cv::format(\"Inference time: %.2f ms\", t);\n",
    "cv::putText(matColored, label, cv::Point(10,30),\n",
    "        cv::FONT_HERSHEY_SIMPLEX, 1.0, cv::Scalar(0, 255, 0));\n",
    "\n",
    "// Display\n",
    "\n",
    "cv::namedWindow(WINDOW_NAME, WINDOW_FLAGS);\n",
    "cv::imshow(WINDOW_NAME, matColored);\n",
    "cv::waitKey(0);\n",
    "\n",
    "return 0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colorize the image and display\n",
    "\n",
    "matColored = cv2.colorizeSegmentation(matFrame, matScore, aColorClasses, aStringClasses, nClasses)\n",
    "\n",
    "# Add timing information\n",
    "freq = cv2.getTickFrequency() / 1000\n",
    "t, layersTimes = net.getPerfProfile()\n",
    "layersTimes /= freq\n",
    "label = \"Inference time: \" + str(layersTimes[0][0]) + \" ms\"\n",
    "cv2.putText(matColored, label, (10,60), cv2.FONT_HERSHEY_SIMPLEX, 2.0, (0, 255, 0), 5)\n",
    "\n",
    "# Display\n",
    "cv2.namedWindow(WINDOW_NAME, WINDOW_FLAGS)\n",
    "cv2.imshow(WINDOW_NAME, matColored)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll have to figure out how to use `blobFromImage()` from OpenCV yourself, and also write `colorizeSegmentation()` for yourself. See if you can get a result similar to the image above. It won't be exactly the same, as the jetson inference code scales the input image slightly differently from `blobFromImage()`. I got the following:\n",
    "\n",
    "<img src=\"img/lab04-3.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning on our own dataset\n",
    "\n",
    "Since the stock model was trained on VOC 2012's 21 classes, it is unable to give a recognizable segmentation on our data set.\n",
    "So here, we will want to fine-tune the FCN-ResNet-18 model on our own floor/obstacle dataset. We will load the existing weights, throw away the 21-class output layer of the existing model, and replace it with our own two-class output layer. We'll keep the weights for all but this last layer, then start training on our dataset.\n",
    "\n",
    "Also, since we will be training on a medium-sized dataset (VOC), we should set up our machine learning model development environment to use a powerful GPU server rather than our poor little laptops.\n",
    "\n",
    "## Torchvision versioning\n",
    "\n",
    "The standard PyTorch Vision module does not have FCN-ResNet-18 -- it only has the larger models like FCN-ResNet-50.\n",
    "\n",
    "The original FCN-ResNet-18 patch for torchvision 0.3.0 is at [this GitHub repository](https://github.com/dusty-nv/vision).\n",
    "\n",
    "By now, depending on your environment, you may be running torchvision 0.6.0, 0.9.1, or higher.\n",
    "\n",
    "If you want to patch your local install of torchvision 0.9.1 for FCN-ResNet-18, [try this Gist](https://gist.github.com/cheadrian/f8ea250d78c2bb9bc913aa89f18f8e21).\n",
    "This patch also works for torchvision 0.6.0 to 0.9.0 but you'll also need the segmentation model code\n",
    "in `lraspp.py`, `mobilenetv2.py`, and `mobilenetv3.py` from version 0.9.1.\n",
    "\n",
    "Luckily, we've already set this up for you on the DS&AI GPU server. See below.\n",
    "\n",
    "## GPU server setup\n",
    "\n",
    "This lab will work fine on the DS&AI JupyterHub\n",
    "server. [Here is the login for AIT](https://puffer.cs.ait.ac.th/hub/login?next=).\n",
    "Use the `CV-Class` image.\n",
    "\n",
    "You might instead want to create a custom environment according to\n",
    "[the RTML GPU setup guide](https://github.com/dsai-asia/RTML/blob/main/Labs/01-Setup/01-Setup.ipynb),\n",
    "but that would require quite a bit of additional work!\n",
    "\n",
    "## Training Scripts\n",
    "\n",
    "Download the [training scripts and data for this lab](https://drive.google.com/file/d/1ihaFWQLTsFPpzAfAHfhrt1cR4WEwhH7r/view).\n",
    "\n",
    "Move the code and data into your project directory and take a look at `train.py` and the code it uses.\n",
    "The code was originally from torchvision then modified by Dustin Franklin at NVIDIA for smaller models like\n",
    "FCN-ResNet-18.\n",
    "\n",
    "The floor data are from an earlier experiment using a high-end mobile phone camera attached to the robot. The autofocus and depth of field are\n",
    "much better on this magic camera than on the Raspberry Pi camera we use with the Jetson Nano. Once you get the training working, you'll want to\n",
    "replace or augment this dataset with the dataset we created in Lab 03.\n",
    "\n",
    "## Train FCN-ResNet-18 from scratch on Pascal VOC\n",
    "\n",
    "Use train.py to learn an initial model from the Pascal VOC 2012 (21 class) dataset. You'll want to train for about 100 epochs and take the model with the best IoU on the validation set. Expect about 56% IoU.\n",
    "\n",
    "The standard location of the VOC trainval dataset is at Oxford. To have `train.py` try to download it for you, uncomment line 99 and comment out line 100.\n",
    "However, we find that the Oxford site has been unavailable recently. If you find it so, you can also manually download VOC from\n",
    "[Joseph Redmond's old mirror](https://pjreddie.com/projects/pascal-voc-dataset-mirror/). You want the VOC 2012 train/validation dataset. Unpack it in your data\n",
    "directory.\n",
    "\n",
    "With that, you should be able to run `train.py`. Check the options. You can change batch size, number of workers, etc. to get the best runtime\n",
    "on your GPU. The model weights after each iteration as well as the best model will be saved in your working directory.\n",
    "\n",
    "## Transfer learning\n",
    "\n",
    "Once we have a good model on VOC, let's fine tune on our robot floor dataset.\n",
    "Take a look at <code>retrain.py</code>. This script loads the pre-trained FCN-ResNet-18 we built in the previous step and fine tunes it on the robot floor data set.\n",
    "\n",
    "Note that the fine tuning script expects your best VOC model weights to be in a file called `fcn_resnet18_voc_best.pth`.\n",
    "\n",
    "Take a look at the code and play around with it, only fine tuning the fresh output layer or also tuning the layers already trained on VOC. You'll also want to experiment with the relative weighting of floor vs. obstacle classes in the loss function. If you don't give obstacles (the rare class) a high weight, the model\n",
    "may learn to classify everything as floor!\n",
    "\n",
    "I didn't put the code to save the model weights here. To get your model saved, read and understand the two scripts `train.py` and `fine_tune.py` and add the code to save your model to the fine tuning script.\n",
    "\n",
    "Give some thought to what criteria you should use for the \"best model so far.\" If you only cared about per-pixel accuracy, you would probably just classify all pixels as \"floor.\" What you probably want to maximize is the mean of the IoU for the two classes.\n",
    "\n",
    "## Export to ONNX and run under OpenCV DNN\n",
    "\n",
    "Once that's all working, export the model to ONNX using the provided script and see if you can get it running on OpenCV DNN.\n",
    "\n",
    "## Put it all together!\n",
    "\n",
    "Finally, you should be able to display, frame by frame, the input image with floor/non-floor pixels identified and a birds-eye view map of the\n",
    "obstacle-free space around the robot (using the homography you saved in Lab 02 or 03).\n",
    "\n",
    "## Try another video\n",
    "\n",
    "[Here's another video from the same environment](https://drive.google.com/file/d/1LKH5zPhZRPKSHF287apsaOL5ZMN3c7JB/view?usp=sharing)\n",
    "taken during the daytime, with different lighting and reflection challenges. Does your model work? Probably not!\n",
    "\n",
    "## The report\n",
    "\n",
    "Post a video of your result to Piazza before the next lab, and turn in a report describing your experiments and results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
